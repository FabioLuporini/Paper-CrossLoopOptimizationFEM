\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

% TODO: ok?

\usepackage{tabularx}
\usepackage{tabulary}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage[longend,linesnumbered,figure]{algorithm2e}
\usepackage{algorithmic}


\newcommand{\code}[1]{\lstset{basicstyle=\small\tt}\lstinline£#1£\lstset
{basicstyle=\footnotesize\tt}}

% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{COFFEE: an Optimizing Compiler\\for Finite Element Local Assembly}


\author{\IEEEauthorblockN{Fabio Luporini\IEEEauthorrefmark{1},
Ana Lucia Varbanescu\IEEEauthorrefmark{2},
Florian Rathgeber\IEEEauthorrefmark{1},
Gheorghe-Teodor Bercea\IEEEauthorrefmark{1}, \\
J. Ramanujam\IEEEauthorrefmark{3},
David A. Ham\IEEEauthorrefmark{4}\IEEEauthorrefmark{1},
and
Paul H. J. Kelly\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Imperial College London, Department of Computing,\\
\IEEEauthorrefmark{4}Imperial College London, Department of Mathematics,\\
\IEEEauthorrefmark{2}University of Amsterdam, Department of Computer Sciences,\\
\IEEEauthorrefmark{3}Louisiana State University, Department of Electrical and Computer Engineering,\\
\IEEEauthorrefmark{1}\{f.luporini12$|$f.rathgeber10$|$gheorghe-teodor.bercea08$|$p.kelly$|$david.ham\}@imperial.ac.uk,\\
\IEEEauthorrefmark{2}a.l.varbanescu@uva.nl,
\IEEEauthorrefmark{3}jxr@ece.lsu.edu
}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle
%OLD 1
%The finite element method is widely-employed to approximate solutions of partial differential equations. Local assembly is one of its steps, in which a problem-specific kernel is applied to all elements in the discretized domain of the equation. Since the domain size can be huge, executing efficient kernels is fundamental. Automating their optimization is, however, a challenging issue. Even if an affine loop nest can be generally distinguished, small problem size and complexity of mathematical expressions make it hard to determine a single or unique sequence of successful transformations. In this context, we present COFFEE, a domain-specific compiler for local assembly kernels. COFFEE manipulates kernels’ abstract syntax trees by introducing composable optimizations aimed at improving instruction-level parallelism, especially SIMD vectorization, and register locality. It then generates C code including vector intrinsics. Systematic performance evaluation using a suite of real-world examples shows that speed-ups over non-optimized kernels up to 4.44$\times$ are achievable.

%OLD 2 before feedback
%The finite element method is widely-employed to approximate solutions of partial differential equations. Local assembly is one of its steps, in which a problem-specific kernel is applied to all elements in the discretized domain of the equation. Since the domain size can be huge, executing efficient kernels is fundamental. Automating their optimization is, however, a challenging issue. Even if an affine loop nest can be generally distinguished, small problem size and complexity of mathematical expressions make it hard to determine a single or unique sequence of successful transformations. In this context, we present design and systematic evaluation of COFFEE, a domain-specific compiler for local assembly kernels. COFFEE manipulates abstract syntax trees by introducing composable optimizations aimed at improving instruction-level parallelism, especially SIMD vectorization, and register locality. It then generates C code including vector intrinsics. Performance evaluation using a suite of real-world examples shows that signifcant speed-ups up are achievable.

\begin{abstract}
%\boldmath
The numerical solution of partial differential equations using the finite
element method is one of the key applications of high performance
computing. Local assembly is its characteristic operation. This entails the
execution of a problem-specific kernel for each element in the discretized problem domain. Since the domain size can be huge, executing efficient kernels is fundamental. Their optimization is, however, a challenging issue. Even though affine loop nests are generally present, the short trip counts and the complexity of mathematical expressions make it hard to determine a single or unique sequence of successful transformations. We present, therefore, the design and systematic evaluation of COFFEE, a domain-specific compiler for local assembly kernels. COFFEE manipulates abstract syntax trees by introducing composable optimizations aimed at improving instruction-level parallelism, especially SIMD vectorization, and register locality. It then generates C code including vector intrinsics. Experiments using a range of finite-element forms of increasing complexity show that significant performance improvement is achieved.
\end{abstract}


% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

\begin{IEEEkeywords}
Finite element integration, local assembly, compilers, optimizations, simd vectorization
\end{IEEEkeywords}




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
In many fields, such as computational fluid dynamics, computational electromagnetics, and structural mechanics, phenomena are modelled by partial differential equations (PDEs). Numerical techniques, such as the finite volume method and finite element method, are widely-employed to approximate solutions of these PDEs. Unstructured meshes are often used to
discretize the equation domain, since their geometric flexibility allows solvers to be extremely effective. The solution is sought in each element of the discretized domain by applying suitable numerical operations. As the number of elements can be of the order of trillions \cite{Rossinelli2013}, a major issue is the time required to execute the computation, which can be hours or days. To address this problem, domain-specific languages (DSLs) have been developed. The successful port of Hydra, an industrial computational fluid dynamics application developed by Rolls Royce for turbomachinery design (based on finite volume method, roughly 50,000 lines of code and with meshes in excess of 100M edges), to OP2~\cite{op2-main}, demonstrates the effectiveness of the DSL approach in the implementation of PDE solvers~\cite{IstvanHydra}.

OP2 adopts a programming model in which computations are expressed through self-contained functions, referred to as ``kernels''. A kernel is applied to all elements in a set of mesh components, such as edges, vertices, or cells, with an implicit synchronization between the application of two consecutive kernels. On commodity multi-cores, a kernel is executed sequentially by a thread, while parallelism is achieved partitioning the mesh and assigning each partition to a thread. Similar programming and execution models are adopted in~\cite{pyop2isc},~\cite{Fenics},~\cite{fluidity_manual_v4},~\cite{lizst}. Kernel optimization is one of the major concerns in unstructured mesh applications. In this paper, we tackle this problem in the context of the finite element method.

We focus on improving the performance of local assembly (``assembly'', in the following), a fundamental step of the finite element method that covers an important fraction of the overall computation run-time, often in the range 30$\%$-60$\%$. During the assembly phase, the solution of the PDE is approximated by executing a suitable kernel over all elements in the discretized domain. A kernel's working set is usually small enough to fit the L1 cache; it might need L2 cache when high-order methods are employed. However, we do not consider the latter case. An assembly kernel is characterized by the presence of an affine, often non-perfect loop nest, in which individual loops are rather small: their trip count rarely exceeds 30, and may be as low as 3 for lower order methods. In the innermost loop, a problem-specific expression evaluates a two dimensional array, representing the result of local assembly in an element of the discretized domain. With such a kernel structure, we focus on aspects like minimization of floating-point operations, register allocation and instruction-level parallelism, especially in the form of SIMD vectorization.

Achieving high-performance is non-trivial. The complexity of the mathematical expressions, often characterized by a large number of operations on constants and small matrices, makes it hard to determine a single or specific sequence of transformations that is successfully applicable to all problems. The variation in loop trip counts and their typically small value further exacerbates the issue. A compiler-based approach is, therefore, the only reasonable option to obtain close-to-peak performance in a wide range of different local assembly kernels. Optimizations like generalized loop-invariant code motion, vector-register tiling, and expression splitting, as well as their composition, are essential, but none are supported by state-of-the-art polyhedral and vendor compilers. BLAS routines could be theoretically employed, although a fairly complicated control- and data-flow analysis is required to automate identification and extraction of matrix-matrix multiplies. In addition, as detailed in Section~\ref{sec:perf-eval-blas}, the small dimension of the  matrices involved and the potential loss in data locality can limit or eliminate the performance gain of this approach.

In order to overcome the constraints of the available compilers and specialized linear-algebra libraries, we have automated a set of generic and model-driven code transformations in COFFEE\footnote{COFFEE stands for COmpiler For FinitE Element local assembly.}, a compiler for optimizing local assembly kernels. COFFEE is integrated with Firedrake~\cite{firedrake-code}, a system for solving PDEs through the finite element method based on the PyOP2 abstraction~\cite{pyop2isc}. It supports all problems expressible with this framework. This allows us to evaluate our code transformations in a range of real-world problems, varying two key parameters that impact both solution accuracy and kernel cost: the polynomial order of the method (from $p=1$ to $p=4$) and the geometry of elements in the discretized domain (2D triangle, 3D tetrahedron, 3D prism).

Our experiments showed that the originally generated code for non-trivial assembly kernels was sub-optimal. Our cost-model-driven sequence of source-to-source code transformations, aimed at improving SIMD vectorization and register data locality, can result in performance improvements up to 1.5$\times$ over code in which only basic transformations have been performed, such as generalized loop-invariant code motion, padding, and data alignment, and up to 4.4$\times$ over original kernels. The contributions of this paper are
\begin{itemize}
\item An optimization strategy for finite element local assembly. Our approach exploits domain knowledge and goes beyond the limits of both vendor and research compilers.
\item Design and implementation of a compiler that automates the proposed code transformations for any problems expressible in Firedrake.
\item Systematic analysis using a suite of examples of real-world importance, and evidence of significant performance improvements on two Intel architectures, a Sandy Bridge CPU and the Xeon Phi.
\end{itemize}

The paper is organized as follows. In Section~\ref{sec:background} we provide some background on local assembly, showing code generated by Firedrake and emphasizing the critical computational aspects. Section~\ref{sec:code-transf} describes the various code transformations, highlighting when and how domain-knowledge has been exploited. The design and implementation of our compiler is discussed in Section~\ref{sec:pyop2-compiler}. Section~\ref{sec:perf-results} shows performance results. Related work is illustrated in Section~\ref{sec:related-work}, while Section~\ref{sec:conclusions} reviews our contributions in the light of our results, and identifies priorities for future work.

% systematic optimization strategy -> discuss the importance of domain knowledge

% A python implementation of OP2, namely PyOP2, is used in the Firedrake project as an intermediate representation for FEM computations. In Firedrake all computational kernels are generated by a compiler, (the Fenics Form Compiler, FFC), whereas in OP2 applications kernels are provided by users.

\section{Background}
\label{sec:background}
%Here we discuss about the computational characteristics of computational science kernels. Briefly cite op2 kernels. Emphasis on Finite Element Assembly. Generalization and formalization of a Finite Element Assembly kernel using quadrature representation.

\begin{algorithm}[t]
\small
\KwIn{element matrix (2D array, initialized to 0),\\~~~~~~~~~element coordinates (array),\\~~~~~~~~~coefficient fields (array, e.g. velocity)}
\KwOut{element matrix (2D array)}
\nlset{1}~~- Compute Jacobian from coordinates \\
~~- Declare of constant data: basis functions and derivatives\\
~~- Compute element matrix using numerical quadrature
\caption{General structure of a local assembly kernel generated by Firedrake.}
\label{code:general-structure}
\end{algorithm}

\begin{algorithm}[t]
\small
\KwSty{void} helmholtz(double A[3][3], double **coords) $\lbrace$\\
~~// K, det = Compute Jacobian (coords) \\
~~\\
~~\KwSty{static const double} W3[3] = $\lbrace$...$\rbrace$\\
~~\KwSty{static const double} X$\_$D10[3][3] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\KwSty{static const double} X$\_$D01[3][3] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$3; i++) \\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$3; k++) \\
~~~~~~~~A[j][k] += ((Y[i][k]*Y[i][j]+\\
~~~~~~~~~~~+((K1*X$\_$D10[i][k]+K3*X$\_$D01[i][k])*\\
~~~~~~~~~~~*(K1*X$\_$D10[i][j]+K3*X$\_$D01[i][j]))+\\
~~~~~~~~~~~+((K0*X$\_$D10[i][k]+K2*X$\_$D01[i][k])*\\
~~~~~~~~~~~*(K0*X$\_$D10[i][j]+K2*X$\_$D01[i][j])))*\\
~~~~~~~~~~~*det*W3[i]);\\
$\rbrace$
\caption{Local assembly code generated by Firedrake for a Helmholtz problem on a 2D triangular mesh using Lagrange $p=1$ elements.}
\label{code:helmholtz}
\end{algorithm}

\begin{algorithm}[t]
\small
\KwSty{void} burgers(double A[12][12], double **c, double **w) $\lbrace$\\
~~// K, det = Compute Jacobian (c) \\
~~\\
~~\KwSty{static const double} W5[5] = $\lbrace$...$\rbrace$\\
~~\KwSty{static const double} X1$\_$D001[5][12] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\KwSty{static const double} X2$\_$D001[5][12] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~//11 other basis functions definitions.\\
~~...\\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$5; i++) $\lbrace$\\
~~~~\KwSty{double} F0 = 0.0;\\
~~~~//10 other declarations (F1, F2,...)\\
~~~~...\\
~~~~\KwSty{for} (\KwSty{int} r = 0; r$<$12; r++) $\lbrace$\\
~~~~~~F0 += (w[r][0]*X1$\_$D100[i][r]);\\
~~~~~~//10 analogous statements (F1, F2, ...)\\
~~~~$\rbrace$\\
~~~~...\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$12; j++) \\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$12; k++) \\
~~~~~~~~A[j][k] += (..(K5*F9)+(K8*F10))*Y1[i][j])+\\
~~~~~~~~~+(((K0*X1$\_$D100[i][k])+(K3*X1$\_$D010[i][k])+\\
~~~~~~~~~+(K6*X1$\_$D001[i][k]))*Y2[i][j]))*F11)+\\
~~~~~~~~~+(..((K2*X2$\_$D100[i][k])+...+(K8*X2$\_$D001[i][k])*\\
~~~~~~~~~*(K2*X2$\_$D100[i][j])+...+(K8*X2$\_$D001[i][j]))..)+\\
~~~~~~~~~+ $<$roughly a hundred sum/muls go here$>$)..)*\\
~~~~~~~~~*det*W5[i]);\\
~~$\rbrace$ \\
$\rbrace$
\caption{Local assembly code generated by Firedrake for a Burgers problem on a 3D tetrahedral mesh using Lagrange $p=1$ elements.}
\label{code:burgers}
\end{algorithm}

Local assembly is the computation of contributions of a specific cell in the discretized domain to the PDE solution. The process consists of numerically evaluating an integral to produce a small dense element matrix or an element vector~\cite{quadrature1, fluidity_manual_v4}. This operation is applied to all cells in the discretized domain. In this work we focus on local matrices, or ``element matrices'', which are more costly to compute than element vectors.

Given a mathematical description of the input problem, expressed through the domain-specific Unified Form Language~\cite{ufl}, Firedrake generates C-code kernels implementing assembly using a numerical integration algorithm. It then triggers compilation of such kernels using a vendor compiler, and manages parallel execution over the mesh. The subject of this paper is to enhance this execution model by adding an optimization stage prior to the generation of C code. The code transformations described next are also generalizable to non-Firedrake assembly kernels, provided that numerical integration is used.

The general structure of a Firedrake-generated kernel is shown in Figure~\ref{code:general-structure}. The inputs are a zero-initialized two dimensional array used to store the element matrix, the element's coordinates in the discretized domain, and coefficient fields, for instance indicating value of velocity or pressure in the element. The output is the evaluated element matrix. A local assembly kernel can be logically split into three parts: 1) calculation of the Jacobian matrix, its determinant and its inverse; 2) definition of a problem-specific set of constant two dimensional arrays of double precision floats containing the values of the basis functions and their derivatives at the integration points; 3) evaluation of the element matrix in an affine loop nest. All elements in the discretized domain share the same basis function arrays, so they are declared as global read-only arrays (i.e. using \texttt{static const} in C). Table~\ref{table:map-name-letters} shows the variable names we will use in the upcoming code snippets to refer to the various kernel objects.

\begin{table}[h]
\begin{center}
\begin{tabulary}{1.0\columnwidth}{|C|C|C|}
\hline
Object name & Type & Variable name(s) \\\hline\hline
Determinant of the Jacobian matrix & double & det  \\ \hline
Inverse of the Jacobian matrix & double[] & K \\ \hline
Coordinates & double** & coords\\ \hline
Fields & double** & w \\ \hline
Numerical integration weights & double[] & W \\ \hline
Basis functions (and derivatives) & double[][] & X, Y, X1, ... \\ \hline
Element matrix & double[][] & A\\ \hline
\end{tabulary}
\end{center}
\caption{Type and variable names used in the various code snippets to identify local assembly objects.}
\label{table:map-name-letters}
\end{table}

The actual complexity of a local assembly kernel depends on the finite element problem being solved. In simpler cases, the loop nest is perfect, it has short trip counts (in the range 3-15), and the computation reduces to a summation of a few products involving basis functions. An example is provided in Figure~\ref{code:helmholtz}, which shows the assembly kernel for a Helmholtz problem using Lagrange basis functions on 2D elements with polynomial order $p=1$. In other scenarios, for instance when solving a non-linear problem like the Burgers equation, whose assembly code is given in Figure~\ref{code:burgers}, the number of arrays involved in the computation of the element matrix can be much larger: in this case, 14 unique arrays are accessed, and the same array can be referenced multiple times within the expression. This may also require the evaluation of constants in outer loops (called $F$ in the code) to act as as scaling factors of arrays; trip counts can be larger (proportionally to the order of the method); and arrays may be block-sparse. In addition to a larger number of operations, more complex cases like the Burgers equation are characterized by high register pressure. 

Despite the infinite variety of problem-specific assembly kernels which Firedrake can generate, it is still possible to identify common domain-specific traits that can be exploited for effective code transformations and SIMD vectorization. These include: 1) memory accesses along the three loop dimensions are always stride-1; 2) the $j$ and $k$ loops are interchangeable, whereas interchanges involving the $i$ loop require pre-computation of values (e.g. the $F$ values in Burgers) and introduction of temporary arrays, as explained in Section~\ref{sec:code-transf}; 3) the $j$ and $k$ loops iterate over the same iteration space; 4) most of the sub-expressions on the right hand side of the element matrix computation depend on just two loops (either $i$-$j$ or $i$-$k$). In Section~\ref{sec:code-transf} we show how to exploit these observations to define a set of systematic, composable optimizations.


\section{Code Transformations}
\label{sec:code-transf}
The code transformations presented in this section are applicable to all finite element problems that can be formulated in Firedrake. One peculiar characteristic is that they all aim at improving the run-time of the \texttt{ijk} loop nest, in which the numerical integration takes place. In rare cases, as in problems involving hyperelasticity, however, such optimizations might not be sufficient to achieve notable performance improvements. Generalized loop-invariant code motion, which is described in Section~\ref{sec:code-transf-licm}, is a fundamental optimization that allows pre-computation of invariant sub-expressions. It is worth noting there are circumstances in which the number of lifted terms, either at the level of an outer loop or even completely outside of the loop nest, is so large (thousands of operations, hundreds of temporaries) that their execution time becomes the dominant factor of the overall local assembly run-time. In these cases, two challenging optimizations are common sub-expression elimination and effective vectorization (superword level parallelism~\cite{SLP} would not be of help, because invariant sub-expressions lack, in general, structure). A comprehensive study of such cases is left as further work.

As already emphasized, the structure of mathematical expressions evaluating the element matrix and the variation in loop trip counts, although typically limited to the order of tens iterations, render the optimization process challenging. It is not always the same set of optimizations that bring performance closest to the machine peak. For example, the Burgers problem in Figure~\ref{code:burgers}, given the large number of arrays accessed, suffers from high register pressure, whereas the Helmholtz problem in Figure~\ref{code:helmholtz} does not; this intuitively suggests that the two problems require a different treatment, based on an in-depth analysis of both data and iteration spaces. Furthermore, domain-knowledge enables transformations that a general-purpose compiler could not apply, making the optimization space even larger. In this context, our goal is to understand the relationship between distinct code transformations, their impact on local assembly kernels, and to what extent their composability is effective in a class of problems and architectures.

% TODO: could add a sentence here to bridge the two paragraphs

% Padding and Data Alignment
\subsection{Prerequisites for Effective Auto-vectorization: Padding and Data Alignment}
Auto-vectorization of assembly code computing the element matrix can be less effective if data are not aligned and if the length of the innermost loop is smaller than the vector length $vl$. Data alignment is enforced in two steps. Initially, all arrays are allocated to addresses that are multiples of $vl$. Then, two dimensional arrays are padded by rounding the number of columns to the nearest multiple of $vl$. For instance, assume the original size of a basis function array is 3$\times$3 and $vl=4$ (e.g. AVX processor, with 256 bits long vector registers and 64-bits double-precision floating-point values). In this case, a padded version of the array will have size 3$\times$4. The compiler is explicitly informed about data alignment using a suitable pragma. Padding of all two dimensional arrays involved in the evaluation of the element matrix also allows us to safely round the loop trip count to the nearest multiple of $vl$. This avoids the introduction of a remainder (scalar) loop from the compiler, which would render vectorization less efficient.

% Autovectorizable LICM -> show code snippet?
\subsection{Generalized Loop-invariant Code Motion}
\label{sec:code-transf-licm}
From the inspection of the codes in Figures~\ref{code:helmholtz} and~\ref{code:burgers}, it can be noticed that the computation of $A$ involves evaluating many sub-expressions which only depend on two iteration variables. Since symbols in most of these sub-expressions are read-only variables, there is ample space for loop-invariant code motion. Vendor compilers apply this technique, although not in the systematic way we need for our assembly kernels. We want to overcome two deficiencies that both $intel$ and $gnu$ compilers exhibit. First, they only identify sub-expressions that are invariant with respect to the innermost loop. This is an issue for sub-expressions depending on $i$-$k$, which are not automatically lifted in the loop order \texttt{ijk}. Second, the hoisted code is scalar, i.e. it is not subjected to auto-vectorization. We work around these limitations with source-level loop-invariant code motion. In particular, we pre-compute all values that an invariant sub-expression assumes along its fastest varying dimension. This is implemented by introducing a temporary array per invariant sub-expression and by adding a new loop to the nest. At the price of extra memory for storing temporaries, the gain is that lifted terms can be auto-vectorized as part of an inner loop. Given the short trip counts of our loops, it is important to achieve auto-vectorization of hoisted terms in order to minimize the percentage of scalar instructions, which could be otherwise significant. It is also worth noting that, in some problems, invariant sub-expressions along $j$ are identical to those along $k$ (e.g. in Helmholtz). In these cases, we safely avoid redundant pre-computation since, as anticipated in Section~\ref{sec:background}, a property of our domain is that $j$ and $k$ loops share the same iteration space.

\begin{algorithm}[t]
\small
\KwSty{void} helmholtz(double A[3][4], double **coords) $\lbrace$\\
~~\KwSty{$\#$define} ALIGN $\_\_$attribute$\_\_$((aligned(32))) \\
~~// K, det = Compute Jacobian (coords) \\
~~\\
~~\KwSty{static const double} W3[3] ALIGN = $\lbrace$...$\rbrace$\\
~~\KwSty{static const double} X$\_$D10[3][4] ALIGN = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\KwSty{static const double} X$\_$D01[3][4] ALIGN = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$3; i++) $\lbrace$ \\
~~~~double LI$\_$0[4] ALIGN;\\
~~~~double LI$\_$1[4] ALIGN;\\
~~~~\KwSty{for} (\KwSty{int} r = 0; r$<$4; r++) $\lbrace$ \\
~~~~~~LI$\_$0[r] = ((K1*X$\_$D10[i][r])+(K3*X$\_$D01[i][r]));\\
~~~~~~LI$\_$1[r] = ((K0*X$\_$D10[i][r])+(K2*X$\_$D01[i][r]));\\
~~~~$\rbrace$\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{$\#$pragma vector aligned}\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k++) \\
~~~~~~~~A[j][k] += (Y[i][k]*Y[i][j]+LI$\_$0[k]*LI$\_$0[j]+\\
~~~~~~~~~~~~~~~~~~~~+LI$\_$1[k]*LI$\_$1[j])*det*W3[i]);\\
~~$\rbrace$\\
$\rbrace$
\caption{Local assembly code generated by Firedrake in which padding, data alignment, and \emph{licm} have been applied to the Helmholtz problem given in Figure~\ref{code:helmholtz} for an AVX architecture. In this specific case, sub-expressions invariant to $j$ are identical to those invariant to $k$, so they can be precomputed once in the $r$ loop.}
\label{code:helmholtz-licm}
\end{algorithm}

% Register allocation and register locality
\subsection{Model-driven Vector-register Tiling}
One notable problem of assembly kernels concerns register allocation and register locality. The critical situation occurs when loop trip counts and the variables accessed are such that the vector-register pressure is high. Since the kernel's working set fits the L1 cache, it is remarkably important to optimize register management. Standard optimizations, such as loop interchange, unroll, and unroll-and-jam, can be employed to deal with this problem. In COFFEE, these optimizations are supported either by means of explicit code transformations (interchange, unroll-and-jam) or indirectly by delegation to the compiler through standard pragmas (unroll). Tiling at the level of vector registers is an additional feature of COFFEE. Based on the observation that the evaluation of the element matrix can be reduced to a summation of outer products along the $j$ and $k$ dimensions, a model-driven vector-register tiling strategy can be implemented. If we consider the code snippet in Figure~\ref{code:helmholtz-licm} and we ignore the presence of the operation \texttt{det*W3[i]}, the computation of the element matrix is abstractly expressible as
\begin{equation}
\label{outer-product}
A_{jk} = \sum_{\substack{
  x \in B' \subseteq B \\
  y \in B'' \subseteq B}}
x_j\cdot y_k ~~~~~~ j,k = 0,...,3
\end{equation}
where $B$ is the set of all basis functions (or temporary variables, e.g. \texttt{LI$\_$0}) accessed in the kernel, whereas $B'$ and $B''$ are generic problem-dependent subsets. Regardless of the specific input problem, by abstracting from the presence of all variables independent of both $j$ and $k$, the element matrix computation is always reducible to this form. Figure~\ref{fig:vect-by-vect} illustrates how we can evaluate 16 entries ($j,k=0,...,4$) of the element matrix using just 2 vector registers, which represent a 4$\times$4 tile, assuming $\vert B' \vert = \vert B'' \vert = 1$. Values in a register are shuffled each time a product is performed. Standard compiler auto-vectorization ($gnu$ and $intel$), instead, executes 4 broadcast operations (i.e. ``splat'' of a value over all of the register locations) along the outer dimension to perform the calculation. In addition to incurring in larger number of cache accesses, it needs to keep between $f=1$ and $f=3$ extra registers to perform the same 16 evaluations when unroll-and-jam is used, with $f$ being the unroll-and-jam factor.

%More importantly, we avoid using $vl$-1 registers per each outer-loop-dependent variable (i.e. all $x$ terms in Equation~\ref{outer-product}), with $vl$ being the vector length. This is a considerable gain, which allows us to slice the iteration space into bigger tiles, implemented directly by vector registers.

\begin{figure}[h]
\centerline{\includegraphics[scale=0.6]{Pictures/vect-by-vect.pdf}}
\caption{Outer-product vectorization by permuting values in a vector register.}
\label{fig:vect-by-vect}
\end{figure}

The storage layout of $A$, however, is incorrect after the application of this outer-product-based vectorization (\emph{op-vect}, in the following). It can be efficiently restored with a sequence of vector shuffles following the pattern highlighted in Figure~\ref{fig:restore-layout}, executed once outside of the \texttt{ijk} loop nest. The generated pseudo-code for the simple Helmholtz problem when using \emph{op-vect} is shown in Figure~\ref{code:helmholtz-opvect}.

\begin{figure}[t]
\centerline{\includegraphics[scale=0.60]{Pictures/vect-restore.pdf}}
\caption{Restoring the storage layout after \emph{op-vect}. The figure shows how 4$\times$4 elements in the top-left block of the element matrix $A$ can be moved to their correct positions. Each rotation, represented by a group of three same-colored arrows, is implemented by a single shuffle intrinsic.}
\label{fig:restore-layout}
\end{figure}

%are sufficiently large to prevent register reuse along the innermost dimension $k$. A combination of the two scenarios is also possible. We want to exploit the small size of the iteration space and the structure of the computation to optimize register management.

\begin{algorithm}[t]
\small
\KwSty{void} helmholtz(double A[8][8], double **coords) $\lbrace$\\
~~// K, det = Compute Jacobian (coords) \\
~~// Declaration of basis function matrices \\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$6; i++) $\lbrace$ \\
~~~~// Do loop-invariant code motion \\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$4; j+=4) \\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$8; k+=4) $\lbrace$\\
~~~~~~~~// $load$ and $set$ intrinsics \\
~~~~~~~~// Compute A[1,1],A[2,2],A[3,3],A[4,4] \\
~~~~~~~~// One $permute\_pd$ intrinsic per $k$-loop $load$\\
~~~~~~~~// Compute A[1,2],A[2,1],A[3,4],A[4,3] \\
~~~~~~~~// One $permute2f128\_pd$ intrinsic per $k$-loop $load$\\
~~~~~~~~// ...\\
~~~~~~$\rbrace$\\
~~~~// Remainder loop (from $j=4$ to $j=6$)\\
~~$\rbrace$\\
~~// Restore the storage layout:\\
~~\KwSty{for} (\KwSty{int} j = 0; j$<$4; j+=4) $\lbrace$\\
~~~~$\_\_$m256d r0, r1, r2, r3, r4, r5, r6, r7;\\
~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$8; k+=4) $\lbrace$\\
~~~~~~r0 = $\_$mm256$\_$load$\_$pd ($\&$A[j+0][k]);\\
~~~~~~// Load A[j+1][k], A[j+2][k], A[j+3][k]\\
~~~~~~r4 = $\_$mm256$\_$unpackhi$\_$pd (r1, r0);\\
~~~~~~r5 = $\_$mm256$\_$unpacklo$\_$pd (r0, r1);\\
~~~~~~r6 = $\_$mm256$\_$unpackhi$\_$pd (r2, r3);\\
~~~~~~r7 = $\_$mm256$\_$unpacklo$\_$pd (r3, r2);\\
~~~~~~r0 = $\_$mm256$\_$permute2f128$\_$pd (r5, r7, 32);\\
~~~~~~r1 = $\_$mm256$\_$permute2f128$\_$pd (r4, r6, 32);\\
~~~~~~r2 = $\_$mm256$\_$permute2f128$\_$pd (r7, r5, 49);\\
~~~~~~r3 = $\_$mm256$\_$permute2f128$\_$pd (r6, r4, 49);\\
~~~~~~$\_$mm256$\_$store$\_$pd ($\&$A[j+0][k], r0);\\
~~~~~~// Store A[j+1][k], A[j+2][k], A[j+3][k]\\
~~~~$\rbrace$\\
~~$\rbrace$\\
$\rbrace$
\caption{Local assembly code generated by Firedrake for the Helmholtz problem in which \emph{op-vect} has been applied on top of the optimizations shown in Figure~\ref{code:helmholtz-licm}. Here, we assume the polynomial order is $p=2$, since \emph{op-vect} can not be used when an iteration space dimension is smaller than the vector length. The original size of the $j$-$k$ iteration space (i.e. before padding was applied) was 6$\times$6. In this example, the unroll-and-jam factor is 1.}
\label{code:helmholtz-opvect}
\end{algorithm}



% Splitting
\subsection{Expression Splitting}
\label{sec:expr-split}
In complex kernels, like Burgers in Figure~\ref{code:burgers}, and on certain architectures, achieving effective register allocation can be challenging. If the number of variables independent of the innermost-loop dimension is close to or greater than the number of available CPU registers, poor register reuse is likely to be obtained. This usually happens when the number of basis function arrays, temporaries introduced by generalized loop-invariant code motion, and problem constants is large. For example, applying loop-invariant code motion to Burgers on a 3D mesh requires 33 temporaries for the \texttt{ijk} loop order. This can make hoisting of the invariant loads out of the $k$ loop inefficient on architectures with a relatively low number of registers. One potential solution to this problem consists of suitably ``splitting'' the computation of the element matrix $A$ into multiple sub-expressions; an example, for the Helmholtz problem, is given in Figure~\ref{code:helmholtz-split}. 

Splitting an expression has three drawbacks. Firstly, it increases the number of accesses to $A$ proportionally to the ``split factor'', which is the number of sub-expressions produced. Secondly, depending on how the split is executed, it can lead to redundant computation; for example, the times the product $det*W3[i]$ is performed is proportional to the number of sub-expressions, as shown in the code snippet. Finally, it might affect register locality: for instance, the same array could be accessed in different sub-expressions, requiring a proportional number of loads be performed. This is not the case of the Helmholtz example. Nevertheless, as shown in Section~\ref{sec:perf-results}, the performance gain from improved register reuse along inner dimensions can still be greater, especially if the split factor and the splitting itself use heuristics to minimize the aforementioned issues.


\begin{algorithm}[t]
\small
\KwSty{void} helmholtz(double A[3][4], double **coords) $\lbrace$\\
%~~\KwSty{$\#$define} ALIGN $\_\_$attribute$\_\_$((aligned(32))) \\
~~~~// Same code as in Figure~\ref{code:helmholtz-licm} up to line 15 \\
%~~// Declaration of basis function matrices \\
%~~\\
%~~\KwSty{for} (\KwSty{int} i = 0; i$<$3; i++) $\lbrace$ \\
%~~~~double LI$\_$0[4] ALIGN;\\
%~~~~double LI$\_$1[4] ALIGN;\\
%~~~~\KwSty{for} (\KwSty{int} r = 0; r$<$4; r++) $\lbrace$ \\
%~~~~~~LI$\_$0[r] = ((K1*X$\_$D10[i][r])+(K3*X$\_$D01[i][r]));\\
%~~~~~~LI$\_$1[r] = ((K0*X$\_$D10[i][r])+(K2*X$\_$D01[i][r]));\\
%~~~~$\rbrace$\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{$\#$pragma vector aligned}\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k++) \\
~~~~~~~~A[j][k] += (Y[i][k]*Y[i][j]+LI$\_$0[k]*LI$\_$0[j])*det*W3[i];\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{$\#$pragma vector aligned}\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k++) \\
~~~~~~~~A[j][k] += LI$\_$1[k]*LI$\_$1[j]*det*W3[i];\\
$\rbrace$
\caption{Local assembly code generated by Firedrake for the Helmholtz problem in which \emph{split} has been applied on top of the optimizations shown in Figure~\ref{code:helmholtz-licm}. In this example, the split factor is 2.}
\label{code:helmholtz-split}
\end{algorithm}

Table~\ref{table:code-transformations} summarizes the code transformations described so far. Given that many of these transformations depend on some parameters (e.g. tile size), we need a mechanism to prune such a large space of optimization. This aspect is treated in Section~\ref{sec:pyop2-compiler}.

\begin{table}[h]
\begin{center}
\begin{tabulary}{1.0\columnwidth}{|C|C|}
\hline
Name (Abbreviation) & Parameter \\\hline\hline
Generalized loop-invariant code motion (\emph{licm}) &   \\ \hline
Padding &  \\ \hline
Data Alignment & \\ \hline
Loop interchange      & loops  \\ \hline
Loop unrolling  & unroll factor \\ \hline
Outer-product vectorization (\emph{op-vect}) & tile size \\ \hline
Expression splitting (\emph{split}) & split point, split factor \\ \hline
\end{tabulary}
\end{center}
\caption{Overview of code transformations for Firedrake-generated assembly kernels.}
\label{table:code-transformations}
\end{table}

% mention (obvious) composability

\section{Overview of COFFEE}
\label{sec:pyop2-compiler}
%Design and structure of the PyOP2 Compiler. Show the steps through which the IR is transformed (need to cite Firedrake here). Briefly describe a simple cost model that allows us to prune the space of code transformations.

\begin{algorithm}[t]
  \textbf{The COFFEE Compiler}\\
  \KwIn{ast, wrapper, isa}
  \KwOut{code}
// Analyze ast and build optimization plan \\
it\_space = analyze(ast) \\
\If{\KwSty{not} it\_space}{
  ast.apply\_inter\_kernel\_vectorization(wrapper, isa)
  \Return{wrapper + ast.from\_ast\_to\_c()}
}
plan = cost\_model(it\_space.n\_inner\_arrays, isa.n\_regs) \\
// Optimize ast based on plan \\
ast.licm() \\
ast.padding() \\
ast.data\_align() \\
\If{plan.permute}{
  ast.permute\_assembly\_loops()
}
\If{plan.sz\_split}{
  ast.split(plan.sz\_split)
}
\If{plan.uaj\_factor}{
  uaj = MIN(plan.uaj\_factor, $\lfloor$it\_space.j.size/isa.vf$\rfloor$) \\
  ast.op\_vect(uaj)
}
\Return{wrapper + ast.from\_ast\_to\_c()}
\caption{Pseudocode of the COFFEE pipeline.}
\label{algo:PyOP2Compiler}
\end{algorithm}


Firedrake users employ the Unified Form Language to write problems in a notation resembling mathematical equations. At run-time, the high-level specification is translated by a modified version of the FEniCS Form Compiler~\cite{FFC-Compiler} into an abstract syntax tree (AST) representation of one or more finite element assembly kernels. ASTs are then passed to COFFEE, which is capable of applying the transformations described in Section~\ref{sec:code-transf}. The output of COFFEE, C code, is eventually provided to PyOP2~\cite{pyop2isc}, where just-in-time compilation and execution over the discretized domain take place. Because of the large number of parametric transformations, COFFEE needs a mechanism to select the most suitable optimization strategy for a given problem. Auto-tuning might be used, although it would significantly increase the run-time overhead, since the generation of ASTs occurs at run-time as soon as problem-specific data are available. Our optimization strategy, based on heuristics and a simple cost model, is described in the following, along with an overview of the compiler.

The compiler structure is outlined in Figure~\ref{algo:PyOP2Compiler}. Initially the AST is inspected, looking for the presence of iteration spaces and domain-specific information provided by the higher layer. If the kernel lacks an iteration space, then so-called inter-kernel vectorization, in which the outer, non-affine loop over mesh elements is vectorized, can be applied. This feature, currently under development, has been proved to be useful in several finite volume applications~\cite{inter-kernel-vect}. Then, an ordered sequence of optimization steps are executed. Application of \emph{licm} must precede padding and data alignment, due to the introduction of temporary arrays. Based on a cost model, loop interchange, \emph{split}, and \emph{op-vect} may be introduced. Their implementation is based on analysis and transformation of the kernel's AST. When \emph{op-vect} is selected, the compiler outputs proper AVX or AVX-512 intrinsics code. Any possible corner cases are handled: for example, if \emph{op-vect} is to be applied, but the size of the iteration space is not a multiple of the vector length, then a remainder loop, amenable to auto-vectorization, is inserted.

All loops are interchangeable provided that temporaries are introduced if the nest is not perfect. For the employed storage layout, the loop permutations \texttt{ijk} and \texttt{ikj} are likely to maximize performance. Conceptually, this is motivated by the fact that if the $i$ loop were in an inner position, then a significantly higher number of load instructions would be required at every iteration. We tested this hypothesis in manually crafted kernels. We found that the performance loss is greater than the gain due to the possibility of accumulating increments in a register, rather than in memory, along the $i$ loop. The choice between \texttt{ijk} and \texttt{ikj} depends on the number of load instructions that can be hoisted out of the innermost dimension. Our compiler chooses as outermost the loop along which the number of invariant loads is smaller so that more registers remain available to carry out the computation of the element matrix.

% "vendor compilers such as Intel's" is awkward and unclear. If Intel is the only one you have looked at, then say "the Intel compiler", otherwise name the ones for which you know this to be true.
Since loop unroll and unroll-and-jam of outer loops are fundamental to the exposure of instruction-level parallelism, tuning critical parameters such as the unroll factor is of great importance. However, inspecting assembly code and comparing with other hand-written implementations indicates that recent versions of vendor compilers such as Intel's are capable of estimating close-to-optimal values for such parameters. This is particularly true for assembly kernels, in which the loop nest is affine, bounds are usually very small and known at compile-time, and memory accesses are stride one. We therefore leave the backend compiler in charge of selecting the unroll factor. This choice also simplifies COFFEE's cost model. The only situation in which we explicitly unroll-and-jam a loop is when \emph{op-vect} is used, since the transformed code seems to prevent the Intel compiler from applying this optimization, even if specific pragmas are added. 

%Note that, regardless of the output of the cost model, the unroll-and-jam factor never exceeds the actual size of the outer loop (line 19 in Figure~\ref{algo:PyOP2Compiler}, assuming the default loop order \texttt{ijk}).

The cost model is shown in Figure~\ref{algo:applyCostModel}. It takes into account the number of available logical vector registers, $n\_regs$, and the number of unique variables accessed: $n\_consts$ counts variables independent of both $j$ and $k$ loops and temporary registers, $n\_outer\_arrays$ counts $j$-dependent variables, and $n\_inner\_arrays$ counts $k$-dependent variables, assuming the \texttt{ijk} loop order. These values are used to estimate unroll-and-jam and split factors for \emph{op-vect} and \emph{split}. If a factor is 0, then the corresponding transformation is not applied. The \emph{split} transformation is triggered whenever the number of hoistable terms is larger than the available registers along the outer dimension (lines 3-8), which is approximated as half of the total (line 2). A split factor of $n$ means that the assembly expression should be ``cut'' into $n$ sub-expressions. Depending on the structure of the assembly expression, each sub-expression might end up accessing a different number of arrays; the cost model is simplified by assuming that all sub-expressions are of the same size. The unroll-and-jam factor for the \emph{op-vect} transformation is determined as a function of the available logical registers, i.e. those not used for storing hoisted terms (line 9-11). Finally, the profitability of loop interchange is evaluated (line 12-16).


\begin{algorithm}[t]
  \textbf{Cost Model}\\
  \KwIn{n\_outer\_arrays, n\_inner\_arrays, n\_consts, n\_regs}
  \KwOut{uaj\_factor, split\_factor}

n\_outer\_regs = n\_regs / 2 \\
split\_factor = 0 \\
// Compute spltting factor \\
\While{n\_outer\_arrays $>$ n\_outer\_regs}{
  n\_outer\_arrays = n\_outer\_arrays / 2 \\
  split\_factor = split\_factor + 1
}

// Compute unroll-and-jam factor for \emph{op-vect} \\
n\_regs\_avail = n\_regs - (n\_outer\_arrays + n\_consts)\\
uaj\_factor = $\lceil$n\_reg\_avail / n\_inner\_arrays$\rceil$ \\
\eIf{n\_outer\_arrays $>$ n\_inner\_arrays}{
  permute = \KwSty{True}
}{
  permute = \KwSty{False}
}
\Return{$<$permute, split\_factor, uaj\_factor$>$}
\caption{The cost model is employed by the compiler to estimate the most suitable unroll-and-jam (when \emph{op-vect} is used) and split factors, avoiding the overhead of auto-tuning.}
\label{algo:applyCostModel}
\end{algorithm}


\section{Performance Evaluation}
\label{sec:perf-results}

\subsection{Experimental Setup}

Experiments were run on two Intel architectures, a Sandy Bridge (I7-2600 CPU, running at 3.4GHz, 32KB L1 cache and 256KB L2 cache) and a Xeon Phi (5110P, running at 1.05Ghz in native mode, 32KB L1 cache and 512KB L2 cache). We have chosen these two architectures because of the differences in the number of logical registers and SIMD lanes, which can impact the effectiveness of the optimization strategy. All experiments are sequential. The \texttt{icc 13.1} compiler was used. On the Sandy Bridge, the compilation flags used were \texttt{-O2} and \texttt{-xAVX} for auto-vectorization. On the Xeon Phi, optimization level \texttt{-O3} was used. Other optimization levels performed, in general, slightly worse. Our code transformations were evaluated in three real-world problems based on the following PDEs:
\begin{itemize}
\item Helmholtz
\item Advection-Diffusion
\item Burgers
\end{itemize}
The code was written in UFL (available at~\cite{ufl-code}) and then executed over real unstructured meshes through Firedrake. The Helmholtz code has already been shown in Figure~\ref{code:helmholtz}. For Advection-Diffusion, the Diffusion equation, which uses the same differential operators as Helmholtz, is considered. In the Diffusion kernel, the main differences with respect to Helmholtz are the absence of the $Y$ array and the presence of a few more constants for computing the element matrix $A$. Burgers is a non-linear problem employing differential operators different from those of Helmholtz, which has a major impact on the generated assembly code (see Figure~\ref{code:burgers}), where a larger number of basis function matrices ($X1$, $X2$, ...) and constants ($F0$, $F1$, ..., $K0$, $K1$, ...) are needed. 

These problems were studied varying both the shape of mesh elements and the polynomial order $p$ of the method, whereas the element family, Lagrange, is fixed. As might be expected, the larger the element shape and $p$, the larger the iteration space. Triangles, tetrahedron, and prisms were tested as element shape. For instance, in the case of Helmholtz with $p=1$, the size of the $j$ and $k$ loops for the three element shapes is, respectively, $3$, $4$, and $6$. Moving to bigger shapes has the effect of increasing the number of basis function arrays, since, intuitively, the behavior of the equation has now to be approximated also along a third axis. On the other hand, the polynomial order affects only the problem size (the three loops $i$, $j$, and $k$, and, as a consequence, the size of $X$ and $Y$ arrays). A range of polynomial orders from $p=1$ to $p=4$ were tested; higher polynomial orders are excluded from the study because of current Firedrake limitations. In all these cases, the size of the element matrix rarely exceeds 30$\times$30, with a peak of 105$\times$105 in Burgers with prisms and $p=4$.

\subsection{Impact of Generalized Loop-invariant Code Motion}
\newcommand{\massmatrixtwodresultsnorms}{
\begin{tabularx}{1.0\textwidth}{p{3.5cm}p{3.5cm}|p{2.25cm}p{2.25cm}p{2.25cm}p{2.25cm}|}
\cline{3-6}
& & \multicolumn{2}{c|}{\texttt{Sandy Bridge}} & \multicolumn{2}{c|}{\texttt{Xeon Phi}}\\
\cline{3-6}
& & \multicolumn{1}{c}{\texttt{licm}} & \multicolumn{1}{c|}{\texttt{licm-ap}} & \multicolumn{1}{c}{\texttt{licm}} & \multicolumn{1}{c|}{\texttt{licm-ap}} \\
\cline{1-6}
%\multicolumn{1}{|c|}{\texttt{Helmholtz}} & \multicolumn{1}{c|}{\texttt{triangle}} & ~~1.34$\times$-4.32$\times$ & ~1.34$\times$-4.32$\times$ & ~~1.34$\times$-4.32$\times$ & ~1.34$\times$-4.32$\times$ \\
\multicolumn{1}{|c|}{\texttt{Helmholtz}} & \multicolumn{1}{c|}{\texttt{triangle}} & ~~1.05$\times$-1.68$\times$ & \multicolumn{1}{c|}{~1.32$\times$-4.14$\times$} & ~~1.15$\times$-1.70$\times$ & ~~1.54$\times$-3.27$\times$ \\
\multicolumn{1}{|c|}{\texttt{Helmholtz}} & \multicolumn{1}{c|}{\texttt{tetrahedron}} & ~~1.36$\times$-2.64$\times$ & \multicolumn{1}{c|}{~1.35$\times$-3.32$\times$} & ~~1.18$\times$-1.87$\times$ & ~~1.27$\times$-2.92$\times$ \\
\multicolumn{1}{|c|}{\texttt{Helmholtz}} & \multicolumn{1}{c|}{\texttt{prism}} & ~~2.16$\times$-2.45$\times$ & \multicolumn{1}{c|}{~2.42$\times$-2.63$\times$} & ~~0.96$\times$-1.64$\times$ & ~~1.84$\times$-1.72$\times$ \\
\cline{1-6}
\multicolumn{1}{|c|}{\texttt{Diffusion}} & \multicolumn{1}{c|}{\texttt{triangle}} & ~~1.09$\times$-1.64$\times$ & \multicolumn{1}{c|}{~1.37$\times$-4.28$\times$} & ~~1.13$\times$-1.22$\times$ & ~~0.92$\times$-2.79$\times$ \\
\multicolumn{1}{|c|}{\texttt{Diffusion}} & \multicolumn{1}{c|}{\texttt{tetrahedron}} & ~~1.30$\times$-3.11$\times$ & \multicolumn{1}{c|}{~1.41$\times$-3.82$\times$} & ~~1.08$\times$-2.11$\times$ & ~~1.10$\times$-5.53$\times$ \\
\multicolumn{1}{|c|}{\texttt{Diffusion}} & \multicolumn{1}{c|}{\texttt{prism}} & ~~1.81$\times$-2.70$\times$ & \multicolumn{1}{c|}{~2.55$\times$-3.13$\times$} & ~~1.04$\times$-2.74$\times$ & ~~1.73$\times$-4.14$\times$ \\
\cline{1-6}
\multicolumn{1}{|c|}{\texttt{Burgers}} & \multicolumn{1}{c|}{\texttt{triangle}} & ~~1.53$\times$-2.46$\times$ & \multicolumn{1}{c|}{~1.56$\times$-2.77$\times$} & ~~1.19$\times$-2.97$\times$ & ~~2.70$\times$-4.27$\times$ \\
\multicolumn{1}{|c|}{\texttt{Burgers}} & \multicolumn{1}{c|}{\texttt{tetrahedron}} & ~~1.58$\times$-2.24$\times$ & \multicolumn{1}{c|}{~1.59$\times$-2.10$\times$} & ~~1.03$\times$-1.21$\times$ & ~~1.31$\times$-1.55$\times$ \\
\multicolumn{1}{|c|}{\texttt{Burgers}} & \multicolumn{1}{c|}{\texttt{prism}} & ~~1.32$\times$-2.11$\times$ & \multicolumn{1}{c|}{~1.42$\times$-2.31$\times$} & ~~1.04$\times$-1.40$\times$ & ~~1.27$\times$-2.18$\times$ \\
\cline{1-6}
\end{tabularx}
}

\begin{table*}[t]\small
\massmatrixtwodresultsnorms
~
\caption{Performance improvement due to generalized loop-invariant code motion (\emph{licm} column) on the Helmholtz, Diffusion and Burgers problems, varying the element shape (triangle, tetrahedron, prism) and the polynomial order ($p \in [1, 4]$), using the Lagrange element family, over the original, non-optimized code. Each entry indicates the minimum and maximum speedup obtained in the range of $p$ values. The column \emph{licm-ap} illustrates the combination of \emph{licm} with data alignment and padding. Results are shown for both the Sandy Bridge and the Xeon Phi architectures.}
\label{table:perf-results-licm}
\end{table*}

Table~\ref{table:perf-results-licm} illustrates the performance improvement obtained on the Sandy Bridge and Xeon Phi machines when \emph{licm}, data alignment, and padding are used. We distinguish between \emph{licm} and \emph{licm-ap}; the latter makes use of padding and data alignment. Inspection of assembly code generated by \texttt{icc} confirmed all limitations described in Section~\ref{sec:code-transf-licm}: only sub-expressions invariant to the inner loop are hoisted and their execution is not vectorized. Padding and data alignment enhance, in general, the quality of auto-vectorization. Sometimes the run-time of \emph{licm-ap} is similar to that of \emph{licm} because the element matrix size is already a multiple of the vector length, so no scalar reminder loop has to be introduced. Occasionally \emph{licm-ap} is slower than \emph{licm} (e.g. in Burgers 3D p3 on the Sandy Bridge). One possible explanation is that the large number of aligned temporaries introduced by \emph{licm} induce cache associativity conflicts.

\subsection{Impact of Vector-register Tiling}
\label{sec:perf-eval-opvect}
Figures~\ref{fig:opvect-helmholtz-speedup} and~\ref{fig:opvect-diffusion-speedup} show the performance achieved applying \emph{op-vect} on top of \emph{licm-ap} to the Helmoltz and Diffusion kernels on the Sandy Bridge, whereas Figures~\ref{fig:opvect-helmholtz-speedup-phi} and~\ref{fig:opvect-diffusion-speedup-phi} illustrate analogous results for the Xeon Phi. For each problem instance we report two bars: one indicates the best run-time obtained by auto-tuning the unroll/unroll-and-jam factors, whereas the other shows the result retrieved with COFFEE's cost model. In general, there is no substantial difference between the two. This is chiefly because these kernels fit the L1 cache, so, within a certain degree of confidence, it is possible to predict the fastest implementation based on register allocation.

The rationale behind these results is that, for smaller configurations, vector-register tiling is marginally helpful, while its impact is the dominant factor for larger problems. This is because in the considered kernels the number of accessed arrays along the innermost loop dimension is rather small (between 2 and 4), so extensive unrolling is often enough to maximize register re-use when the loops are relatively short. On the other hand, as the iteration space becomes bigger, vector-register tiling leads to percentage improvements up to 1.4$\times$ on the Sandy Bridge (Diffusion, prismatic mesh, $p=4$) and up to 1.4$\times$ on the Xeon Phi (Helmholtz, tetrahedral mesh, $p=3$). Using the Intel Architecture Code Analyzer tool~\cite{IACA} on the Sandy Bridge, we confirmed that this is a consequence of increased register re-use. In Helmholtz $p=4$, for example, the tool showed that when using \emph{op-vect} the number of clock cycles to execute one iteration of the $j$ loop decreases by roughly 17$\%$, and that this is a result of the relieved pressure on both of the data (cache) ports available in the core.

On the Sandy Bridge, we have also measured the performance of individual kernels in terms of floating-point operations per second. The theoretical peak on a single core, with the Intel Turbo Boost technology activated, is 30.4 GFlop/s. In the case of Diffusion using a prismatic mesh and $p=4$, we achieved a maximum of 21.9 GFlop/s with \emph{op-vect} enabled, whereas 16.4 GFlop/s was obtained when only \emph{licm-ap} is used. This result is in line with the expectations: analysis of assembly code showed that, in the \texttt{jk} loop nest, which in this problem represents the bulk of the computation, 73$\%$ of instructions are actually floating-point operations.

Application of \emph{op-vect} to the Burgers problem induces significant slow downs due to the large number of temporary arrays that need to be tiled, which exceeds the available logical registers on the underlying architecture. Expression splitting can be used in combination with \emph{op-vect} to alleviate this issue; this is discussed in the next section.

\begin{figure}[h]
\includegraphics[scale=0.7]{Pictures/helmholtz-normalized-opvect.pdf}
\caption{Performance improvement obtained applying \emph{op-vect} on top of \emph{licm-ap} to the Helmholtz kernel on the Sandy Bridge. CM refers to where the cost model has been used to determine the sequence of code transformations.}
\label{fig:opvect-helmholtz-speedup}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.7]{Pictures/helmholtz-normalized-opvect-phi.pdf}
\caption{Performance improvement obtained applying \emph{op-vect} on top of \emph{licm-ap} to the Helmholtz kernel on the Xeon Phi. CM refers to where the cost model has been used to determine the sequence of code transformations.}
\label{fig:opvect-helmholtz-speedup-phi}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.7]{Pictures/diffusion-normalized-opvect.pdf}
\caption{Performance improvement obtained applying \emph{op-vect} on top of \emph{licm-ap} to the Diffusion kernel on the Sandy Bridge. CM refers to where the cost model has been used to determine the sequence of code transformations.}
\label{fig:opvect-diffusion-speedup}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.7]{Pictures/diffusion-normalized-opvect-phi.pdf}
\caption{Performance improvement obtained applying \emph{op-vect} on top of \emph{licm-ap} to the Diffusion kernel on the Xeon Phi. CM refers to where the cost model has been used to determine the sequence of code transformations.}
\label{fig:opvect-diffusion-speedup-phi}
\end{figure}

%for which efficient register allocation can be already guaranteed

\subsection{Impact of Expression Splitting}
\label{sec:perf-results-split}
Expression splitting relieves the register pressure when the element matrix evaluation reads from a large number of arrays, at the price of increasing accesses to the element matrix itself and potentially affecting data locality, as detailed in Section~\ref{sec:expr-split}. It is not particularly useful, therefore, for Helmholtz and Diffusion, where only between 4 and 8 basis functions or temporaries need to be read at every iteration. Slow downs up to 1.4$\times$ and up to 1.6$\times$ were observed, respectively, on the Sandy Bridge and the Xeon Phi. The cost model, however, prevents the adoption of the transformation: the \texttt{while} statement at line 5 in Figure~\ref{algo:applyCostModel} is indeed never entered. For the Burgers kernel, \emph{split} has a key role on the Sandy Bridge, whereas it has only a minimal impact on the Xeon Phi. Figure~\ref{fig:split-burgers-speedup} and~\ref{fig:split-burgers-speedup-phi} show the improvement due to using \emph{split} over the fastest implementation between \emph{licm} and \emph{licm-ap}. On the Sandy Bridge, in almost all test cases, a split factor of 1, meaning that the original expression was divided into two parts, ensured close-to-peak perforance. The transformation negligibly affected register locality. On the contrary, the larger number of vector units present on the Xeon Phi relieve register spilling, making \emph{split} only marginally useful for loop unrolling. 

The \emph{split} transformation was also tried in combination with \emph{op-vect} (\emph{split-op-vect}), although the cost model prevents its adoption on both platforms. Despite improvements up to 1.22$\times$, \emph{split-op-vect} never outperforms \emph{split}. This is for two reasons: for small split factors, such as 1 and 2, the data space to be tiled is still too big, and register spilling affects run-time; for higher ones, sub-expressions become so small that, as explained in Section~\ref{sec:perf-eval-opvect}, full or partial unrolling of the loop nest achieves a certain degree of register re-use by itself.

\begin{figure}[h]
\includegraphics[scale=0.7]{Pictures/burgers-normalized-split.pdf}
\caption{Performance improvement obtained applying \emph{op-vect} on top of \emph{licm-ap} to the Burgers kernel on the Sandy Bridge. CM refers to the cases in which the cost model has been used to determine the sequence of code transformations.}
\label{fig:split-burgers-speedup}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.7]{Pictures/burgers-normalized-split-phi.pdf}
\caption{Performance improvement obtained applying \emph{op-vect} on top of \emph{licm-ap} to the Burgers kernel on the Xeon Phi. CM refers to the cases in which the cost model has been used to determine the sequence of code transformations.}
\label{fig:split-burgers-speedup-phi}
\end{figure}

\subsection{Comparison with FEniCS Form Compiler's built-in Optimizations}
\label{sec:perf-results-fenics}
We have modified the FEniCS Form Compiler (FFC) to make it return an abstract syntax tree representation of a local assembly kernel, rather than plain C++ code, so as to enable code transformations in COFFEE. Besides Firedrake, FFC is used in the FEniCS project~\cite{Fenics}. In FEniCS, FFC can apply its own model-driven optimizations to local assembly kernels~\cite{quadrature1}, which mainly consist of loop-invariant code motion and elimination, at code generation time, of floating point operations involving zero-valued entries in basis function arrays. The FEniCS Form Compiler's loop-invariant code motion is, however, different from COFFEE's, since it is based on expansion of arithmetic operations, for example applying distributivity and associativity to products and sums at code generation time, to identify terms that are invariant of the whole loop nest. In addition, depending on the way expansion is performed, operation count often does not decrease significantly. On the other hand, elimination of zero-valued terms, which are the result of certain properties of the finite element problem, has the effect of introducing indirection arrays in the generated code. This kind of optimization is currently under development in COFFEE, although it will differ from FEniCS' by avoiding non-contiguous memory accesses, which would otherwise affect vectorization, at the price of removing fewer zero-valued contributions.

Table~\ref{table:comparison-to-FFC-opt} summarizes the performance achieved by COFFEE over the best FEniCS (FFC) implementation (i.e. the combination of optimizations for each problem which results in the fastest code) on the Sandy Bridge for the Burgers, Helmholtz and Diffusion kernels. Burgers' slow downs occur in presence of a small iteration space (triangular mesh, $p \in [1, 2]$; tetrahedral mesh, $p \in [1, 2]$; prismatic mesh, $p = 1$). The figure shown is for the worst slow down, which was obtained with a triangular mesh and $p = 1$. This is a result of removing zero-valued entries in FEniCS' basis functions: some operations are avoided, but indirection arrays prevent auto-vectorization, which significantly impacts performance as soon as the element matrix becomes bigger than 12$\times$12. However, with the forthcoming zero-removal optimization in COFFEE, we expect to outperform FEniCS in all problems. In the cases of Helmholtz and Diffusion, the minimum improvements are, respectively, 1.10$\times$ and 1.18$\times$ (2D mesh, $p=1$), which tend to increase with polynomial order and element shape up to the values illustrated in the table.

\begin{table}[h]
\begin{center}
\begin{tabulary}{1.0\columnwidth}{|C|C|C|}
\hline
Problem & Max slow down & Max speed up \\\hline\hline
Helmholtz & / & 4.14$\times$ \\
Diffusion & / & 4.28$\times$ \\
Burgers & 2.24$\times$ & 1.61$\times$ \\\hline
\end{tabulary}
\end{center}
\caption{Performance comparison between FEniCS (optimizations enabled) and COFFEE on the Sandy Bridge.}
\label{table:comparison-to-FFC-opt}
\end{table}

\subsection{Comparison with hand-made BLAS-based implementations}
\label{sec:perf-eval-blas}
For the Helmholtz problem on a tetrahedral mesh, manual implementations based on \texttt{Intel MKL BLAS} were tested on the Sandy Bridge. This particular kernel can be easily reduced to a sequence of four matrix-matrix multiplies that can be computed via calls to BLAS \texttt{dgemm}. In the case of $p=4$, where the element matrix is of size 35$\times$35, the computation was almost twice slower than the case in which \emph{licm-ap} was used, with the slow down being even worse for smaller problem sizes. These experiments suggest that the question regarding to what extent linear algebra libraries or, analogously, auto-tuning strategies can improve performance cannot be trivially answered. This is due to a combination of issues: the potential loss in data locality as exposed in Section~\ref{sec:expr-split}, the actual effectiveness of external tools when the arrays are relatively small, and the problem inherent to assembly kernels concerning extraction of matrix-matrix multiplies from static analysis of the kernel's code. A comprehensive study of these aspects will be addressed in further work. 


\section{Related Work}
\label{sec:related-work}
The finite element method is extensively used to approximate solutions of PDEs. Well-known frameworks and applications include nek5000~\cite{nek5000-web-page}, the FEniCS project~\cite{Fenics}, Fluidity~\cite{fluidity_manual_v4}, and of course Firedrake. Numerical integration is usually employed to implement the local assembly phase. The recent introduction of DSLs to decouple the finite element specification from its underlying implementation facilitated, however, the development of novel approaches. Methods based on tensor contraction~\cite{FFC-TC} and symbolic manipulation~\cite{Francis} have been developed. We have designed COFFEE to specifically target numerical integration because it has been demonstrated that it remains the optimal choice for a wide class of problems~\cite{quadrature1}.

Optimization of local assembly using numerical integration for CPU platforms has been addressed in FEniCS~\cite{quadrature1}. The comparison between COFFEE and this work is presented in Section~\ref{sec:perf-results-fenics}. In~\cite{Markall20101815}, and more recently in~\cite{petsc-integration-gpu}, the problem has been studied for GPU architectures. In~\cite{assembly-opencl}, variants of the standard numerical integration algorithm have been specialized and evaluated for the PowerXCell processor, but an exhaustive study from the compiler viewpoint - like ours - is missing, and none of the optimizations presented in Section~\ref{sec:code-transf} are mentioned. Among these efforts, to the best of our knowledge, COFFEE is the first work targeting low-level optimizations through a real compiler approach.

Our compiler-based optimization approach is made possible by the top-level DSL, which enables automated code generation. DSLs have been proven successful in auto-generating optimized code for other domains: Spiral~\cite{Pueschel:05} for digital signal processing numerical algorithms, ~\cite{Spampinato:14} for dense linear algebra, or Pochoir~\cite{pochoir} and SDSL~\cite{stencil-compiler} for image processing and finite difference stencils. Similarly, PyOP2 is used by Firedrake to express iteration over unstructured meshes in scientific codes. COFFEE improves automated code generation in Firedrake.

Many code generators, like those based on the Polyhedral model~\cite{PLUTO} and those driven by domain-knowledge~\cite{modeldriven}, make use of cost models. The alternative of using auto-tuning to select the best implementation for a given problem on a certain platform has been adopted by nek5000~\cite{nek5000} for small matrix-matrix multiplies, the ATLAS library~\cite{ATLAS}, and FFTW~\cite{FFTW} for fast fourier transforms. In both cases, pruning the implementation space is fundamental to mitigate complexity and overhead. Likewise, COFFEE uses a cost model and heuristics (Section~\ref{sec:pyop2-compiler}) to steer the optimization process. 

%(i.e. given a mesh, those that lead to ``less accurate'' solutions than when using high-order polynomials, at a lower computational cost)

%Show other work of people working on optimizations for computational science kernels. Inter-kernel vectorisation paper from Istvan. Previous work in FFC. Spencer et-al + Shin et. al. attempt to optimize computations that could benefit from using BLAS, but in practice they don't, due to the very small dgemm operations employed. Saday's model-driven SIMD code vectorisation for the tensor contraction engine.


\section{Conclusions}
\label{sec:conclusions}

In this paper, we have presented design, optimizations and systematic performance evaluation of COFFEE, a compiler for finite element local assembly. In this context, to the best of our knowledge, COFFEE is the first compiler capable of introducing low-level optimizations to maximize instruction-level parallelism, register locality and SIMD vectorization. Assembly kernels have peculiar characteristics. Their iteration space is usually very small, with the size depending on aspects like the degree of accuracy one wants to reach (polynomial order of the method) and the mesh discretization employed. The data space, in terms of number of arrays and scalars required to evaluate the element matrix, grows proportionally with the complexity of the finite element problem. COFFEE has been developed taking into account all of these degrees of freedom, based on the idea that reducing the problem of local assembly optimization to a fixed sequence of transformations is far too superficial if close-to-peak performance needs to be reached. The various optimizations overcome limitations of current vendor and research compilers. The exploitation of domain knowledge allows some of them to be particularly effective, as demonstrated by our experiments on two state-of-the-art Intel platforms. Further work include a comprehensive study about feasibility and constraints on transforming kernels into a sequence of calls to external linear algebra libraries. COFFEE supports all of the problems expressible in Firedrake, and it is already integrated with this framework.



% use section* for acknowledgement
\section*{Acknowledgment}

This research is partly funded by the MAPDES project, by the Department of Computing at Imperial College London, by EPSRC through grants EP/I00677X/1, EP/I006761/1, and EP/L000407/1, and by NERC grants NE/K008951/1 and NE/K006789/1. The authors would like to thank Dr. Carlo Bertolli, Dr. Lawrence Mitchell, and Dr. Francis Russell for their invaluable suggestions and their contribution to the Firedrake project.



\bibliographystyle{plain}	% (uses file "plain.bst")
\bibliography{biblio}		% expects file "myrefs.bib"



% that's all folks
\end{document}
