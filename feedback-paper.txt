CARLO

Hi,

First of all, thanks for sharing this - it is a highly practical yet well thought paper on an extremely critical subject.
In terms of contributions, it should be highlighted in the paper that the generality of the structure of the studied kernels enables indeed simple transformations, but these can be conversely applied to a large set of programs, apart from fenics/computational science - that is, simplicity is key to large applicability and, as you show, convincing performance improvements. I would really like to see a commercial compiler able to detect the cases you identify and apply your transformations, possibly not in a distant future.  On the other hand, very complex transformations for extremely specific cases may deliver incredibly good (read my lips) performance improvements, but no way commercial compilers will consider those corner cases. You study the tension between these two opposites, leaning towards the former, in a successful manner. Maybe I am biased.

Some comments that may be useful:

Introduction:

- To address this problem, domain-specific languages (DSLs) have been developed.

As you know, DSL are developed and used to both offer a high level model to computational scientists and give full opportunity to the software stacks that underly them to introduce optimizations.
Probably a rephrasing of this would be:  "DSL have been developed to offer a high level model to computation scientists, nearer to their ever-day abstractions, and to feed more information on the program to the underlying software stack, often with optimizations in mind." Or something like this, you write the actual text :-)


- with an implicit synchronization between the application of two consecutive kernels.

I am not sure this is important here - it may be more important to tell the reader that OP2 offers a parallel loop construct with access descriptors.


- it might need L2 cache when high-order methods are employed to improve the accuracy of the solution. However, we do not consider the latter case.

I am not sure about where the results point, but I thought that some transformations were more successful for high polynomial orders. This sentence seems to suggest that you are throwing out of the window the main optimization: I suggest rephrasing it to meet your needs.

- (the trip count rarely exceeds 30,

Is this related to a single loop or to the total trip count of the loop nest?

- With such small kernels, we focus on aspects like minimization of floating-point operations, register allocation and instruction- level parallelism, especially in the form of SIMD vectorization.

Does this complement data locality techniques e.g. targeting L2 cache? Is that trivial to do? What's the relationship?
This does not require any fixing in the paper, but it may be a question asked by a referee.

- can result in performance improvements up to 1.5x
over “softly-optimized” code (i.e. where only basic transfor- mations are performed, such as generalized loop-invariant code motion, padding, and data alignment), and up to 4.44x over original kernels.

Any idea of what a "optimal" improvement would be? This would put things into context. I am asking because, sometime, it is possible to see how good an optimization is related to hardware factors. For instance, in a 4 lane vector SIMD you'd ideally would like to see 4x improvement between a vectorized version and the corresponding un-vectorized one.

- Systematic evaluation using a suite of examples of 24
real-world importance, and evidence of significant performance improvements.

On what machines?

Background

- floatin point values

floatin*g*

-  (i.e. using static const in C language).

why? (I may know, but you should spell it out)

- 14 unique arrays

unique?

- called F in the code

A constant called F and a function called K?

- Despite assembly kernels being problem-dependent, meaning that the space of codes that Firedrake can generate is infinite, it is still possible to identify common domain-specific traits, which can be exploited for effective code transformations and SIMD vectorization.


Perhaps you may want to say that your compiler infrastructure targets a large and relevant subset of the kernels that can be generated by firedrake?
The paragraph that follows does not help in this direction: why are you not considering those other cases? You also mention that "This is not true in general, since bent elements might be used in the unstructured mesh, which would require the Jacobian be re-computed at every i iteration": what would happen to your transformation for these cases? How relevant are these cases?

Section 3:

- The code transformations presented in this section are applicable to all finite element problems that can be formulated in Firedrake.

But...you just said that you don't...I am confused.


Section 3D

This transformation looks like something that an off-the-shelf compiler may be able to do easily. However, the goal of this paper is not to show extremely complex (sequences of) transformations, but what relatively known ones can be composed successfully for the studied kernels and architectures. I think this comment should be highlighted at the beginning of section 3 (so that no one is going to complain about the relatively straightforwardness of the single transformations).

 After 3D

- we need a mechanism to prune such a large space of optimization.

This is the key I was trying to highlight above.

Section 4:

-  Autotuning might be used, although at the moment we avoid it to minimize the run- time overhead.

It is unclear why autotuning should impact run-time (I know the reason for this, but most compilers' people don't, so be super clear here).

- the transformed code seems to prevent the intel compiler from applying this optimization

What about gcc?

- The cost model is shown in Figure 8.

Figure 8 is Helmoltz assembly code. Figure 10 looks a better pick for this reference.

- Figure 10 label: The cost model is employed

Remove "The" and "is".

- Finally, the profitability of loop interchange is evaluated (line 12-16)

Missing full stop at the end of sentence.

Section 5

- You have to include phi results (!!): having two variations over the same (lousy) architecture will help delivering the message.

At this point it is just too late and I had too many beers - I will continue tomorrow.
In the meantime, as usual, I hope this helps.


Thanks


===================================================================================================
