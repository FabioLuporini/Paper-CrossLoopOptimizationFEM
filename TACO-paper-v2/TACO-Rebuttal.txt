We thank our reviewers (R1-R4). We appreciate the positive feedback and the
critical comments on our paper. Below we answer the various points raised by
the reviewers.  We have addressed many minor issues, and we have added two
substantial new sections - an application case study (§5.3) and an extended
discussion of the generality of the work (§6).

R1: We agree on the need for more specific discussion of the generality of our
approach and the applicability of the code transformations to other
computational domains (or, perhaps, their integration within a general-purpose
compilers). We therefore added a complete new section (Section 6) that explains
these aspects in detail. This should help readers understand how to transfer
our work to a different domain or compiler.

We have also addressed all of the other very comments (1-8) provided: 
(1) “a compiler is the only reasonable option for obtaining close-to-peak performance
for these loops, but you don't say what other options were considered nor what
their downsides are”: 
The goal is to develop a class of optimizations for an
infinite variety of local assembly codes. In that paragraph we say "(...) makes
it hard to determine a single or specific sequence of transformations that is
successfully applicable to all problems (...)", which implies that different
finite element problems may require different code optimizations due to the
inherent diversity in mathematical expressions and loop trip counts (this
aspect is reiterated throughout the paper). Therefore, a compiler-based
approach, in which the input and some parameters are analyzed to decide how to
optimally transform the code is needed to maximize the performance. Another
possibility would be a library-based approach, although it would be quite
pointless since we can already generate the most suitable code for each
problem. 

2) On page 6, why do varying iteration counts for your loops make
optimization challenging?  If your loop trip count is small enough, why not
just unroll a number of times to remove this loop?” 
The variation in loop trip counts impacts loops transformation. If the loop is relatively large, for
example, vector-register tiling is likely to be useful to minimize register
spilling; if it is small, then full or partial unrolling is an option.
Understanding the border between "small" and "large" is challenging.  The cost
model helps in this direction. We tried to address the point you make in
various sections in the paper: the sentence at page 6 has been re-written to
anticipate the fact that "different loop sizes require different
transformations". Then we elaborate on the importance and the impact of loop
unrolling in Section 4 (there is a complete new paragraph about that).
Finally, in Section 5.2.5, we demonstrate our claim when discussing the impact
of vector-register tiling (paragraphs 4, 5 and 6 of that section) in our
benchmarks. 

3) “Do you mean that COFFEE produces this pragma for the C
iompiler?“: 
Clarified in the paper by adding/re-writing a few sentences. The short answer is: yes. 

4) “a large optimization space, but never actually give an indication of its size”: 
Explained in the paper.

5) “picture rather than the code listing in figure 5“: 
We agree. We removed the code listing and expanded Figure 4. 

6) “two architectures...differences”: 
Explained in the paper. 

7) “different compilation flags”: 
This was already explained by the sentence "Other optimization levels performed, in general, slightly worse." 

8) “Tables III and IV would definitely be more readable as charts”: 
We believe each of those tables can be represented by 6 different charts (1 for each
<Architecture, Equation> examined>). We have not done that for lack of space
(the submission for acceptance is limited to 25 pages) but we can easily do
that, if really needed, in the final version of the paper (which can be 30-page
long)

******************************************************************************

R2: Two main critiques are offered on our paper.

a) “the techniques of the paper are mostly interesting to a subset of the
people in the intersection of code optimization and finite elements methods.”:

We do not think that "the techniques of the paper are mostly interesting to a
subset of the people in the intersection of code optimization and finite
element methods". However, we do agree that the paper was missing an in-depth
explanation about how to re-adapt/re-use our transformations in other contexts,
perhaps other DSLs or even general-purpose compilers. Also, we believe that our
experience of developing COFFEE and integrating it in a real-world framework
based on a domain-specific language can be of inspiration for other research
and/or engineering work. We therefore introduced a complete new section,
Section 6, about the generality of our approach and the applicability of the
code transformations to other computational domains. Your comments have
prompted us to think pretty hard about this, and we think this adds
significantly to the paper’s contributions. 

b) “it is not clear if the results of the paper are applicable to other kernels
beyond the ones analysed and presented” and “The paper appears to be a
collection of optimizations that work for these 3 kernels but is not clear how
broad these techniques are and if they will be effective for other kernels. It
would make sense to at least include a number of kernels in the study from real
applications and use some of them as a black box in the evaluation section.
Kernels are small pieces of code, so presumably there will be many of them
available. If on the other hand there is only a limited number of kernels
available, then why not optimize them directly by hand.”

The results of the paper are *really* applicable to other kernels beyond those
analyzed and presented. First of all, we want to emphasize a number of aspects.
What is important to clarify is that kernels are not "mere small pieces of
code", but rather, in general, quite complex numerical routines evaluating
integrals in an element of the discretized equation domain, produced by means
of real (used by many scientists around the world) domain-specific language and
compiler.  There is an infinite variety of kernels that can be generated in
Firedrake; the generated code (e.g. loop sizes and mathematical expression)
depends on the partial differential equation being solved. This is explained
and re-iterated in the paper. Also, varying the polynomial order of the method
or the element shape (e.g. triangles, tetrahedrons, prisms), as we do and
explain in the performance evaluation section, changes the generated code by
increasing loops and arrays sizes and by making mathematical expressions more
articulated (this happens, for example, when moving from 2D to 3D domains). So
we are already testing many different code variants (three different kernels,
and then for each kernel we try multiple polynomial orders and element shapes).
This is carefully explained in Section 5.2.1. Premised this, we now go back to
your point about "applicability and effectiveness of our transformations in
other local assembly kernels". Our answer is twofold:

- Section 5.2.1 explains in-depth the motivations behind the choice of the
three benchmarks (Helmholtz, Diffusion, Burgers), explaining that "(...)
distinct problems, possibly arising in completely different fields, may employ
(subsets of) the same differential operators of our benchmarks, which implies
similarities and redundant patterns in the generated code.  Consequently, the
proposed code transformations have a domain of applicability that goes far
beyond that of the three analyzed equations. (...)". Then we reinforce this
later on in the section and in the paper. Other kernels arising from completely
different equations will still have the same structure of the ones we examined
(e.g. same memory access pattern, same ranges of loop sizes, same problems
concerning the presence of loop-invariant code and register pressure) as well
as similar patterns in the mathematical expressions, so there is no evident
reason for which our code optimizations would not be effective. The critical
issue in our research was to understand what transformations our local assembly
kernels required, and how to instantiate parametric transformations
(vector-register tiling, loop interchange, expression splitting) optimally in
each possible problem (this led to the development of the cost model).

- In Section 5.3 we provide a full-application study in which we analyze a new
equation (Linear Elasticity), which brings the total number of studied
equations to 4 (Helmholtz, Diffusion, Burgers, Elasticity). Adding more kernels
to the performance evaluation section would make the paper only more chaotic:
we specifically selected these three problems because they allow us to
highlight all pros and cons of our code transformations. This is also the
reason we comment on the performance of the code transformations (and their
interplay) in specific sub-sections (referring to, describing, and carefully
analyzing the provided plots), rather than in a single, centralized section. 

R2’s recommendations: * “It does not discuss and/or compare to other approaches
to improving the performance of finite element methods: (a) The authors briefly
comment on using linear algebra libs but then dismiss this approach as
requiring more work”

- Section 5.2.7 and Table V present a head-to-head performance comparison with
the FEniCS Form Compiler's built-in optimization system, which is very widely
used and, as indicated in the cited article, the state of the art.  Section
5.2.8 presents our experience with using MKL BLAS.

(b) It seems that some type of hardware support, e.g. graphics card, would be
more appropriate for floating point intensive applications. How does the
performance presented compare to a graphics card (or other) approach? Does it
make sense for someone to go down the path presented in this paper or just buy
and use one (or more) graphics card(s)?

This is now clarified in the introduction of the paper, paragrah 5. Here, we
give a more detailed explanation.

At various points in the paper (introduction and related work) we cite studies
concerning local assembly and GPUs. These works however deeply differ from ours
in a few ways: (i) that they do not rely on automated code generation, (ii)
they are limited to only very low-order methods (typically only polynomial
order 1), (iii) they do not try to optimize for cross-loop arithmetic intensity
as we do, and (iv) their performance comparisons with CPU implementations are
based on code which has not benefited from the optimizations in this paper. The
goal in these works is rather to understand how to effectively parallelize the
local assembly code (so as to have multiple threads executing a single kernel,
as opposed to our execution model in which we assume a single thread per
kernel) in specific equations (not in a possibly infinite range as we do).  A
critical issue when executing local assembly kernels on GPUs is represented by
memory requirements. We give here two examples of this problem, although more
issues could be found and discussed. Firstly, the transfer of the mesh from
host to device may be a great obstacle: if too big, the mesh will not fit the
available GPU memory, and distributed execution, which is marginally supported
in available finite element frameworks, would be required. Secondly,
optimizations such as generalized loop-invariant code motion increase the
memory requirements due to the introduction of temporary arrays.  This may
actually lead to poor per-thread shared memory and registers allocation. These
aspects suggest that the problem of executing generic, automatically-generated
local assembly kernels on GPUs require a separate, in-depth study. However, in
the new Section 6, we provide some intuitions of how we could re-adapt COFFEE's
transformations for execution on GPUs.  Therefore, we invite once more to go
through the new Section 6 carefully.


* “The speedup results presented refer to the kernels only. It would be better
to show the improvement on the execution time of the whole finite element
calculation, end-to-end”:

We agree it is useful to report speedups and execution times for the whole
finite element calculation. We have added a new section (Section 5.3) about a
full-application study, showing exactly these values.

* “Related work should cover the individual compilation techniques discussed
and how they are adapted in this work”:

A new paragraph has been added to the related work section addressing this
issue (Section 7, paragraph 3).

******************************************************************************

R3: We answer here the 5 points of the review.  (1) “The targeted computational
problems seem ideal for GPGPU computing. So why not target at optimizations of
the kernel functions for GPGPU architectures rather than Intel CPUs. In fact,
Sandy Bridge also contains a GPGPU.”

We replicate here the same answer to R2.

This is now clarified in the introduction of the paper, paragrah 5. Here, we
give a more detailed explanation.

At various points in the paper (introduction and related work) we cite studies
concerning local assembly and GPUs. These works however deeply differ from ours
in a few ways: (i) that they do not rely on automated code generation, (ii)
they are limited to only very low-order methods (typically only polynomial
order 1), (iii) they do not try to optimize for cross-loop arithmetic intensity
as we do, and (iv) their performance comparisons with CPU implementations are
based on code which has not benefited from the optimizations in this paper. The
goal in these works is rather to understand how to effectively parallelize the
local assembly code (so as to have multiple threads executing a single kernel,
as opposed to our execution model in which we assume a single thread per
kernel) in specific equations (not in a possibly infinite range as we do).  A
critical issue when executing local assembly kernels on GPUs is represented by
memory requirements. We give here two examples of this problem, although more
issues could be found and discussed. Firstly, the transfer of the mesh from
host to device may be a great obstacle: if too big, the mesh will not fit the
available GPU memory, and distributed execution, which is marginally supported
in available finite element frameworks, would be required. Secondly,
optimizations such as generalized loop-invariant code motion increase the
memory requirements due to the introduction of temporary arrays.  This may
actually lead to poor per-thread shared memory and registers allocation. These
aspects suggest that the problem of executing generic, automatically-generated
local assembly kernels on GPUs require a separate, in-depth study. However, in
the new Section 6, we provide some intuitions of how we could re-adapt COFFEE's
transformations for execution on GPUs.  Therefore, we invite once more to go
through the new Section 6 carefully.

2) “Using profile guided optimization has been very commonplace, even for some
commercial compilers. Some of the domain specific knowledge listed in this work
could be discovered by profile runs. For example, it is rather easy for a
profile run to tell that the trip counts are small, the kernel working set can
fit the L1 data cache, whether data alignment and padding are needed. This work
should discuss what domain knowledge would not be available from profiles and
thus call for domain specific transformations.”

We agree that profile-guided optimization could be used to improve the
performance of kernels, although this aspect is orthogonal to most of our code
transformations, because in no way profile information could suggest, for
example, to trigger generalized loop-invariant code motion and avoidance of
redundant computation (described in Section 3.2). Profile-guided optimization
could theoretically be used for padding and data-alignment of arrays, provided
that the compiler has sufficient machinery in place to perform (we quote the
revised paper, Section 6) "some sort of data-flow analysis (...) to ensure that
the extra loop iterations over the padded region do not affect the numerical
results.". We discuss on the generality of our code transformation and on their
integrability in a general-purpose compiler (and in its profile-guided
optimization system) in the new Section 6 of the paper, which also represents
novel contribution of our work.

3) “Compile time has always been an important issue. A domain specific compiler
should have clear advantages over general compilers. This issue has not been
discussed in the paper.”

We have added a new subsection (5.2.9) about the compilation time. In short,
compilation time is never a problem for COFFEE, since it usually takes order of
milliseconds, while the overall assembly process can take order of hours or
realistic meshes.

4) “This goal of this paper is to understand the relationship between distinct
code transformations, impact on local assembly kernels, and to what extent
their composibility is effective in a class of problems and architectures.
However, it is quite clear that the code transformations discussed in this work
is tightly coupled with Intel architectures with SSE/AVX SIMD extensions. It is
not clear whether such transformations are effective on other architectures
such as PPC AltiVec, MIPS, SPARC, ARM with Neon, and Vector computers such as
Cray. COFFEE generates C code, so in theory, the performance of the code will
be very dependent on the target architecture. Is Firedrake designed only for
Intel architectures? If yes, this has to be clarified in the paper.”

Firedrake is *not* specifically designed for Intel architectures and compilers.
The decision of using Intel platforms and tools is twofold: from one hand, we
believe that Intel's is one of the most powerful vectorizing and optimizing
compilers, so for research like ours it was important to compare to the best
available on the market; from the other hand, their wide availability
represents a strong incentive because users will be likely to run their
experiments on these machines.  It is also worth noting that all
general-purpose compilers are, to different extents, limited in optimizing for
register locality and code motion (for instance, this is true for the Intel and
GNU compilers, although we only used the latter for "local" experimentation),
so our work is expected to be equally/similarly effective on different yet
"standard" CPU architectures.  From the practical viewpoint, we observe that
COFFEE introduces optimizations aimed at improving cross-loop arithmetic
intensity by performing source-level transformations, so there are absolutely
no technical issues for using our domain-specific compiler on architectures
different than Intel. Actually, this fact is a strength of our work, because it
allows COFFEE to be almost completely platform-independent.  The only exception
regards intrinsics code: COFFEE is currently only capable of outputting AVX
intrinsics, so at the moment we cannot currently apply vector-register tiling
(the so-called outer-product vectorization technique) to other architectures.
However, COFFEE's internal algorithms use an intermediate representation of
intrinsics code, so extending our compiler to other instruction sets would be
just a matter of adding a new dictionary of intrinsics functions.

5) “Expression split has many issues involved: how to limit the number of
registers used, how to avoid incurring too many redundant operations, how to
minimize loop overhead, how to minimize the impact from reduced cache and
register locality. In this work, expression split is handled with a simple
heuristic. This is insufficient. Ideally, this problem should be modeled as a
graph partition problem with a cost model to select the best partitioning.
Perhaps in Firedrake, this is not a big issue that the performance of splitting
is insensitive. If this is the case, it should be addressed so that this work
could be qualified for a journal article.”

We agree that there are many issues involved. Our cost model offers a simple
alternative for driving expression splitting. Simplicity is the key to
effectiveness, portability, and robustness. Probably using a graph partitioning
approach as you rightly point out would allow us to obtain better performance
gains, but three points are worth highlighting: - The split factor is a
parameter of the transformation. In our experiments we have tested several
split factors, which means testing as many code variants (i.e. as many as the
ways an expression can be split based on sum's associativity) and, as shown by
the various graphs in the paper, our cost model seems to be capable of properly
approximating such a parameter. This is of course encouraging.  - In the
perspective of integrating expression splitting in a general-purpose compiler,
or generalizing this technique to other problems (as elaborated in the new
Section 6), we feel that a simple model is preferable to keep the
implementation complexity as low as possible.  - Graph partitioning for
expression splitting seems to us a complete new research problem. On the other
hand, this transformation is only one of the many applicable by COFFEE, and we
just do not want the focus of the paper being on "how to optimally and rapidly
split a mathematical expression". Rather, we prefer to show that, for our
problems, there is clearly a problem of register pressure and that by
rescheduling the computation in different loops (and sub-expressions) we can
actually obtain a performance gain. We have a strategy for expression
splitting; we are not claiming it is the best strategy, but at least we
demonstrate it is effective and easily predictable.  We spent some time
thinking about the problem: two things that we may want to try in the future
are 1) a model based on graph partitioning and 2) a model based on a set of
rewrite rules that exploit arithmetic properties such as commutativity,
associativity, and distributivity/factorization to systematically optimize the
arithmetic intensity of the expression.

Other comments: a) “Complete loop unwinding: in all the code examples, such as
Listing 1,2,3 and 4, the trip counts in the kernels are constants (such as 3,
5, 12, and 4). In ILP compilers, complete loop unwinding is often used to
increase ILP and enable more effective optimizations. Is it also used in
COFFEE?”

We elaborate on the use of loop unwinding (or unrolling) in Section 4, in which
we have added a new paragraph (named "Loop unrolling"). This paragraph
elaborates on "complete/partial loop unwinding" as well as other general
aspects concerning pros and cons of applying this transformation.

b) “On page 14 and 15, in Table III and IV (...) any reason why such
inconsistency exist?”

It was copy-paste leftover, now fixed

c) “Padding, LICM (not as general, though) are supported in vendor compilers.
Some forms of expression splitting are also supported to control register
pressure and compile time. The quoted statement (...) is too strong.”

Right, fixed by making the statement less strong

d) “COFFEE is currently implemented as a stage between FFC and PyOP2. Could you
explain in the paper why COFFEE and PyOP2 is not integrated. Current
transformations in COFFEE do not have direct control on the final code to be
executed on the target machine.”

This is all clarified and explained in details in the new Section 6. The short
answer is that we hope COFFEE can be re-used in frameworks other than Firedrake
(e.g. the FEniCS framework that we compare to in the paper). 

******************************************************************************

R4: We addressed all of the observations in your review:

1) TYPO page 3

Fixed, thanks

2) “What is a "polyhedral compiler"?”

The polyhedral model is well-known among compilers people. Basically, it is a
mathematical model for representing affine loop nests and determining in a
systematic way all of its possible legal transformations. Cost models are often
used by polyhedral compilers to estimate the effectiveness of a certain
transformation. Polyhedral optimizations often aim at improving cache locality.
In the Related Work section, we cite one of the most relevant researches on
polyhedral compilers; note that one of the co-authors of our paper is also a
co-author of that paper (i.e. he is an expert in polyhedral compilers).

3 and 4) “Which exact timings are used for measuring the reported speedups in
Section 5? Is it computing the local matrix? Assembling the full matrix? Or
solving the PDE? The speedups seem to be unreasonably good if they are not for
computing only the local matrix (if they also include sparse matrix insertion
and linear solve). Please explain and comment. I would also like to see some
numbers on how large a mesh was used (number of cells) and execution time in
seconds (not only speedups).” and “Related to the previous remark: if the
speedups are for calling the kernel, what is the corresponding (smaller - if
that is the case) speedup for the full assembly (including sparse matrix
insertion) and for the full solution process (including sparse matrix insertion
and linear solve)?”

The speed ups only refer to the computation of the local element matrix.  This
is now clarified in Section 5.1, and also re-iterated later on in the paper. We
used different meshes for the various experiments. In Section 5.2.1 we provide
a link to a repository in which the code of the experiments as well as the mesh
sizes employed can be found. We note that for these experiments the mesh size
was set such that the working set of the computation could at least fit the
last level cache (L3 on the Sandy Bridge architecture).  The goal of these
experiments (Section 5.2 in the revised paper) was only to measure speed ups
achieved by optimizing local assembly kernels, so there was no point in running
over big meshes (i.e. meshes that would not fit the last level cache), which
would have only increased the completion time of our test suite (which is
already order of hours). 

We however completely agree that reporting execution times and speedups of the
overall finite element calculation is of fundamental importance. Therefore,
following your suggestion, we have added a new section to the paper, Section
5.3, which is about a full-application study concerning a linear elasticity
equation. This means that the performance evaluation section is now structured
in two parts: Section 5.2, about local assembly speed ups; Section 5.3, about a
real application study. We think this is the best structuring to let readers
understand the effectiveness of our transformations "at the level of loop
nests" as well as their impact on the whole application. In the new section, we
provide the numbers you suggested: we show execution times of assembly and
solve, we comment on the achieved speed ups, and we highlight the cases in
which matrix insertion is negligible because of the high computational cost of
local assembly. For this application, we use a realistic mesh composed of
196608 cells (as reported in the paper). We also provide a link to the code of
the application. 


