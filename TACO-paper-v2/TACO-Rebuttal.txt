We thank our reviewers (R1-R4). We appreciate the positive feedback and the
critical comments on our paper. Below we answer the various points arose by the
reviewers.

R1:
We agree the big missing part was a discussion on the generality of our
approach and the applicability of the code transformations to other
computational domains (or, perhaps, their integration within a general-purpose
compilers). We added therefore a complete new section (Section 6) that explains
these aspects in details. We believe this addition was necessary to let readers
understand how to transfer our work to a different domain or compiler (or, at
least, to let them gain something from our experience of designing/developing 
the code transformations and COFFEE).

We also addressed all of the other useful comments (1-8) provided:
1) The goal is to develop a class of optimizations for an infinite variety of 
local assembly code. In that paragraph we say "(...) makes it hard to determine 
a single or specific sequence of transformations that is successfully 
applicable to all problems (...)", which implies that different finite element 
problems may require different code optimizations due to the inherent diversity 
in mathematical expressions and loop trip counts (this aspect is reiterated 
throughout the paper). Therefore, a compiler-based approach, in which the input 
and some parameters are analyzed to decide how to optimally transform the code 
is needed to get as close as possible to peak performance. Another possibility 
would be a library-based approach, although it would be quite pointless since 
we can directly generate the most suitable code for each problem.
2) The variation in loop trip counts impacts loop transformation. If the loop 
is relatively large, for example, vector-register tiling is likely to be useful 
to minimize register spilling; if it's small, then full or partial unrolling is 
an option. Understanding the border between "small" and "large" is challenging.  
The cost model helps in this direction. We tried to clarify the point you make 
in various sections in the paper: the sentence at page 6 has been re-written to 
anticipate the fact that "different loop sizes require different 
transformations". Then we discuss on the importance and the impact of loop 
unrolling in Section 4 (there is a complete new paragraph about that).  
Finally, in Section 5.2.5, we demonstrate our claim as discussing the impact of 
vector-register tiling (paragraphs 4, 5 and 6 of that section) in our 
benchmarks.
3) Clarified in the paper by adding/re-writing a few sentences. The short 
answer is: yes.
4) Explained in the paper.
5) We agree. We removed the code listing and expanded Figure 4.
6) Explained in the paper.
7) This was already explained by the sentence "Other optimization levels 
performed, in general, slightly worse."
8) We believe each of those tables can be represented by 6 different charts (1 
for each <Architecture, Equation> examined>). We have not done that for lack of 
space (the submission for acceptance is limited to 25 pages) but we can easily 
do that, if really needed, in the final version of the paper (which can be 
30-page long)

******************************************************************************

R2:
Two main critiques are moved to our paper.

a) We do not think that "the techniques of the paper are mostly interesting to 
a subset of the people in the intersection of code optimization and finite 
element methods". However, we do agree that the paper was missing an in-depth 
explanation about how to re-adapt/re-use our transformations in other contexts, 
perhaps other DSLs or even general-purpose compilers. Also, we believe that our 
experience of developing COFFEE and integrating it in a real-world framework 
based on a domain-specific language can be of inspiration for other research 
and/or engineering work. We therefore introduced a complete new section, 
Section 6, about the generality of our approach and the applicability of the 
code transformations to other computational domains. We hope you can read this 
section carefully, because it represents novel contribution of the paper and 
aims to answer exactly the concern you pointed out.

b) The results of the paper are *really* applicable to other kernels beyond 
those analyzed and presented. First of all, we want to emphasize a number of 
aspects. What is important to clarify is that kernels are not "mere small 
pieces of code" but rather numerical routines that evaluate an integral in each 
element of the discretized equation domain, produced by means of a real (used 
by many scientists around the world) domain-specific language (and compiler).  
Therefore, there is an infinite variety of kernels that can be generated in 
Firedrake, and their code structure (loop sizes and mathematical expression) 
depends on the partial differential equation that is being solved. This is 
explained and re-iterated in the paper. Also, when we vary the polynomial order 
of the method or the domain discretization (triangles, tetrahedrons, prisms) we 
actually change the code generation by increasing loops and arrays sizes and by 
making mathematical expressions more articulated (this happens, for example, 
when moving from 2D domains to 3D ones). So we are already testing many 
different code variants. This is carefully explained in Section 5.2.1. Now, 
premised that, back to your very point, which is "applicability and 
effectiveness of our transformations in other local assembly kernels". Our 
answer is twofold:

- Section 5.2.1 explains in-depth the motivations behind the choice of the 
  three problems (helmholtz, Diffusion, Burgers), explaining (in short) that 
"(...) distinct problems, possibly arising in completely different fields, may 
employ (subsets of) the same differential operators of our benchmarks, which 
implies similarities and redundant patterns in the generated code. 
Consequently, the proposed code transformations have a domain of applicability 
that goes far beyond that of the three analyzed equations. (...)". Then we 
re-inforce this later on in the section and in the paper.

- In Section 5.3 we provide a full-application study in which we analyze a 
  different equation (Linear Elasticity), which brings the total number of 
studied equations to 4 (Helmholtz, Diffusion, Burgers, Elasticity)
Adding more kernels to the performance evaluation section would make the paper 
only more chaotic: we selected these three problems to highlight all pros and 
cons of our code transformations. This is the reason for which we comment on 
the code transformations and their interplay in specific sub-sections, 
referring to, describing, and carefully analyzing all of the provided plots. 

Other kernels arising from completely different equations will still have the 
same high-level structure of the kernels we examined (e.g. same memory access 
pattern, same ranges of loop sizes, same problems concerning the presence of 
loop-invariant code and register pressure) so there is no reason for which our 
code optimizations would not be effective. The critical issue in our research 
was to understand what transformations our local assembly kernels required, and 
how to instantiate parametric transformations (vector-register tiling, loop 
interchange, expression splitting) optimally in each possible problem (this led 
to the development of the cost model).
