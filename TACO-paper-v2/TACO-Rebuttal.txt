We thank our reviewers (R1-R4). We appreciate the positive feedback and the
critical comments on our paper. Below we answer the various points arose by the
reviewers.

R1:
We agree the big missing part was a discussion on the generality of our
approach and the applicability of the code transformations to other
computational domains (or, perhaps, their integration within a general-purpose
compilers). We added therefore a complete new section (Section 6) that explains
these aspects in details. We believe this addition was necessary to let readers
understand how to transfer our work to a different domain or compiler (or, at
least, to let them gain something from our experience of designing/developing 
the code transformations and COFFEE).

We also addressed all of the other useful comments (1-8) provided:
1) The goal is to develop a class of optimizations for an infinite variety of 
local assembly codes. In that paragraph we say "(...) makes it hard to 
determine a single or specific sequence of transformations that is successfully 
applicable to all problems (...)", which implies that different finite element 
problems may require different code optimizations due to the inherent diversity 
in mathematical expressions and loop trip counts (this aspect is reiterated 
throughout the paper). Therefore, a compiler-based approach, in which the input 
and some parameters are analyzed to decide how to optimally transform the code 
is needed to maximize the performance. Another possibility would be a 
library-based approach, although it would be quite pointless since we can 
already generate the most suitable code for each problem.
2) The variation in loop trip counts impacts loops transformation. If the loop 
is relatively large, for example, vector-register tiling is likely to be useful 
to minimize register spilling; if it is small, then full or partial unrolling 
is an option. Understanding the border between "small" and "large" is 
challenging.  The cost model helps in this direction. We tried to clarify the 
point you make in various sections in the paper: the sentence at page 6 has 
been re-written to anticipate the fact that "different loop sizes require 
different transformations". Then we elaborate on the importance and the impact 
of loop unrolling in Section 4 (there is a complete new paragraph about that).  
Finally, in Section 5.2.5, we demonstrate our claim when discussing the impact 
of vector-register tiling (paragraphs 4, 5 and 6 of that section) in our 
benchmarks.
3) Clarified in the paper by adding/re-writing a few sentences. The short 
answer is: yes.
4) Explained in the paper.
5) We agree. We removed the code listing and expanded Figure 4.
6) Explained in the paper.
7) This was already explained by the sentence "Other optimization levels 
performed, in general, slightly worse."
8) We believe each of those tables can be represented by 6 different charts (1 
for each <Architecture, Equation> examined>). We have not done that for lack of 
space (the submission for acceptance is limited to 25 pages) but we can easily 
do that, if really needed, in the final version of the paper (which can be 
30-page long)

******************************************************************************

R2:
Two main critiques are moved to our paper.

a) We do not think that "the techniques of the paper are mostly interesting to 
a subset of the people in the intersection of code optimization and finite 
element methods". However, we do agree that the paper was missing an in-depth 
explanation about how to re-adapt/re-use our transformations in other contexts, 
perhaps other DSLs or even general-purpose compilers. Also, we believe that our 
experience of developing COFFEE and integrating it in a real-world framework 
based on a domain-specific language can be of inspiration for other research 
and/or engineering work. We therefore introduced a complete new section, 
Section 6, about the generality of our approach and the applicability of the 
code transformations to other computational domains. We hope you can read this 
section carefully, because it represents novel contribution of the paper and 
aims to answer exactly the concerns you pointed out.

b) The results of the paper are *really* applicable to other kernels beyond 
those analyzed and presented. First of all, we want to emphasize a number of 
aspects. What is important to clarify is that kernels are not "mere small 
pieces of code", but rather, in general, quite complex numerical routines 
evaluating integrals in an element of the discretized equation domain, produced 
by means of real (used by many scientists around the world) domain-specific 
language and compiler.  There is an infinite variety of kernels that can be 
generated in Firedrake; the generated code (e.g. loop sizes and mathematical 
expression) depends on the partial differential equation being solved. This is 
explained and re-iterated in the paper. Also, varying the polynomial order of 
the method or the element shape (e.g. triangles, tetrahedrons, prisms), as we 
do and explain in the performance evaluation section, changes the generated 
code by increasing loops and arrays sizes and by making mathematical 
expressions more articulated (this happens, for example, when moving from 2D to 
3D domains). So we are already testing many different code variants (three 
different kernels, and then for each kernel we try multiple polynomial orders 
and element shapes). This is carefully explained in Section 5.2.1. Premised 
this, we now go back to your point about "applicability and effectiveness of 
our transformations in other local assembly kernels". Our answer is twofold:

- Section 5.2.1 explains in-depth the motivations behind the choice of the 
  three benchmarks (Helmholtz, Diffusion, Burgers), explaining that "(...) 
distinct problems, possibly arising in completely different fields, may employ 
(subsets of) the same differential operators of our benchmarks, which implies 
similarities and redundant patterns in the generated code.  Consequently, the 
proposed code transformations have a domain of applicability that goes far 
beyond that of the three analyzed equations. (...)". Then we re-inforce this 
later on in the section and in the paper. Other kernels arising from completely 
different equations will still have the same structure of the ones we examined 
(e.g. same memory access pattern, same ranges of loop sizes, same problems 
concerning the presence of loop-invariant code and register pressure) as well 
as similar patterns in the mathematical expressions, so there is no evident 
reason for which our code optimizations would not be effective. The critical 
issue in our research was to understand what transformations our local assembly 
kernels required, and how to instantiate parametric transformations 
(vector-register tiling, loop interchange, expression splitting) optimally in 
each possible problem (this led to the development of the cost model).

- In Section 5.3 we provide a full-application study in which we analyze a new 
  equation (Linear Elasticity), which brings the total number of studied 
equations to 4 (Helmholtz, Diffusion, Burgers, Elasticity)
Adding more kernels to the performance evaluation section would make the paper 
only more chaotic: we specifically selected these three problems because they 
allow us to highlight all pros and cons of our code transformations. This is 
also the reason we comment on the performance of the code transformations (and 
their interplay) in specific sub-sections (referring to, describing, and 
carefully analyzing the provided plots), rather than in a single, centralized 
section. 

Other concerns:
- It is actually not true that we do not consider other optimization 
  strategies: 1) a comparison with using linear algebra libriries (the 
comparison uses hand-crafted kernels) and 2) a comparison with the FEniCS Form 
Compiler's built-in optimization system (which is the result of a cited paper) 
are provided.
- We agree it is useful to report speed ups and execution times for the whole 
  finite element calculation. We have added a new section (Section 5.3) about a 
full-application study, showing exactly these values.
- In some points in the paper (introduction and related work) we cite studies 
  concerning local assembly and GPUs. These works however deeply differ from 
ours for a number of reasons, including the facts that they do not rely on 
automated code generation, they are limited to only very low-order methods 
(typically only polynomial order 1), and they do not try to optimize for 
cross-loop arithmetic intensity as we do. The goal in these works is rather to 
understand how to effectively parallelize the local assembly code (so as to 
have multiple threads executing a single kernel, as opposed to our execution 
model in which we assume a single thread per kernel) in specific equations (not 
in a possibly infinite range as we do). A critical issue when executing local 
assembly kernels on GPUs is represented by memory requirements. We give here 
two examples of this problem, although more issues could be found and 
discussed. Firstly, the transfer of the mesh from host to device may be a great 
obstacle: if too big, the mesh will not fit the available GPU memory, and 
distributed execution, which is marginally supported in available finite 
element frameworks, would be required. Secondly, optimizations such as 
generalized loop-invariant code motion increase the memory requirements due to 
the introduction of temporary arrays.  This may actually lead to poor 
per-thread shared memory and registers allocation. These aspects suggest that 
the problem of executing generic, automatically-generated local assembly 
kernels on GPUs require a separate, in-depth study. However, in the new Section 
6, we provide some intuitions of how we could re-adapt COFFEE's transformations 
for execution on GPUs.  Therefore, we invite once more to go through the new 
Section 6 carefully.

******************************************************************************

R3:
We answer here the 5 points of the review.
1) For what concerns execution on a GPGPU, we replicate here the same answer to 
R2.

In some points in the paper (introduction and related work) we cite studies 
concerning local assembly and GPUs. These works however deeply differ from ours 
for a number of reasons, including the facts that they do not rely on automated 
code generation, they are limited to only very low-order methods (typically 
only polynomial order 1), and they do not try to optimize for cross-loop 
arithmetic intensity as we do. The goal in these works is rather to understand 
how to effectively parallelize the local assembly code (so as to have multiple 
threads executing a single kernel, as opposed to our execution model in which 
we assume a single thread per kernel) in specific equations (not in a possibly 
infinite range as we do). A critical issue when executing local assembly 
kernels on GPUs is represented by memory requirements. We give here two 
examples of this problem, although many more issues could be found. Firstly, the 
transfer of the mesh from host to device may be a great obstacle: if too big, 
the mesh will not fit the available GPU memory, and distributed execution, 
which is marginally supported in available finite element frameworks, would be 
required.  Secondly, optimizations such as generalized loop-invariant code 
motion increase the memory requirements due to the introduction of temporary 
arrays.  This may actually lead to poor per-thread shared memory and registers 
allocation. These aspects suggest that the problem of executing generic, 
automatically-generated local assembly kernels on GPUs require a separate, 
in-depth study. However, in the new Section 6 of the revised paper, we provide 
some intuitions of how we could re-adapt COFFEE's transformations for execution 
on GPUs. 

2) We agree that profile-guided optimization could be used to improve the 
performance of kernels, although this aspect is orthogonal to most of our code
transformations, because in no way profile information could suggest, for 
example, to trigger generalized loop-invariant code motion and avoidance of 
redundant computation (described in Section 3.2). Profile-guided optimization 
could theoretically be used for padding and data-alignment of arrays, provided 
that the compiler has sufficient machinery in place to perform (we quote the 
revised paper, Section 6) "some sort of data-flow analysis (...) to ensure that 
the extra loop iterations over the padded region do not affect the numerical 
results.". We discuss on the generality of our code transformation and on their 
integrability in a general-purpose compiler (and in its profile-guided 
optimization system) in the new Section 6 of the paper, which also represents 
novel contribution of our work.

3) TODO

4) Firedrake is *not* specifically designed for Intel architectures and 
compilers. The decision of using Intel platforms and tools is twofold: from one 
hand, we believe that Intel's is one of the most powerful vectorizing and 
optimizing compilers, so for a research like ours it was important to compare 
to the best available on the market; from the other hand, their wide 
availability represents a strong incentive because users will be likely to run 
their experiments on these machines.
It is also worth noting that all general-purpose compilers are, to different 
extents, limited in optimizing for register locality and code motion (for 
instance, this is true for the Intel and GNU compilers, although we only used 
the latter for "local" experimentation), so our work is expected to be 
equally/similarly effective on different yet "standard" CPU architectures.
From the practical viewpoint, we observe that COFFEE introduces optimizations 
aimed at improving cross-loop arithmetic intensity by performing source-level 
transformations, so there are absolutely no technical issues for using our 
domain-specific compiler on architectures different than Intel. Actually, this 
fact is a strength of our work, because it allows COFFEE to be almost 
completely platform-independent.  The only exception regards intrinsics code: 
COFFEE is currently only capable of outputing AVX intrinsics, so at the moment 
we cannot apply vector-register tiling (the also called outer-product 
vectorization technique) to architectures different than x86+AVX and x86+AVX2.  
However, COFFEE's internal algorithms use an intermediate representation of 
intrinsics code, so extending our compiler to other instruction sets would be 
just a matter of adding a new dictionary of intrinsics functions.

5) We agree that expression split has many issues involved. Our cost model 
offers a simple alternative for driving expression splitting. Simplicity is the 
key to effectiveness, portability, and robustness. Probably using a graph 
partitioning approach as you rightly point out would allow us to obtain better 
performance gains, but three points are worth to be highlighted:
- The split factor is a parameter of the transformation. In our experiments we 
  have tested several split factors, which means testing as many code variants 
(i.e. as many as the ways an expression can be split based on sum's 
associativity) and, as shown by the various graphs in the paper, our cost model 
seems to be capable of properly approximating such a parameter. This is of 
course encouraging.
- In the perspective of integrating expression splitting in a general-purpose 
  compiler, or generalizing this technique to other problems (as elaborated in 
the new Section 6), we feel that a simple model is preferable to keep the 
implementation complexity as low as possible.
- Graph partitioning for expression splitting seems to us a complete new 
  research problem. On the other hand, this transformation is only one of the 
many applicable by COFFEE, and we just do not want the focus of the paper being 
on "how to optimally and rapidly split a mathematical expression". Rather, we 
prefer to show that, for our problems, there is clearly a problem of register 
pressure and that by rescheduling the computation in different loops (and 
sub-expressions) we can actually obtain a performance gain. We have a strategy 
for expression splitting; we are not claiming it is the best strategy, but at 
least we demonstrate it is effective and easily predictable. 
We spent some time thinking about the problem: two things that we may want to 
try in the future are 1) a model based on graph partitioning and 2) a model 
based on a set of rewrite rules that exploit arithmetic properties such as 
commutativity, associativity, and distributivity/factorization to 
systematically optimize the arithmetic intensity of the expression.

Other comments:
a) We elaborate on the use of loop unwinding (or unrolling) in Section 4, in 
which we have added a new paragraph (named "Loop unrolling"). This paragraph 
elaborates on "complete/partial loop unwinding" as well as other general 
aspects concerning pros and cons of applying this transformation.

b) It was copy-paste leftover, now fixed

c) Right, fixed by making the statement less strong

d) This is all clarified and explained in details in the new Section 6. The 
short answer is that we hope COFFEE can be re-used in frameworks other than 
Firedrake (e.g. the FEniCS framework that we compare to in the paper). 

******************************************************************************

R4:
We addressed all of the observations in your review:
1) Fixed, thanks

2) The polyhedral model is well-known among compilers people. Basically, it is 
a mathematical model for representing affine loop nests and determining in a 
systemtic way all of its possible legal transformations. Cost models are often 
used by polyhedral compilers to estimate the effectiveness of a certain 
transformation. Polyhedral optimizations often aim at improving cache locality.  
In the Related Work section, we cite one of the most relevant researches on
polyhedral compilers; note that one of the co-authors of our paper is also a 
co-author of that paper (i.e. he is an expert of polyhedral compilers).

3 and 4) The speed ups only refer to the computation of the local element 
matrix.  This is now clarified in Section 5.1, and also re-iterated later on in 
the paper. We used different meshes for the various experiments. In Section 
5.2.1 we provide a link to a repository in which the code of the experiments as 
well as the mesh sizes employed can be found. We note that for these 
experiments the mesh size was set such that the working set of the computation 
could at least fit the last level cache (L3 on the Sandy Bridge architecture).  
The goal of these experiments (Section 5.2 in the revised paper) was only to 
measure speed ups achieved by optimizing local assembly kernels, so there was 
no point in running over big meshes (i.e. meshes that would not fit the last 
level cache), which would have only increased the completion time of our test 
suite (which is already order of hours).
We however completely agree that reporting execution times and speed ups of the 
overall finite element calculation is of fundamental importance. Therefore, 
following your suggestion, we have added a new section to the paper, Section 
5.3, which is about a full-application study concerning a linear elasticity 
equation.  This means that the performance evaluation section is now structured 
in two parts: Section 5.2, about local assembly speed ups; Section 5.3, about a 
real application study. We think this is the best structuring to let readers 
understand the effectiveness of our transformations "at the level of loop 
nests" as well as their impact on the whole application. In the new section, we 
provide the numbers you suggested: we show execution times of assembly and 
solve, we comment on the achieved speed ups, and we highlight the cases in 
which matrix insertion is negligible because of the high computational cost of 
local assembly. For this application, we use a realistic mesh composed of 
196608 cells (as reported in the paper). We also provide a link to the code of 
the application. 
