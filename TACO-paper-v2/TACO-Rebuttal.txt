We thank our reviewers (R1-R4). We appreciate the positive feedback and the
critical comments on our paper. Below we answer the various points arose by the
reviewers.

R1:
We agree the big missing part was a discussion on the generality of our
approach and the applicability of the code transformations to other
computational domains (or, perhaps, their integration within a general-purpose
compilers). We added therefore a complete new section (Section 6) that explains
these aspects in details. We believe this addition was necessary to let readers
understand how to transfer our work to a different domain or compiler (or, at
least, to let them gain something from our experience of designing/developing 
the code transformations and COFFEE).

We also addressed all of the other useful comments (1-8) provided:
1) The goal is to develop a class of optimizations for an infinite variety of 
local assembly code. In that paragraph we say "(...) makes it hard to determine 
a single or specific sequence of transformations that is successfully 
applicable to all problems (...)", which implies that different finite element 
problems may require different code optimizations due to the inherent diversity 
in mathematical expressions and loop trip counts (this aspect is reiterated 
throughout the paper). Therefore, a compiler-based approach, in which the input 
and some parameters are analyzed to decide how to optimally transform the code 
is needed to get as close as possible to peak performance. Another possibility 
would be a library-based approach, although it would be quite pointless since 
we can directly generate the most suitable code for each problem.
2) The variation in loop trip counts impacts loop transformation. If the loop 
is relatively large, for example, vector-register tiling is likely to be useful 
to minimize register spilling; if it's small, then full or partial unrolling is 
an option. Understanding the border between "small" and "large" is challenging.  
The cost model helps in this direction. We tried to clarify the point you make 
in various sections in the paper: the sentence at page 6 has been re-written to 
anticipate the fact that "different loop sizes require different 
transformations". Then we discuss on the importance and the impact of loop 
unrolling in Section 4 (there is a complete new paragraph about that).  
Finally, in Section 5.2.5, we demonstrate our claim as discussing the impact of 
vector-register tiling (paragraphs 4, 5 and 6 of that section) in our 
benchmarks.
3) Clarified in the paper by adding/re-writing a few sentences. The short 
answer is: yes.
4) Explained in the paper.
5) We agree. We removed the code listing and expanded Figure 4.
6) Explained in the paper.
7) This was already explained by the sentence "Other optimization levels 
performed, in general, slightly worse."
8) We believe each of those tables can be represented by 6 different charts (1 
for each <Architecture, Equation> examined>). We have not done that for lack of 
space (the submission for acceptance is limited to 25 pages) but we can easily 
do that, if really needed, in the final version of the paper (which can be 
30-page long)

******************************************************************************

R2:
Two main critiques are moved to our paper.

a) We do not think that "the techniques of the paper are mostly interesting to 
a subset of the people in the intersection of code optimization and finite 
element methods". However, we do agree that the paper was missing an in-depth 
explanation about how to re-adapt/re-use our transformations in other contexts, 
perhaps other DSLs or even general-purpose compilers. Also, we believe that our 
experience of developing COFFEE and integrating it in a real-world framework 
based on a domain-specific language can be of inspiration for other research 
and/or engineering work. We therefore introduced a complete new section, 
Section 6, about the generality of our approach and the applicability of the 
code transformations to other computational domains. We hope you can read this 
section carefully, because it represents novel contribution of the paper and 
aims to answer exactly the concern you pointed out.

b) The results of the paper are *really* applicable to other kernels beyond 
those analyzed and presented. First of all, we want to emphasize a number of 
aspects. What is important to clarify is that kernels are not "mere small 
pieces of code" but rather numerical routines that evaluate an integral in each 
element of the discretized equation domain, produced by means of a real (used 
by many scientists around the world) domain-specific language (and compiler).  
Therefore, there is an infinite variety of kernels that can be generated in 
Firedrake, and their code structure (loop sizes and mathematical expression) 
depends on the partial differential equation that is being solved. This is 
explained and re-iterated in the paper. Also, when we vary the polynomial order 
of the method or the domain discretization (triangles, tetrahedrons, prisms) we 
actually change the code generation by increasing loops and arrays sizes and by 
making mathematical expressions more articulated (this happens, for example, 
when moving from 2D domains to 3D ones). So we are already testing many 
different code variants. This is carefully explained in Section 5.2.1. Now, 
premised that, back to your very point, which is "applicability and 
effectiveness of our transformations in other local assembly kernels". Our 
answer is twofold:

- Section 5.2.1 explains in-depth the motivations behind the choice of the 
  three problems (helmholtz, Diffusion, Burgers), explaining (in short) that 
"(...) distinct problems, possibly arising in completely different fields, may 
employ (subsets of) the same differential operators of our benchmarks, which 
implies similarities and redundant patterns in the generated code.  
Consequently, the proposed code transformations have a domain of applicability 
that goes far beyond that of the three analyzed equations. (...)". Then we 
re-inforce this later on in the section and in the paper. Other kernels arising 
from completely different equations will still have the same high-level 
structure of the kernels we examined (e.g. same memory access pattern, same 
ranges of loop sizes, same problems concerning the presence of loop-invariant 
code and register pressure) so there is no reason for which our code 
optimizations would not be effective. The critical issue in our research was to 
understand what transformations our local assembly kernels required, and how to 
instantiate parametric transformations (vector-register tiling, loop 
interchange, expression splitting) optimally in each possible problem (this led 
to the development of the cost model).

- In Section 5.3 we provide a full-application study in which we analyze a 
  different equation (Linear Elasticity), which brings the total number of 
studied equations to 4 (Helmholtz, Diffusion, Burgers, Elasticity)
Adding more kernels to the performance evaluation section would make the paper 
only more chaotic: we selected these three problems to highlight all pros and 
cons of our code transformations. This is the reason for which we comment on 
the code transformations and their interplay in specific sub-sections, 
referring to, describing, and carefully analyzing all of the provided plots. 

Other concerns:
- It is actually not true that we do not consider other optimization 
  strategies: 1) a comparison with using linear algebra libriries (the 
comparison uses hand-crafted kernels) and 2) a comparison with the FEniCS Form 
Compiler's built-in optimization system (which is the result of a cited paper) 
are provided.
- We agree it is useful to report speed ups and execution times for the whole 
  finite element calculation. We have added a new section (Section 5.3) about a 
full-application study, showing exactly these values.
- At some points in the paper (introduction and related work) we cite studies 
  concerning local assembly and GPUs. These works however deeply differ from 
ours for a number of reasons, including the fact that they are outside of the 
context of automated code generation, they are limited to only very low-order 
methods (typically only polynomial order 1), and they do not try to optimize 
for cross-loop arithmetic intensity as we do. The goal in these works is rather 
to understand how to effectively parallelize the local assembly code (so as to 
have multiple threads executing a single kernel, as opposed to our execution 
model in which we assume a single thread per kernel) in specific equations (not 
in a possibly infinite range as we do). A critical issue when executing local 
assembly kernels on GPUs is represented by memory requirements. We give here 
two examples, although more problems could be discussed. First, the transfer of 
the mesh from host to GPU may be a notable obstacle: if too big, the mesh
will not fit the available GPU memory, and distributed execution, which is 
marginally supported in available finite element frameworks, would be required.  
Secondly, optimizations like generalized loop-invariant code motion further 
increase the memory requirements due to the introduction of temporary arrays.  
This may actually lead to poor per-thread shared memory and registers 
allocation. These aspects suggest that the problem of executing generic, 
automatically-generated local assembly kernels on GPUs require a separate, 
in-depth study. However, in the new Section 6, we provide some intuitions of 
how we could re-adapt COFFEE's transformations for execution on GPUs.

******************************************************************************

R3:
We answer here the 5 points of the review.
1) For what concerns execution on a GPGPU, we replicate here the same answer to 
R2.
2) We agree that profile-guided optimization could be used to improve the 
performance of kernels, although this aspect is orthogonal to most of our code
transformations, because in no way profile information could be used, for 
example, to trigger generalized loop-invariant code motion and avoidance of 
redundant computation (as explained in Section 3.2). Profile-guided 
optimization could theoretically be used for padding and data-alignment of 
arrays, provided that the compiler has sufficient machinery in place to perform 
(we quote the revised paper, Section 6) "some sort of data-flow analysis (...) 
to ensure that the extra loop iterations over the padded region do not affect 
the numerical results.". We discuss on the generality of our code 
transformation and on their integrability in a general-purpose compiler (and in 
its profile-guided optimization system) in the new Section 6 of the paper, 
which also represents novel contribution of our work.
3) TODO
4) Firedrake is *not* specifically designed for Intel architectures and 
compilers. The decision of using Intel platforms and tools is twofold: from one 
hand, we believe that Intel's is one of the most powerful vectorizing and 
optimizing compilers, so for a research like ours it was important to compare 
to the best available on the market; from the other hand, their wide 
availability represents a strong incentive for studying the problems object of 
the paper on these machines.
It is also worth noting that all general-purpose compilers are, to different 
extents, limited in optimizing for register locality and code motion (for 
instance, this is true for the Intel and GNU compilers, the latter only used 
for "local" experimentation), so our work is expected to be equally/similarly 
effective on different "standard" CPU architectures.
From the practical viewpoint, we observe that COFFEE introduces optimizations 
aimed at improving cross-loop arithmetic intensity by performing source-level 
transformations, so there are absolutely no technical issues for 
straightforwardly using our domain-specific compiler on architectures different 
than Intel. Actually, this fact is a strength of our work, because it allows 
COFFEE to be almost completely platform-independent.  The only exception 
regards intrinsics code: COFFEE is currently only capable of outputing AVX 
intrinsics, so at the moment we cannot apply vector-register tiling (the also 
called outer-product vectorization technique) to architectures different than 
x86+AVX. However, COFFEE's internal algorithms use an intermediate 
representation of intrinsics code, so extending our compiler to other 
instruction sets would be just a matter of adding a new dictionary of 
compiler-specific intrinsics.
5) We agree that expression split has many issues involved. Our cost model 
offers a simple alternative for driving expression splitting. Simplicity is the 
key to effectiveness, portability, and robustness. Probably using a graph 
partitioning approach as you rightly point out would allow us to obtain better 
performance gains, but three points are worth to be highlighted:
- The split factor is a parameter of the transformation. In our experiments we 
  have tested several split factors, which means testing as many code variants 
(i.e. as many ways of splitting an expression based on sum's associativity) 
and, as shown by the various graphs in the paper, our cost model seems to be 
capable of properly approximating such a parameter. This is of course 
encouraging.
- In the perspective of integrating expression splitting in a general-purpose 
  compiler, or generalizing this technique to other problems (as elaborated in 
the new Section 6), we feel that a simple model is preferable to keep the 
implementation complexity as low as possible.
- TODO

Other comments:
a) We elaborate on the use of loop unwinding (or unrolling) in Section 4, in 
which we have added a new paragraph (named "Loop unrolling"). This paragraph 
elaborates on "complete/partial loop unwinding" as well as other general 
aspects concerning pros and cons of applying this transformation.
b) It was copy-paste leftover, now fixed
c) Right, fixed by making the statement less strong
d) This is all clarified and explained in details in the new Section 6. The 
short answer is that we hope COFFEE can be re-used in frameworks other than 
Firedrake (e.g. the FEniCS framework that we compare to in the paper). 

******************************************************************************

R4:
We addressed all of the observations in your review:
1) Fixed, thanks
2) The polyhedral model is well-known among compilers people. Basically, it is 
a mathematical model for representing affine loop nests and determining in a 
systemtic way all of its possible legal transformations. Cost models are often 
used by polyhedral compilers to estimate the effectiveness of a certain 
transformation. Polyhedral optimizations often aim at improving cache locality.  
In the Related Work section, we cite one of the most relevant researches on
polyhedral compilers; note that one of the co-authors of our paper is also a 
co-author of that paper (i.e. he is an expert of polyhedral compilers).
3) The speed ups only refer to the computation of the local matrix. This is now 
clarified in Section 5.1, and also re-iterated later on in the paper. 
We used different meshes for the various experiments. There is a link to 
the code of the experiments as well as the mesh sizes employed, for each 
experiment. This is now explained in the paper, Section 5.2.1.
4) 
