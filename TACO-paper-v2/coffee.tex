% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{tabulary}
\usepackage{lineno}

\usepackage{alltt}
\renewcommand{\ttdefault}{txtt}

\usepackage{listings}
\lstset{language=C}

\usepackage[cmex10]{amsmath}
\usepackage{url}

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
\usepackage{multicol}

% Metadata Information
%\acmVolume{9}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2010}
%\acmMonth{3}

% Document starts
\begin{document}

% Page heads
\markboth{F. Luporini et al.}{Cross-Loop Optimization of Arithmetic Intensity for Finite Element Local Assembly}
% TODO: "...Intensity for Finite..." OR rather "...Intensity in Finite..."

% Title portion
\title{Cross-Loop Optimization of Arithmetic Intensity for Finite Element Local Assembly}
\author{Fabio Luporini
\affil{Imperial College London}
Ana Lucia Varbanescu
\affil{University of Amsterdam}
Florian Rathgeber
\affil{Imperial College London}
Gheorghe-Teodor Bercea
\affil{Imperial College London}
J. Ramanujam
\affil{Louisiana State University}
David A. Ham
\affil{Imperial College London}
Paul H.J. Kelly
\affil{Imperial College London}}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
We study and systematically evaluate a class of composable code
transformations that improve arithmetic intensity in local assembly
operations, which represent a significant fraction of the execution
time in finite element methods. Their performance optimization is indeed a
challenging issue. Even though affine loop nests are generally present,
the short trip counts and the complexity of mathematical expressions, which
vary among different problems, make it hard to determine a single or unique
sequence of successful transformations. Our investigation has resulted in the
implementation of a compiler for local assembly kernels, fully
integrated with a framework for developing finite element methods.
The compiler manipulates abstract syntax trees generated from a domain-specific
language by introducing domain-aware optimizations and, eventually,
produces C code including vector SIMD intrinsics. Experiments using a range of real-world finite-element
problems of increasing complexity show that significant performance improvement
is achieved. The generality of the approach and the applicability of the proposed
code transformations to other domains is also discussed.

\end{abstract}

\category{G.1.8}{Numerical Analysis}{Partial Differential Equations -
  Finite element methods}

\category{G.4}{Mathematical Software}{Parallel and vector
  implementations}

\terms{Design, Performance}

\keywords{Finite element integration, local assembly, compilers,
  optimizations, SIMD vectorization}

\acmformat{Fabio Luporini, Ana Lucia Varbanescu, Florian Rathgeber,
  Gheorghe-Teodor Bercea, J. Ramanujam, David A. Ham, and Paul
  H. J. Kelly, 2014. Cross-Loop Optimization of Arithmetic Intensity for Finite Element Local Assembly.}

% At a minimum you need to supply the author names, year and a title.
% IMPORTANT: Full first names whenever they are known, surname last,
% followed by a period.  In the case of two authors, 'and' is placed
% between them.  In the case of three or more authors, the serial
% comma is used, that is, all author names except the last one but
% including the penultimate author's name are followed by a comma, and
% then 'and' is placed before the final author's name.  If only first
% and middle initials are known, then each initial is followed by a
% period and they are separated by a space.  The remaining information
% (journal title, volume, article number, date, etc.) is
% 'auto-generated'.

\begin{bottomstuff}

This research is partly funded by the MAPDES project, by the
Department of Computing at Imperial College London, by EPSRC through
grants EP/I00677X/1, EP/I006761/1, and EP/L000407/1, by NERC grants
NE/K008951/1 and NE/K006789/1, by the U.S.  National Science
Foundation through grants 0811457 and 0926687, by the U.S. Army
through contract W911NF-10-1-000, and by a HiPEAC collaboration
grant. The authors would like to thank Dr. Carlo Bertolli,
Dr. Lawrence Mitchell, and Dr. Francis Russell for their invaluable
suggestions and their contribution to the Firedrake project.
We thank the ACM TACO reviewers for their suggestions that led
to a significantly improved version.

Author's addresses: Fabio Luporini $\&$ Florian Rathgeber $\&$
Gheorghe-Teodor Bercea $\&$ Paul H. J. Kelly, Department of Computing,
Imperial College London; Ana Lucia Varbanescu, Informatics Institute,
University of Amsterdam; David A. Ham, Department of Computing and
Department of Mathematics, Imperial College London; J. Ramanujam,
iCenter for Computation and Technology and the School of Elec.~Engr.\&
Comp.~Sci., Louisiana State University.
\end{bottomstuff}

\maketitle



\section{Introduction}

In many fields such as computational fluid dynamics, computational
electromagnetics and structural mechanics, phenomena are modelled by
partial differential equations (PDEs). Numerical techniques, such as the
finite volume method and the finite element method, are widely employed
to approximate solutions of these PDEs. Unstructured meshes are often
used to discretize the computational domain, since they allow an
accurate representation of complex geometries. The solution is sought by
applying suitable numerical operations, described by computational
kernels, to all entities of a mesh, such as edges, vertices, or cells.
On standard clusters of multicores, typically, a kernel is executed
sequentially by a thread, while parallelism is achieved by partitioning
the mesh and assigning each partition to a different node or thread.
Such an execution model, with minor variations, is adopted, for example,
by \citeN{pyop2isc}, \citeN{Fenics}, \citeN{fluidity_manual_v4},
\citeN{lizst}.

The time required to apply the numerical kernels is a major issue,
since the equation domain needs to be discretized into an extremely
large number of cells to obtain a satisfactory approximation
of the PDE, possibly of the order of trillions, as in
\citeN{Rossinelli2013}. For example, it has been well established that mesh
resolution is critical in the accuracy of numerical weather
forecasts. However, operational forecast centers have a strict time
limit in which to produce a forecast - 60 minutes in the case of the
UK Met Office. Producing efficient kernels has a direct scientific
payoff in higher resolution, and therefore more accurate,
forecasts. Computational cost is a dominant problem in
computational science simulations, especially for those based on
finite elements, which are the subject of this paper.

% FIXME: citation for the 30-60% figure

We address, in particular, the well-known problem of optimizing the local assembly
phase of the finite element method \cite{Francis,quadrature1,petsc-integration-gpu,Kirby-FEM-opt},
which can be responsible for a significant fraction of the overall computation
run-time, often in the range 30-60$\%$. With respect to these studies,
we propose a novel set of composable code transformations targeting, for
the first time, cross-loop arithmetic intensity, with emphasis on instruction-level
parallelism, redundant computation, and register locality. How such transformations
generalize to domains other than finite element local assembly will also be discussed.

During the assembly phase, the solution of the PDE is approximated by
executing a problem-specific kernel over all cells, or elements, in the
discretized domain. In this work, we focus on relatively low order finite
element methods, in which an assembly kernel's working set is usually small
enough to fit the L1 cache of traditional CPU architectures. Low order methods are by no means exotic: they
are employed in a wide variety of fields, including climate and ocean
modeling, computational fluid dynamics, and structural mechanics. The
efficient assembly of high order methods such as the spectral element
method \cite{SPENCER} requires a significantly different loop nest structure.
High order methods are therefore excluded from our study.

We aim to maximize our impact on the platforms that are realistically used
for finite element applications, so we target conventional CPU architectures
rather than GPUs. 
The key limiting factor to the execution on GPUs is the stringent memory requirements. 
Only relatively small problems fit in a GPU memory, and support for
distributed GPU execution in general purpose finite element frameworks is minimal. There has been some research on
adapting local assembly to GPUs, which we mention in Section~\ref{sec:related-work},
although it differs from ours in several ways, including: (i) not relying on
automated code generation from a domain-specific language (explained next),
(ii) testing only order 1 methods (while we focus on the range 1-4), (iii) not optimizing for
cross-loop arithmetic intensity (the goal is rather effective multi-thread
parallelization). In addition, our code transformations would drastically impact the 
GPU parallelization strategy, for example by increasing a thread's working set. 
For all these reasons, a study on extending our work to GPU architectures is 
beyond the scope of this paper. In Section~\ref{sec:generality}, however, we provide 
some intuitions about this research direction.

An assembly kernel is characterized by the presence of an affine,
often non-perfect loop nest, in which individual loops are rather
small: their trip count rarely exceeds 30, and may be as low as 3 for
low order methods. In the innermost loop, a problem-specific,
compute intensive expression evaluates a two dimensional array,
representing the result of local assembly in an element of the
discretized domain. With such a kernel structure, we focus on aspects
like the minimization of floating-point operations, register allocation
and instruction-level parallelism, especially in the form of SIMD
vectorization.

Achieving high performance on CPUs is non-trivial. The complexity of the
mathematical expressions, often characterized by a large number of
operations on constants and small matrices, makes it hard to determine a
single or specific sequence of transformations that is successfully
applicable to different problems (equations). Loop trip counts are typically small and can
vary significantly, which further exacerbates the issue. A
compiler-based approach is, therefore, the only reasonable option to
obtain close-to-peak performance in a wide range of different local
assembly kernels. Optimizations like padding, generalized loop-invariant
code motion, vector-register tiling, and expression splitting, as well
as their composition, are essential, but their support in
state-of-the-art polyhedral and vendor compilers, if any, is very
limited. BLAS routines could theoretically be employed, although a
fairly complicated control and data flow analysis would be required to
automate identification and extraction of matrix-matrix multiplies. In
addition, as detailed in Section~\ref{sec:perf-eval-blas}, the small
dimension of the matrices involved and the potential loss in data
locality can limit or eliminate the performance gain of this approach.

In order to overcome the constraints of the available compilers and
specialized linear algebra libraries, we have automated a set of generic
and model-driven code transformations in COFFEE\footnote{COFFEE stands
for COmpiler For FinitE Element local assembly.}, a compiler for
optimizing local assembly kernels. COFFEE is integrated with
Firedrake~\cite{firedrake-code}, a system for solving PDEs through the
finite element method based on the PyOP2
abstraction~\cite{pyop2ws,pyop2isc}. All problems expressible with this
framework are supported by COFFEE, including equations that can be found
at the core of real-world simulations, such as those used in our
performance evaluation. In evaluating our code transformations for a
range of relevant problems, we vary two key parameters that impact
solution accuracy and kernel cost: the polynomial order of the method
(we investigate from $p=1$ to $p=4$) and the geometry of elements in the
discretized domain (2D triangle, 3D tetrahedron, 3D prism). From the
point of view of the generated code, these two parameters directly
impact the size of both loop nests and mathematical expressions, as
elaborated in Section~\ref{sec:real-examples}.

%JUNK: 1.5$\times$ over code in which only basic transformations have
%been performed, such as generalized loop-invariant code motion,
%padding, and data alignment, and up to

Our experiments show that the original generated code for non-trivial
assembly kernels, despite following state-of-the-art techniques,
% FIXME: what are these state-of-the-art techniques?
remains suboptimal in the context of modern multicore
architectures. Our domain-aware cost-model-driven sequence of code
transformations, aimed at improving SIMD vectorization and register
data locality, can result in performance improvements up to
4.4$\times$ over original kernels. Around 70$\%$ of the test cases
obtain a speed up greater than 2$\times$.

Summarizing, the contributions of this paper are:
\begin{enumerate}
\item An optimization strategy for finite element local assembly that
  exploits domain knowledge and goes beyond the limits of both vendor
  and research compilers.
\item The design and implementation of a compiler that automates the
  proposed code transformations for any problems expressible in
  Firedrake.
\item A systematic analysis using a suite of examples of real-world
  importance that is evidence of significant performance improvements on
  two Intel architectures, a Sandy Bridge CPU and the Xeon Phi.
 \item A full application evaluation to demonstrate that the optimization
 strategy greatly impacts the whole finite element problem.
 \newcounter{enumTemp}
 \setcounter{enumTemp}{\theenumi}
\end{enumerate}
Our contributions also include a theoretical study involving
\begin{enumerate}
 \setcounter{enumi}{\theenumTemp}
 \item An analysis of the generality of the proposed code transformations
 and an investigation of their applicability to different computational
 domains.
\end{enumerate}
The paper is organized as follows. In Section~\ref{sec:background} we
provide some background on local assembly, show code generated by
Firedrake and emphasize the critical computational
aspects. Section~\ref{sec:code-transf} describes the various code
transformations, highlighting when and how domain knowledge has been
exploited. The design and implementation of our compiler is discussed
in Section~\ref{sec:pyop2-compiler}. Section~\ref{sec:perf-results}
shows performance results. The generality of our optimization strategy
and the applicability to other domains is discussed in Section~\ref{sec:generality}.
Related work is described in Section~\ref{sec:related-work}. Finally,
Section~\ref{sec:conclusions} reviews our contributions in the light
of our results, and identifies priorities for future work.

% systematic optimization strategy -> discuss the importance of domain
% knowledge

% A python implementation of OP2, namely PyOP2, is used in the
% Firedrake project as an intermediate representation for FEM
% computations. In Firedrake all computational kernels are generated
% by a compiler, (the Fenics Form Compiler, FFC), whereas in OP2
% applications kernels are provided by users.

\section{Background and Motivating Examples}
\label{sec:background}
%Here we discuss about the computational characteristics of
%computational science kernels. Briefly cite op2 kernels. Emphasis on
%Finite Element Assembly. Generalization and formalization of a Finite
%Element Assembly kernel using quadrature representation.

%\NoCaptionOfAlgo
%\RestoreCaptionOfAlgo

\begin{figure}[b]
\begin{alltt}
\scriptsize
\textbf{Input:} element matrix (2D array, initialized to 0), coordinates (array), coefficients (array, e.g. velocity)
\textbf{Output:} element matrix (2D array)
- Compute Jacobian from coordinates
- Define basis functions
- Compute element matrix in an affine loop nest
\end{alltt}
\caption{Structure of a local assembly kernel}
\label{code:general-structure}
\end{figure}

\begin{table}[b]
\tbl{Type and variable names used in the various listings to identify local assembly objects.}{
\scriptsize
\begin{tabulary}{1.0\columnwidth}{C|C|C}
\hline
Object name & Type & Variable name(s) \\\hline
Determinant of the Jacobian matrix & double & det  \\
Inverse of the Jacobian matrix & double & K1, K2, ... \\
Coordinates & double** & coords\\
Fields (coefficients) & double** & w \\
Numerical integration weights & double[] & W \\
Basis functions (and derivatives) & double[][] & X, Y, X1, ... \\
Element matrix & double[][] & A\\ \hline
\end{tabulary}
}
\label{table:map-name-letters}
\end{table}

Local assembly is the computation of contributions of a specific cell in
the discretized domain to the linear system which yields the PDE
solution. The process consists of numerically evaluating
problem-specific integrals to produce a matrix and a vector
\cite{quadrature1,fluidity_manual_v4}, whose sizes depend on the order
of the method. This operation is applied to all cells in the discretized
domain. In this work we focus on local matrices, or \emph{element
matrices}, which are more costly to compute than element vectors.

Given a finite element description of the input problem, expressed
through the domain-specific Unified Form Language (UFL)~\cite{ufl},
Firedrake employs the FEniCS form compiler (FFC)~\cite{FFC-Compiler} to
generate an abstract syntax tree of a kernel implementing assembly using
numerical quadrature. This kernel can be applied to any
element in the mesh, which follows from a mathematical property of the
finite element method. The evaluation of a local matrix can be
reduced to integration on a fixed \emph{reference} element --- a special
element that does not belong to the domain --- after a suitable change
of coordinates. Firedrake triggers the compilation of an assembly
kernel using an available vendor compiler, and manages its (parallel)
execution over all elements in the mesh. As already explained, the
subject of this paper is to enhance this execution model by adding an
optimization stage prior to the generation of C code.

%The code transformations described in this work are also
%generalizable to non-Firedrake assembly kernels, provided that the
%same numerical integration algorithm is employed.

The structure of a local assembly kernel is shown in
Figure~\ref{code:general-structure}. The inputs are a zero-initialized
two dimensional array used to store the element matrix, the element's
coordinates in the discretized domain, and coefficient fields, for
instance indicating the values of velocity or pressure in the
element. The output is the evaluated element matrix. The kernel body
can be logically split into three parts:
\begin{enumerate}
  \item Calculation of the Jacobian matrix, its determinant and its
    inverse required for the aforementioned change of coordinates from
    the reference element to the one being computed.
  % FIXME: this rather hand-wavy and not technically correct
  \item Definition of basis functions used to interpolate fields at the
    quadrature points in the element. The choice of basis functions is
    expressed in UFL directly by users. In the generated code, they are
    represented as global read-only two dimensional arrays (i.e., using
    \texttt{static const} in C) of double precision floats.
  \item Evaluation of the element matrix in an affine loop nest, in which
    the integration is performed.
\end{enumerate}
Table~\ref{table:map-name-letters} shows the variable names
we will use in the upcoming code snippets to refer to the various
kernel objects.

%JUNK SENTENCES: Since only the values assumed at the integration
%points are needed to compute the element matrix, a basis function is
%implemented as a two dimensional array of double precision
%floats. The same basis functions can be used to evaluate all element
%matrices, provided that a suitable change of coordinates from a
%``reference'' element to a real one is performed. Therefore, basis
%functions are declared in the kernel body as global read-only arrays
%(i.e., using \texttt{static const} in C), rather than passed as
%function parameters. Basis functions are chosen at the UFL layer
%based on the problem being solved, so they are actually an input to
%the code generator.

\begin{algorithm}[t]
\scriptsize
\SetAlgorithmName{LISTING}{}

\KwSty{void} helmholtz(\KwSty{double} A[3][3], \KwSty{double} **coords) $\lbrace$\\
~~// K, det = Compute Jacobian (coords) \\
~~\\
~~\KwSty{static const double} W[3] = $\lbrace$...$\rbrace$\\
~~\KwSty{static const double} X$\_$D10[3][3] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\KwSty{static const double} X$\_$D01[3][3] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$3; i++) \\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$3; k++) \\
~~~~~~~~A[j][k] += ((Y[i][k]*Y[i][j]+\\
~~~~~~~~~~~+((K1*X$\_$D10[i][k]+K3*X$\_$D01[i][k])*(K1*X$\_$D10[i][j]+K3*X$\_$D01[i][j]))+\\
~~~~~~~~~~~+((K0*X$\_$D10[i][k]+K2*X$\_$D01[i][k])*(K0*X$\_$D10[i][j]+K2*X$\_$D01[i][j])))*\\
~~~~~~~~~~~*det*W[i]);\\
$\rbrace$
\caption{Local assembly code generated by Firedrake for a Helmholtz problem on a 2D triangular mesh using Lagrange $p=1$ elements.}
\label{code:helmholtz}
\end{algorithm}

\begin{algorithm}[t]
\scriptsize
\SetAlgorithmName{LISTING}{}

\KwSty{void} burgers(\KwSty{double} A[12][12], \KwSty{double} **coords, \KwSty{double} **w) $\lbrace$\\
~~// K, det = Compute Jacobian (coords) \\
~~\\
~~\KwSty{static const double} W[5] = $\lbrace$...$\rbrace$\\
~~\KwSty{static const double} X1$\_$D001[5][12] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\KwSty{static const double} X2$\_$D001[5][12] = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~//11 other basis functions definitions.\\
~~...\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$5; i++) $\lbrace$\\
~~~~\KwSty{double} F0 = 0.0;\\
~~~~//10 other declarations (F1, F2,...)\\
~~~~...\\
~~~~\KwSty{for} (\KwSty{int} r = 0; r$<$12; r++) $\lbrace$\\
~~~~~~F0 += (w[r][0]*X1$\_$D100[i][r]);\\
~~~~~~//10 analogous statements (F1, F2, ...)\\
~~~~~~...\\
~~~~$\rbrace$\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$12; j++) \\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$12; k++) \\
~~~~~~~~A[j][k] += (..(K5*F9)+(K8*F10))*Y1[i][j])+\\
~~~~~~~~~+(((K0*X1$\_$D100[i][k])+(K3*X1$\_$D010[i][k])+(K6*X1$\_$D001[i][k]))*Y2[i][j]))*F11)+\\
~~~~~~~~~+(..((K2*X2$\_$D100[i][k])+...+(K8*X2$\_$D001[i][k]))*((K2*X2$\_$D100[i][j])+...+(K8*X2$\_$D001[i][j]))..)+\\
~~~~~~~~~+ $<$roughly a hundred sum/muls go here$>$)..)*\\
~~~~~~~~~*det*W[i]);\\
~~$\rbrace$ \\
$\rbrace$
\caption{Local assembly code generated by Firedrake for a Burgers
problem on a 3D tetrahedral mesh using Lagrange $p=1$ elements.}
\label{code:burgers}
\end{algorithm}

The actual complexity of a local assembly kernel depends on the finite
element problem being solved. In simpler cases, the loop nest is
perfect, has short trip counts (in the range 3--15), and the computation
reduces to a summation of a few products involving basis functions. An
example is provided in Listing~\ref{code:helmholtz}, which shows the
assembly kernel for a Helmholtz problem using Lagrange basis functions
on 2D elements with polynomial order $p=1$. In other scenarios, for
instance when solving the Burgers equation, the number of arrays
involved in the computation of the element matrix can be much larger.
The assembly code is given in Listing~\ref{code:burgers} and contains 14
unique arrays that are accessed, where the same array can be referenced
multiple times within the same expression. This may also require the
evaluation of constants in outer loops (called $F$ in the code) to act
as scaling factors of arrays. Trip counts grow proportionally to the
order of the method and arrays may be block-sparse. In addition to a
larger number of operations, more complex cases like the Burgers
equation are characterized by high register pressure.

The Helmholtz and Burgers equations exemplify a large class of problems
and, as such, will constitute our benchmark problems, along with the
Diffusion equation. We carefully motivate this choice in
Section~\ref{sec:real-examples}.

Despite the infinite variety of assembly kernels which Firedrake can
generate, it is still possible to identify common domain-specific
traits that can be exploited for effective code transformations and
SIMD vectorization. These include: 1) memory accesses along the three
loop dimensions are always unit stride; 2) the \texttt{j} and \texttt{k}
loops are interchangeable, whereas interchanges involving the $i$ loop
require pre-computation of values (e.g. the $F$ values in Burgers) and
introduction of temporary arrays, as explained in
Section~\ref{sec:code-transf}; 3) depending on the problem being
solved, the \texttt{j} and \texttt{k} loops could iterate over the
same iteration space; 4) most of the sub-expressions on the right hand
side of the element matrix computation depend on just two loops
(either \texttt{i}-\texttt{j} or \texttt{i}-\texttt{k}). In
Section~\ref{sec:code-transf} we show how to exploit these
observations to define a set of systematic, composable optimizations.

\section{Code Transformations}
\label{sec:code-transf}
The code transformations presented in this section are applicable to
all finite element problems that can be formulated in Firedrake.
Their generality and the potential for applicability in other domains is also discussed in
Section~\ref{sec:generality}.

As already emphasized, the variations in the structure of
mathematical expressions and in loop trip counts, although typically
limited to the order of tens of iterations, render the optimization process
challenging, requiring distinct sets of transformations to bring performance
closest to the machine peak in different problems. For example, the Burgers problem in Listing~\ref{code:burgers},
given the large number of arrays accessed, suffers from high register pressure,
whereas the Helmholtz problem in Listing~\ref{code:helmholtz} does
not; this intuitively suggests that the two problems require a
different treatment, based on an in-depth analysis of both data and
iteration spaces. Furthermore, domain knowledge enables
transformations that a general-purpose compiler could not apply,
making the optimization space even larger. In this context, our goal
is to understand the relationship between distinct code
transformations, their impact on cross-loop arithmetic intensity, and to what
extent their composability is effective in a class of problems and
architectures.

% Padding and Data Alignment
\subsection{Padding and Data Alignment}
The absence of stencils renders the element matrix computation easily
auto-vectorizable by a vendor compiler. Nevertheless,
auto-vectorization is not efficient if data are not aligned to cache-line boundaries and if the
length of the innermost loop is not a multiple of the vector length
$\mbox{\texttt{VL}}$, especially when the loops are small as in local assembly.

Data alignment is enforced in two steps. Firstly, all arrays are
allocated to addresses that are multiples of $\mbox{\texttt{VL}}$. Then,
two dimensional arrays are padded by rounding the number of columns to
the nearest multiple of $\mbox{\texttt{VL}}$. For instance, assume the
original size of a basis function array is 3$\times$3 and
$\mbox{\texttt{VL}}=4$ (e.g. AVX processor, with 256-bit long vector
registers and 64-bit double-precision floats). In this case, a padded
version of the array will have size 3$\times$4. The compiler is
explicitly told about data alignment using suitable pragmas; for
example, in the case of the Intel compiler, the annotation
\texttt{$\#$pragma vector aligned} is added before the loop (as shown in
later figures) to inform that all of the memory accesses in the loop
body will be properly aligned. This allows the compiler to issue aligned
load and store instructions, which are notably faster than unaligned
ones.

Padding of all two dimensional arrays involved in the evaluation of
the element matrix also allows to safely round the loop trip count to
the nearest multiple of $\mbox{\texttt{VL}}$. This avoids the introduction of a
remainder (scalar) loop from the compiler, which would render
vectorization less efficient. These extra iterations only write to the
padded region of the element matrix, and therefore have no side
effects on the final result.

% Autovectorizable LICM -> show code snippet?
\subsection{Generalized Loop-invariant Code Motion}
\label{sec:code-transf-licm}
From the inspection of the codes in Listings~\ref{code:helmholtz}
and~\ref{code:burgers}, it can be noticed that the computation of $A$
involves evaluating many sub-expressions which only depend on two
iteration variables. Since symbols in most of these sub-expressions
are read-only variables, there is ample space for loop-invariant code
motion. Vendor compilers apply this technique, although not in the
systematic way we need for our assembly kernels. We want to overcome
two deficiencies that both Intel and GNU compilers exhibit. First,
they only identify sub-expressions that are invariant with respect to
the innermost loop. This is an issue for sub-expressions depending on
\texttt{i}-\texttt{k}, which are not automatically lifted in the loop
order \texttt{ijk}. Second, the hoisted code is scalar and therefore not
subjected to auto-vectorization.

We work around these limitations with source-level loop-invariant code
motion. In particular, we pre-compute all values that an invariant
sub-expression assumes along its fastest varying dimension. This is
implemented by introducing a temporary array per invariant
sub-expression and by adding a new loop to the nest. At the price of
extra memory for storing temporaries, the gain is that lifted terms
can be auto-vectorized as part of an inner loop. Given the short trip
counts of our loops, it is important to achieve auto-vectorization of
hoisted terms in order to minimize the percentage of scalar
instructions, which could otherwise be significant. It is also worth
noting that, in some problems, for instance Helmholtz, invariant
sub-expressions along \texttt{j} are identical to those along
\texttt{k}, and both loops iterate over the same iteration space, as
anticipated in Section~\ref{sec:background}. In these cases, we safely
avoid redundant pre-computation.

Listing~\ref{code:helmholtz-licm} shows the Helmholtz assembly code
after the application of loop-invariant code motion, padding, and data
alignment.

\begin{algorithm}[t]
\scriptsize
\SetAlgorithmName{LISTING}{}

\KwSty{void} helmholtz(\KwSty{double} A[3][4], \KwSty{double} **coords) $\lbrace$\\
~~\KwSty{$\#$define} ALIGN $\_\_$attribute$\_\_$((aligned(32))) \\
~~// K, det = Compute Jacobian (coords) \\
~~\\
~~\KwSty{static const double} W[3] ALIGN = $\lbrace$...$\rbrace$\\
~~\KwSty{static const double} X$\_$D10[3][4] ALIGN = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\KwSty{static const double} X$\_$D01[3][4] ALIGN = $\lbrace\lbrace$...$\rbrace\rbrace$\\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$3; i++) $\lbrace$ \\
~~~~\KwSty{double} LI$\_$0[4] ALIGN;\\
~~~~\KwSty{double} LI$\_$1[4] ALIGN;\\
~~~~\KwSty{for} (\KwSty{int} r = 0; r$<$4; r++) $\lbrace$ \\
~~~~~~LI$\_$0[r] = ((K1*X$\_$D10[i][r])+(K3*X$\_$D01[i][r]));\\
~~~~~~LI$\_$1[r] = ((K0*X$\_$D10[i][r])+(K2*X$\_$D01[i][r]));\\
~~~~$\rbrace$\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{$\#$pragma vector aligned}\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k++) \\
~~~~~~~~A[j][k] += (Y[i][k]*Y[i][j]+LI$\_$0[k]*LI$\_$0[j]+LI$\_$1[k]*LI$\_$1[j])*det*W[i]);\\
~~$\rbrace$\\
$\rbrace$
\caption{Local assembly code for the Helmholtz problem in
  Listing~\ref{code:helmholtz} after application of padding, data
  alignment, and \emph{licm}, for an AVX architecture. In this
  example, sub-expressions invariant to \texttt{j} are identical to
  those invariant to \texttt{k}, so they can be precomputed once in
  the $r$ loop.}
\label{code:helmholtz-licm}
\end{algorithm}

% Register allocation and register locality
\subsection{Model-driven Vector-register Tiling}
\label{sec:op-vect}
One notable problem of assembly kernels concerns register allocation
and register locality. The critical situation occurs when loop trip
counts and the variables accessed are such that the vector-register
pressure is high. Since the kernel's working set fits the L1 cache, it
is particularly important to optimize register management. Standard
optimizations, such as loop interchange, unroll, and unroll-and-jam,
can be employed to deal with this problem. In COFFEE, these
optimizations are supported either by means of explicit code
transformations (interchange, unroll-and-jam) or indirectly by
delegation to the compiler through standard pragmas (unroll). Tiling
at the level of vector registers is an additional feature of
COFFEE. Based on the observation that the evaluation of the element
matrix can be reduced to a summation of outer products along the
\texttt{j} and \texttt{k} dimensions, a model-driven vector-register
tiling strategy can be implemented. If we consider the code snippet in
Listing~\ref{code:helmholtz-licm} and we ignore the presence of the
operation \texttt{det*W3[i]}, the computation of the element matrix is
abstractly expressible as
\begin{equation}
\scriptsize
\label{outer-product}
A_{jk} = \sum_{\substack{
  x \in B' \subseteq B \\
  y \in B'' \subseteq B}}
x_j\cdot y_k ~~~~~~ j,k = 0,...,2
\end{equation}
where $B$ is the set of all basis functions (or temporary variables,
e.g., \texttt{LI$\_$0}) accessed in the kernel, whereas $B'$ and $B''$
are generic problem-dependent subsets. Regardless of the specific
input problem, by abstracting from the presence of all variables
independent of both \texttt{j} and \texttt{k}, the element matrix
computation is always reducible to this form.
Figure~\ref{fig:vect-by-vect} illustrates how we can evaluate 16 entries
($j,k=0,...,3$) of the element matrix using just 2 vector registers,
which represent a 4$\times$4 tile, assuming $\vert B' \vert = \vert B''
\vert = 1$. Values in a register are shuffled each time a product is
performed. Standard compiler auto-vectorization for both GNU and Intel
compilers, instead, executes 4 broadcast operations (i.e., ``splat'' of
a value over all of the register locations) along the outer dimension to
perform the calculation. In addition to incurring a larger number of
cache accesses, it needs to keep between $f=1$ and $f=3$ extra registers
to perform the same 16 evaluations when unroll-and-jam is used, with $f$
being the unroll-and-jam factor.

%More importantly, we avoid using $\mbox{\texttt{VL}}$-1 registers per each
%outer-loop-dependent variable (i.e., all $x$ terms in
%Equation~\ref{outer-product}), with $\mbox{\texttt{VL}}$ being the vector
%length. This is a considerable gain, which allows us to slice the
%iteration space into bigger tiles, implemented directly by vector
%registers.

\begin{figure}[h]
\centerline{\includegraphics[scale=0.6]{Pictures/vect-by-vect-inline.pdf}}
\caption{Outer-product vectorization by permuting values in a vector
  register.}
\label{fig:vect-by-vect}
\end{figure}

The storage layout of $A$, however, is incorrect after the application
of this outer-product-based vectorization (\emph{op-vect}, in the
following). It can be efficiently restored with a sequence of vector
shuffles following the pattern highlighted in
Figure~\ref{fig:restore-layout}, executed once outside of the
\texttt{ijk} loop nest. The generated pseudo-code for the simple
Helmholtz problem when using \emph{op-vect} is shown in
Figure~\ref{code:helmholtz-opvect}.

\begin{figure}[h]
\centerline{\includegraphics[scale=0.6]{Pictures/vect-restore-inline.pdf}}
\caption{Restoring the storage layout after \emph{op-vect}. The figure
  shows how 4$\times$4 elements in the top-left block of the element
  matrix $A$ can be moved to their correct positions. Each rotation,
  represented by a group of three same-colored arrows, is implemented
  by a single shuffle intrinsic.}
\label{fig:restore-layout}
\end{figure}

%are sufficiently large to prevent register reuse along the innermost
%dimension \texttt{k}. A combination of the two scenarios is also
%possible. We want to exploit the small size of the iteration space
%and the structure of the computation to optimize register management.

\begin{algorithm}[t]
\scriptsize
\SetAlgorithmName{LISTING}{}

\begin{multicols}{2}
\KwSty{void} helmholtz(\KwSty{double} A[4][4], \KwSty{double} **coords) $\lbrace$\\
~~// K, det = Compute Jacobian (coords) \\
~~// Declaration of basis function matrices \\
~~\\
~~\KwSty{for} (\KwSty{int} i = 0; i$<$3; i++) $\lbrace$ \\
~~~~// Do generalized loop-invariant code motion \\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$4; j+=4) \\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k+=4) $\lbrace$\\
~~~~~~~~// \emph{load} and \emph{set} intrinsics \\
~~~~~~~~// Compute A[0,0], A[1,1], A[2,2], A[3,3] \\
~~~~~~~~// One \emph{permute\_pd} intrinsic per \texttt{k}-loop \emph{load}\\
~~~~~~~~// Compute A[0,1], A[1,0], A[2,3], A[3,2] \\
~~~~~~~~// One \emph{permute2f128\_pd} intrinsic per \texttt{k}-loop \emph{load}\\
~~~~~~~~// ...\\
~~~~~~$\rbrace$\\
~~~~// Remainder loop (from $j=4$ to $j=6$)\\
~~$\rbrace$\\
~~// Restore the storage layout:\\
~~\KwSty{for} (\KwSty{int} j = 0; j$<$4; j+=4) $\lbrace$\\
~~~~\KwSty{$\_\_$m256d} r0, r1, r2, r3, r4, r5, r6, r7;\\
~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k+=4) $\lbrace$\\
~~~~~~r0 = $\_$mm256$\_$load$\_$pd ($\&$A[j+0][k]);\\
~~~~~~// Load A[j+1][k], A[j+2][k], A[j+3][k]\\
~~~~~~r4 = $\_$mm256$\_$unpackhi$\_$pd (r1, r0);\\
~~~~~~r5 = $\_$mm256$\_$unpacklo$\_$pd (r0, r1);\\
~~~~~~r6 = $\_$mm256$\_$unpackhi$\_$pd (r2, r3);\\
~~~~~~r7 = $\_$mm256$\_$unpacklo$\_$pd (r3, r2);\\
~~~~~~r0 = $\_$mm256$\_$permute2f128$\_$pd (r5, r7, 32);\\
~~~~~~r1 = $\_$mm256$\_$permute2f128$\_$pd (r4, r6, 32);\\
~~~~~~r2 = $\_$mm256$\_$permute2f128$\_$pd (r7, r5, 49);\\
~~~~~~r3 = $\_$mm256$\_$permute2f128$\_$pd (r6, r4, 49);\\
~~~~~~$\_$mm256$\_$store$\_$pd ($\&$A[j+0][k], r0);\\
~~~~~~// Store A[j+1][k], A[j+2][k], A[j+3][k]\\
~~~~$\rbrace$\\
~~$\rbrace$\\
$\rbrace$
\end{multicols}
\caption{Local assembly code generated by Firedrake for the Helmholtz
  problem after application of \emph{op-vect} on top of the
  optimizations shown in Listing~\ref{code:helmholtz-licm}. In this
  example, the unroll-and-jam factor is 1.}
\label{code:helmholtz-opvect}
\end{algorithm}



% Splitting
\subsection{Expression Splitting}
\label{sec:expr-split}

\begin{algorithm}
\scriptsize
\SetAlgorithmName{LISTING}{}

\KwSty{void} helmholtz(\KwSty{double} A[3][4], \KwSty{double} **coords) $\lbrace$\\
~~~~// Same code as in Listing~\ref{code:helmholtz-licm} up to the j loop\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{$\#$pragma vector aligned}\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k++) \\
~~~~~~~~A[j][k] += (Y[i][k]*Y[i][j]+LI$\_$0[k]*LI$\_$0[j])*det*W3[i];\\
~~~~\KwSty{for} (\KwSty{int} j = 0; j$<$3; j++) \\
~~~~~~\KwSty{$\#$pragma vector aligned}\\
~~~~~~\KwSty{for} (\KwSty{int} k = 0; k$<$4; k++) \\
~~~~~~~~A[j][k] += LI$\_$1[k]*LI$\_$1[j]*det*W3[i];\\
$\rbrace$
\caption{Local assembly code generated by Firedrake for the Helmholtz
  problem in which \emph{split} has been applied on top of the
  optimizations shown in Listing~\ref{code:helmholtz-licm}. In this
  example, the split factor is 2.}
\label{code:helmholtz-split}
\end{algorithm}

In complex kernels, like Burgers in Listing~\ref{code:burgers}, and on
certain architectures, achieving effective register allocation can be
challenging. If the number of variables independent of the
innermost-loop dimension is close to or greater than the number of
available CPU registers, poor register reuse is likely. This usually
happens when the number of basis function arrays, temporaries introduced
by generalized loop-invariant code motion, and problem constants is
large. For example, applying loop-invariant code motion to Burgers on a
3D mesh requires 24 temporaries for the \texttt{ijk} loop order. This
can make hoisting of the invariant loads out of the \texttt{k} loop
inefficient on architectures with a relatively low number of registers.
One potential solution to this problem consists of suitably
``splitting'' the computation of the element matrix $A$ into multiple
sub-expressions; an example, for the Helmholtz problem, is given in
Listing~\ref{code:helmholtz-split}. The transformation can be regarded
as a special case of classic loop fission, in which associativity of the
sum is exploited to distribute the expression across multiple loops. To
the best of our knowledge, expression splitting is not supported by
available compilers.

Splitting an expression (henceforth \emph{split}) has, however, several drawbacks. Firstly, it
increases the number of accesses to $A$ proportionally to the ``split
factor'', which is the number of sub-expressions produced. Also,
depending on how the split is executed, it can lead to redundant
computation. For example, the number of times the product $det*W3[i]$ is
performed is proportional to the number of sub-expressions, as shown in
the code snippet. Further, it increases loop overhead, for example
through additional branch instructions. Finally, it might affect
register locality: for instance, the same array could be accessed in
different sub-expressions, requiring a proportional number of loads be
performed. This is not the case for the Helmholtz example. Nevertheless,
as shown in Section~\ref{sec:perf-results}, the performance gain from
improved register reuse along inner dimensions can still be greater,
especially if the split factor and the splitting itself use heuristics
to minimize the aforementioned issues.
~\\

Some of the transformations presented in this section, as well as other typical compiler optimizations applicable by COFFEE, like loop interchange and unrolling, are parametric; for instance, expression splitting and loop unrolling depend on the split and the unroll factors, respectively. Since all transformations are composable, the optimization space can be as large as order of thousands variants for local assembly kernels in which mathematical expressions are moderately complex and the iteration space is not particularly small, so a strategy to prune it is required. This problem is treated in the following section.

\section{Overview of COFFEE}
\label{sec:pyop2-compiler}
%Design and structure of the PyOP2 Compiler. Show the steps through
%which the IR is transformed (need to cite Firedrake here). Briefly
%describe a simple cost model that allows us to prune the space of
%code transformations.

% FIXME: ``mesh used to discretize''
%\rule[0.2ex]{16cm}{0.5pt}
\begin{figure}[b]
\begin{center}
\includegraphics[scale=0.70]{Pictures/coffee-pipeline.pdf}
\caption{High-level view of Firedrake. COFFEE is at the core,
  receiving ASTs from a modified version of the FEniCS Form Compiler
  and producing optimized C code kernels.}
\label{fig:coffee-pipeline}
\end{center}
\end{figure}

Firedrake users employ the Unified Form Language to express problems in
a notation resembling mathematical equations. At run-time, the
high-level specification is translated by a modified version of the
FEniCS Form Compiler (FFC)~\cite{FFC-Compiler} into an abstract syntax
tree (AST) representation of one or more finite element assembly
kernels. ASTs are then passed to COFFEE to apply the transformations
described in Section~\ref{sec:code-transf}. The output of COFFEE, C
code, is eventually provided to PyOP2~\cite{pyop2ws,pyop2isc}, where
just-in-time compilation and execution over the discretized domain take
place. The flow and the compiler structure are outlined in Figure~\ref{fig:coffee-pipeline}.

COFFEE uses heuristics and a simple cost model to select the most suitable
optimization strategy for a given problem. Auto-tuning might be used, although
it would significantly increase the  run-time overhead, since the generation of
ASTs occurs at run-time as soon as problem-specific data are available.

The compiler applies an ordered sequence of optimization steps to the
ASTs received from FFC. Application of \emph{licm} must precede padding
and data alignment, due to the introduction of temporary arrays. These
transformations are always performed because they are likely to improve
the run-time performance, as demonstrated by our results in Section~\ref{sec:analysis-code-transf}.
Based on a cost model (described later), loop interchange, \emph{split},
and \emph{op-vect} may be introduced. Their implementation is centered
on analysis and manipulation of the kernel AST. When \emph{op-vect}
is selected, the compiler outputs AVX or AVX-512 intrinsics code. Any
possible corner cases are handled: for example, if \emph{op-vect} is to
be applied, but the size of the iteration space is not a multiple of the vector
length, then a remainder loop, amenable to auto-vectorization, is inserted.

\paragraph{Loop interchange}
All loops are interchangeable, provided that temporaries are
introduced if the nest is not perfect. For the employed storage
layout, the loop permutations \texttt{ijk} and \texttt{ikj} are likely
to maximize performance. Conceptually, this is motivated by the fact
that if the \texttt{i} loop were in an inner position, then a
significantly higher number of load instructions would be required at
every iteration. We tested this hypothesis in manually crafted
kernels. We found that the performance loss is greater than the gain
due to the possibility of accumulating increments in a register,
rather than memory, along the \texttt{i} loop. The choice between
\texttt{ijk} and \texttt{ikj} depends on the number of load
instructions that can be hoisted out of the innermost dimension. Our
compiler chooses as outermost the loop along which the number of
invariant loads is smaller so that more registers remain available to
carry out the computation of the element matrix.

% "vendor compilers such as Intel's" is awkward and unclear. If Intel
% is the only one you have looked at, then say "the Intel compiler",
% otherwise name the ones for which you know this to be true.
\paragraph{Loop unrolling}
Loop unroll (or unroll-and-jam of outer loops) is fundamental to the
exposure of instruction-level parallelism, and tuning unroll
factors is particularly important.

We first observe that manual full (or extensive) unrolling is unlikely to be effective for two reasons. Firstly, the \texttt{ijk} loop nest would need to be small enough such that the unrolled instructions do not exceed the instruction cache, which is rarely the case: it is true that in a local assembly kernel the minimum size of the \texttt{ijk} loop nest is 3$\times$3$\times$3 (triangular mesh and polynomial order 1), but this increases rapidly with the polynomial order of the method and the discretization employed (e.g. tetrahedral meshes imply larger loop nests than triangular ones), so sizes greater than 10$\times$10$\times$10, for which extensive unrolling would already be harmful, are in practice very common. Secondly, manual unrolling is dangerous because it may compromise compiler auto-vectorization by either removing loops (most compilers search for vectorizable loops) or losing spatial locality within a vector register.

By comparison with implementations characterized by manually-unrolled loops, we noticed that recent
versions of compilers like GNU's and Intel's estimate close-to-optimal unroll
factors when the loops are affine and their bounds are relatively small and known
at compile-time, which is the case of our kernels. Our choice, therefore, is to leave
the backend compiler in charge of selecting unroll factors. This
also simplifies COFFEE's cost model, described next. The only situation in
which we explicitly unroll-and-jam a loop is when \emph{op-vect} is
used, since the transformed code prevents the Intel compiler from
applying this optimization, even if specific pragmas are added.

%Note that, regardless of the output of the cost model, the
%unroll-and-jam factor never exceeds the actual size of the outer loop
%(line 19 in Figure~\ref{algo:PyOP2Compiler}, assuming the default
%loop order \texttt{ijk}).

\begin{figure}
\begin{alltt}\scriptsize\internallinenumbers
\textbf{Input:} n\_outer\_arrays, n\_inner\_arrays, n\_consts, n\_regs
\textbf{Output:} uaj\_factor, split\_factor
n\_outer\_regs = n\_regs / 2
split\_factor = 0
\textbf{// Compute spltting factor }
\textbf{while} n\_outer\_arrays > n\_outer\_regs
  n\_outer\_arrays = n\_outer\_arrays / 2
  split\_factor = split\_factor + 1
\textbf{// Compute unroll-and-jam factor for \emph{op-vect}}
n\_regs\_avail = n\_regs - (n\_outer\_arrays + n\_consts)
uaj\_factor = n\_reg\_avail / n\_inner\_arrays
\textbf{// Estimate the benefit of permuting loops}
permute = n\_outer\_arrays > n\_inner\_arrays
\textbf{return} <permute, split\_factor, uaj\_factor>
\end{alltt}
\caption{The cost model is employed by the compiler to estimate the
  most suitable unroll-and-jam (when \emph{op-vect} is used) and split
  factors, avoiding the overhead of auto-tuning.}
\label{algo:applyCostModel}
\end{figure}

\paragraph{Cost model}
The cost model is shown in Figure~\ref{algo:applyCostModel}. It takes
into account the number of available logical vector registers,
\texttt{n\_regs}, and the number of unique variables accessed:
\texttt{n\_consts} counts variables independent of both \texttt{j} and
\texttt{k} loops and temporary registers, \texttt{n\_outer\_arrays}
counts \texttt{j}-dependent variables, and \texttt{n\_inner\_arrays}
counts \texttt{k}-dependent variables, assuming the \texttt{ijk} loop
order. These values are used to estimate unroll-and-jam and split
factors for \emph{op-vect} and \emph{split}. If a factor is 0, then
the corresponding transformation is not applied. The \emph{split}
transformation is triggered whenever the number of hoistable terms is
larger than the available registers along the outer dimension (lines
3-8), which is approximated as half of the total (line 3). A split
factor of $n$ means that the assembly expression should be ``cut''
into $n$ sub-expressions. Depending on the structure of the assembly
expression, each sub-expression might end up accessing a different
number of arrays; the cost model is simplified by assuming that all
sub-expressions are of the same size. The unroll-and-jam factor for
the \emph{op-vect} transformation is determined as a function of the
available logical registers, i.e., those not used for storing hoisted
terms (line 9-11). Finally, the profitability of loop interchange is
evaluated (line 13).


\section{Performance Evaluation}
\label{sec:perf-results}

\subsection{Experimental Setup}

Experiments were run on a single core of two Intel architectures, a
Sandy Bridge (I7-2600 CPU, running at 3.4GHz, 32KB L1 cache and 256KB
L2 cache) and a Xeon Phi (5110P, running at 1.05Ghz in native mode,
32KB L1 cache and 512KB L2 cache). We have chosen these two
architectures because of the differences in the number of logical
registers and SIMD lanes (16 256-bit registers in the Sandy Bridge,
and 32 512-bit registers in the Xeon Phi), which can impact the
optimization strategy. The \texttt{icc 13.1} compiler was used. 
We selected the best optimization levels for each platform using the
following compilation flags: \texttt{-xAVX} for auto-vectorization and
\texttt{-O2} were used on the Sandy Bridge; \texttt{-O3} was adopted
on the Xeon Phi.

We present two studies. Firstly, in
Section~\ref{sec:analysis-code-transf}, we analyze the impact of the
proposed code transformations, and their interplay, in three real-world
representative equations. We quantify the speedups achievable at the
level of the local assembly kernel, by excluding the other stages of the
computation from the measurements. Then, in Section~\ref{sec:full-app},
we provide a full application investigation. Here, we use another
equation, bringing the total number of examined problems to four,
to demonstrate that COFFEE's optimizations actually allow a significant
reduction in the overall computation run-time to be achieved, which
practically motivates our research.

\subsection{In-depth Performance Analysis of the Code Transformations and their Interplay}
\label{sec:analysis-code-transf}

\subsubsection{Choice and Properties of the Benchmarks}
\label{sec:real-examples}
Our code transformations were evaluated in three real-world problems
based on the following PDEs: (i) Helmholtz, (ii) Advection-Diffusion, 
and (iii) Burgers.

The three chosen benchmarks are \emph{real-life kernels} and comprise
the core differential operators in some of the most frequently
encountered finite element problems in scientific computing. This is
of crucial importance because distinct problems, possibly arising  in
completely different fields, may employ (subsets of) the same differential
operators of our benchmarks, which implies similarities
and redundant patterns in the generated code. Consequently, the proposed
code transformations have a domain of applicability that goes far beyond
that of the three analyzed equations.

The Helmholtz and Diffusion kernels are archetypal second order elliptic
operators. They are complete and unsimplified examples of the
operators used to model diffusion and viscosity in fluids, and for
imposing pressure in compressible fluids. As such, they are both
extensively used in climate and ocean modeling. Very similar
operators, for which the same optimisations are expected to be equally
effective, apply to elasticity problems, which are at the base of
computational structural mechanics. The Burgers kernel is a typical
example of a first order hyperbolic conservation law, which occurs in
real applications whenever a quantity is transported by a fluid (the
momentum itself, in our case). We chose this particular kernel since
it applies to a vector-valued quantity, while the elliptic operators
apply to scalar quantities; this impacts the generated code, as
explained next. The operators we have selected are characteristic of
both the second and first order operators that dominate fluids and
solids simulations.
%and a wide variety on local assembly codes can be tested.

The benchmarks were written in UFL (code available at~\cite{ufl-code})
and executed over real unstructured meshes through Firedrake. The
Helmholtz code has already been shown in
Listing~\ref{code:helmholtz}. For Advection-Diffusion, the Diffusion
equation, which uses the same differential operators as Helmholtz, is
considered. In the Diffusion kernel code, the main differences with
respect to Helmholtz are the absence of the $Y$ array and the presence
of additional constants for computing the element matrix. Burgers is a
non-linear problem employing differential operators different from
those of Helmholtz and relying on vector-valued quantities, which has
a major impact on the generated assembly code (see
Listing~\ref{code:burgers}), where a larger number of basis function
arrays ($X1$, $X2$, ...) and constants ($F0$, $F1$, ..., $K0$, $K1$,
...) are generated.

These problems were studied varying both the shape of mesh elements
and the polynomial order $p$ of the method, whereas the element
family, Lagrange, is fixed. As might be expected, the larger the
element shape and $p$, the larger the iteration space. Triangles,
tetrahedra, and prisms were tested as element shape. For instance, in
the case of Helmholtz with $p=1$, the size of the \texttt{j} and
\texttt{k} loops for the three element shapes is, respectively, $3$,
$4$, and $6$. Moving to bigger shapes has the effect of increasing the
number of basis function arrays, since, intuitively, the behaviour of
the equation has to be approximated also along a third axis. On
the other hand, the polynomial order affects only the problem size
(the three loops \texttt{i}, \texttt{j}, and \texttt{k}, and, as a
consequence, the size of $X$ and $Y$ arrays). A range of polynomial
orders from $p=1$ to $p=4$ were tested; higher polynomial orders are
excluded from the study because of current Firedrake limitations. In
all these cases, the size of the element matrix rarely exceeds
30$\times$30, with a peak of 105$\times$105 in Burgers with prisms and
$p=4$.

\subsubsection{Loop interchange}
In the following, only results for the loop order \texttt{ijk} are
shown. For the considerations exposed in
Section~\ref{sec:pyop2-compiler}, loop interchanges having an inner
loop along \texttt{i} caused slowdowns; also, interchanging
\texttt{j} and \texttt{k} loops while keeping \texttt{i} as outermost
loop did not provide any benefits.

\subsubsection{Impact of Generalized Loop-invariant Code Motion}
\label{licm-impact}

\newcommand{\licmresultsnorms}{
\begin{tabulary}{1.0\textwidth}{cccccc|cccc}
\cline{3-10}
& & \multicolumn{4}{c}{\texttt{Sandy Bridge}} & \multicolumn{4}{c}{\texttt{Xeon Phi}} \\
\cline{1-10}
\texttt{problem} & \texttt{shape} & \texttt{p1} & \texttt{p2} & \texttt{p3} & \texttt{p4} & \texttt{p1} & \texttt{p2} & \texttt{p3} & \texttt{p4} \\
\texttt{Helmholtz} & \texttt{triangle} & 1.05 & 1.46 & 1.68 & 1.67 & 1.49 & 1.06 & 1.05 & 1.17 \\
\texttt{Helmholtz} & \texttt{tetrahedron} & 1.36 & 2.10 & 2.64 & 2.27 & 1.28 & 1.29 & 2.05 & 1.73 \\
\texttt{Helmholtz} & \texttt{prism} & 2.16 & 2.28 & 2.45 & 2.06 & 1.04 & 2.26 & 1.93 & 1.64 \\[0.1cm]
%\cline{1-18}
\texttt{Diffusion} & \texttt{triangle} & 1.09 & 1.68 & 1.97 & 1.64 & 1.07 & 1.06 & 1.18 & 1.16 \\
\texttt{Diffusion} & \texttt{tetrahedron} & 1.30 & 2.20 & 3.12 & 2.60 & 1.00 & 1.38 & 2.02 & 1.74\\
\texttt{Diffusion} & \texttt{prism} & 2.15 & 1.82 & 2.71 & 2.32 & 1.11 & 2.16 & 1.85 & 2.83\\[0.1cm]
%\cline{1-18}
\texttt{Burgers} & \texttt{triangle} & 1.53 & 1.81 & 2.68 & 2.46 & 1.21 & 1.42 & 2.34 & 2.97  \\
\texttt{Burgers} & \texttt{tetrahedron} & 1.61 & 2.24 & 1.69 & 1.59 & 1.01 & 2.55 & 0.98 & 1.21  \\
\texttt{Burgers} & \texttt{prism} & 2.11 & 2.20 & 1.66 & 1.32 & 1.39 & 1.56 & 1.18 & 1.04 \\
\cline{1-10}
\end{tabulary}
}

\begin{table*}[t]
\tbl{Performance improvement due to generalized loop-invariant code
  motion over the original non-optimized code.}{ \scriptsize \licmresultsnorms }
\label{table:perf-results-licm}
\end{table*}
Table~\ref{table:perf-results-licm} illustrates the performance
improvement obtained when \emph{licm} is applied. In general, the
speedups are notable. The main reasons were anticipated in
Section~\ref{sec:code-transf-licm}: in the original code, 1)
sub-expressions invariant to outer loops are not automatically
hoisted, while 2) sub-expressions invariant to the innermost loop are
hoisted, but their execution is not auto-vectorized. These
observations come from inspection of assembly code generated by the
compiler.

The gain tends to grow with the computational cost of the kernels:
bigger loop nests (i.e., larger element shapes and polynomial orders)
usually benefit from the reduction in redundant computation, even
though extra memory for the temporary arrays is required. Some
discrepancies to this trend are due to a less effective
auto-vectorization. For instance, on the Sandy Bridge, the improvement
at $p=3$ is larger than that at $p=4$ because, in the latter case, the
size of the innermost loop is not a multiple of the vector length, and
a remainder scalar loop is introduced at compile time. Since the loop
nest is small, the cost of executing the extra scalar iterations can
have a significant impact. The remainder loop overhead is more
pronounced on the Xeon Phi, where the vector length is twice as long,
which leads to proportionally larger scalar remainder loops.


\subsubsection{Impact of Padding and Data Alignment}

\newcommand{\licmapresultsnorms}{
\begin{tabulary}{1.0\textwidth}{cccccc|cccc}
\cline{3-10}
& & \multicolumn{4}{c}{\texttt{Sandy Bridge}} & \multicolumn{4}{c}{\texttt{Xeon Phi}} \\
\cline{1-10}
\texttt{problem} & \texttt{shape} & \texttt{p1} & \texttt{p2} & \texttt{p3} & \texttt{p4} & \texttt{p1} & \texttt{p2} & \texttt{p3} & \texttt{p4} \\[0.1cm]
\texttt{Helmholtz} & \texttt{triangle} & 1.32 & 1.88 & 2.87 & 4.13 & 1.50 & 2.41 & 1.30 & 1.96 \\
\texttt{Helmholtz} & \texttt{tetrahedron} & 1.35 & 3.32 & 2.66 & 3.27 & 1.41 & 1.50 & 2.79 & 2.81 \\
\texttt{Helmholtz} & \texttt{prism} & 2.63 & 2.74 & 2.43 & 2.75 & 2.38 & 2.47 & 2.15 & 1.71\\[0.1cm]
%\cline{1-18}
\texttt{Diffusion} & \texttt{triangle} & 1.38 & 1.99 & 3.07 & 4.28 & 1.08 & 1.88 & 1.20 & 1.97\\
\texttt{Diffusion} & \texttt{tetrahedron} & 1.41 & 3.70 & 3.18 & 3.82 & 1.05 & 1.51 & 2.76 & 3.00\\
\texttt{Diffusion} & \texttt{prism} & 2.55 & 3.13 & 2.73 & 2.69 & 2.41 & 2.52 & 2.05 & 2.48\\[0.1cm]
%\cline{1-18}
\texttt{Burgers} & \texttt{triangle} & 1.56 & 2.28 & 2.61 & 2.77 & 2.84 & 2.26 & 3.96 & 4.27 \\
\texttt{Burgers} & \texttt{tetrahedron} & 1.61 & 2.10 & 1.60 & 1.78 & 1.48 & 3.83 & 1.55 & 1.29 \\
\texttt{Burgers} & \texttt{prism} & 2.19 & 2.32 & 1.64 & 1.42 & 2.18 & 2.82 & 1.24 & 1.25 \\
\cline{1-10}
\end{tabulary}
}

\begin{table*}[t]
\tbl{Performance improvement due to generalized loop-invariant code
  motion, data alignment, and padding over the original non-optimized code.}{ \scriptsize \licmapresultsnorms }
\label{table:perf-results-licmap}
\end{table*}

Table~\ref{table:perf-results-licmap} shows the cumulative impact of
\emph{licm}, data alignment, and padding over the original code. In
the following, this version of the code is referred to as
\emph{licm-ap}. Padding, which avoids the introduction of a remainder
loop as described in Section~\ref{licm-impact}, as well as data
alignment, enhance the quality of auto-vectorization. Occasionally the
run-time of \emph{licm-ap} is close to that of \emph{licm}, since the
non-padded element matrix size is already a multiple of the vector
length. Rarely \emph{licm-ap} is slower than \emph{licm} (e.g. in
Burgers $p=3$ on the Sandy Bridge). One possible explanation is that
the number of aligned temporaries introduced by \emph{licm} is so
large to induce cache associativity conflicts.

\subsubsection{Impact of Vector-register Tiling}
\label{sec:perf-eval-opvect}
In this section, we evaluate the impact of vector-register tiling. We
compare two versions: the baseline, \emph{licm-ap}; and
vector-register tiling on top of \emph{licm-ap}, which in the
following is referred to simply as \emph{op-vect}.

Figures~\ref{fig:opvect-helmholtz-speedup}
and~\ref{fig:opvect-diffusion-speedup} illustrate the speed up
achieved by \emph{op-vect} over \emph{licm-ap} in the Helmoltz and
Diffusion kernels, respectively. As explained in
Section~\ref{sec:pyop2-compiler}, the \emph{op-vect} version requires
the unroll-and-jam factor to be explicitly set. To distinguish between
the two ways this parameter was determined, for each problem instance
(equation, element shape, polynomial order) we report two bars: one
shows the best speed-up obtained after all feasible unroll-and-jam
factors were tried; the other shows the speed up when the
unroll-and-jam factor was retrieved via the cost model. In a plot
legend, cost model bars are suffixed with ``CM''.

It is worth noticing that, in most cases, the cost model successfully
determines how to transform a kernel to maximize its performance. This
is chiefly because assembly kernels fit the L1 cache, so, within a
certain degree of confidence, it is possible to predict how to obtain
a fast implementation by simply reasoning on the register
pressure. For each problem, the cost model stated to use the default
loop permutation, to apply a particular unroll-and-jam factor, and not
to perform expression splitting, which, as explained in
Section~\ref{sec:perf-results-split}, only deteriorates performance in
Helmholtz and Diffusion.

The rationale behind these results is that the effect of
\emph{op-vect} is significant in problems in which the assembly loop
nest is relatively big. When the loops are short, since the number of
arrays accessed at every loop iteration is rather small (between 4 and
8 temporaries, plus the element matrix itself), there is no need for
vector-register tiling; extensive unrolling is sufficient to improve
register re-use and, therefore, to maximize the performance. However,
as the iteration space becomes larger, \emph{op-vect} leads to
improvements up to 1.4$\times$ on the Sandy Bridge (Diffusion,
prismatic mesh, $p=4$ - increasing the overall speed up from
2.69$\times$ to 3.87$\times$), and up to 1.4$\times$ on the Xeon Phi
(Helmholtz, tetrahedral mesh, $p=3$ - bringing the overall speed up
from 1.71$\times$ to 2.42$\times$).

Using the Intel Architecture Code Analyzer tool~\cite{IACA} on the
Sandy Bridge, we confirmed that speed ups are a consequence of
increased register re-use. In Helmholtz $p=4$, for example, the tool
showed that when using \emph{op-vect} the number of clock cycles to
execute one iteration of the \texttt{j} loop decreases by roughly
17$\%$, and that this is a result of the relieved pressure on both of
the data (cache) ports available in the core.

On the Sandy Bridge, we have also measured the performance of
individual kernels in terms of floating-point operations per
second. The theoretical peak on a single core, with the Intel Turbo
Boost technology activated, is 30.4 GFlop/s. In the case of Diffusion
using a prismatic mesh and $p=4$, we achieved a maximum of 21.9
GFlop/s with \emph{op-vect} enabled, whereas 16.4 GFlop/s was obtained
when only \emph{licm-ap} is used. This result is in line with the
expectations: analysis of assembly code showed that, in the
\texttt{jk} loop nest, which in this problem represents the bulk of
the computation, 73$\%$ of instructions are actually floating-point
operations.

Application of \emph{op-vect} to the Burgers problem induces
significant slowdowns due to the large number of temporary arrays
that need to be tiled, which exceeds the available logical registers
on the underlying architecture. Expression splitting can be used in
combination with \emph{op-vect} to alleviate this issue; this is
discussed in the next section.

\begin{figure}[t]
	\centering
	\subfloat[Sandy Bridge]{	\includegraphics[scale=0.48]{Pictures/helmholtz-normalized-opvect.pdf}}
	\subfloat[Xeon Phi]{	\includegraphics[scale=0.48]{Pictures/helmholtz-normalized-opvect-phi.pdf}}
	\caption{Performance improvement over \emph{licm-ap} obtained
          by \emph{op-vect} in the Helmholtz kernel. Bars suffixed
          with ``CM'' indicate that the cost model was used to
          transform the kernel.}
	\label{fig:opvect-helmholtz-speedup}
\end{figure}

\begin{figure}
	\centering
	\subfloat[Sandy Bridge]{	\includegraphics[scale=0.48]{Pictures/diffusion-normalized-opvect.pdf}}
	\subfloat[Xeon Phi]{	\includegraphics[scale=0.48]{Pictures/diffusion-normalized-opvect-phi.pdf}}
	\caption{Performance improvement over \emph{licm-ap} obtained
          by \emph{op-vect} in the Diffusion kernel. Bars suffixed
          with ``CM'' indicate that the cost model was used to
          transform the kernel.}
	\label{fig:opvect-diffusion-speedup}
\end{figure}

%for which efficient register allocation can be already guaranteed

\subsubsection{Impact of Expression Splitting}
\label{sec:perf-results-split} Expression splitting relieves the
register pressure when the element matrix evaluation needs to read from
a large number of basis function arrays. As detailed in
Section~\ref{sec:expr-split}, the price to pay for this optimazion is an
increased number of accesses to the element matrix and, potentially,
redundant computation. Similarly to the analysis of vector-register
tiling, we compare two versions: the baseline, \emph{licm-ap}, and
expression splitting on top of \emph{licm-ap}, which, for simplicity, in
the following is referred to as \emph{split}.

For the Helmholtz and Diffusion kernels, in which only between 4 and 8
temporaries are read at every loop iteration, \texttt{split} tends to
slow down the computation, because of the aforementioned
drawbacks. Slow downs up to 1.4$\times$ and up to 1.6$\times$ were
observed, respectively, on the Sandy Bridge and the Xeon Phi. Note
that the cost model prevents the adoption of the transformation: the
\texttt{while} statement in Figure~\ref{algo:applyCostModel} is never
entered.

In the Burgers kernels, between 12 and 24 temporaries are accessed at
every loop iteration, so \emph{split} plays a key role on the Sandy
Bridge, where the number of available logical registers is only
16. Figure~\ref{fig:split-burgers-speedup} shows the performance
improvement achieved by \emph{split} over \emph{licm-ap}. In almost
all cases, a split factor of 1, meaning that the original expression
was divided into two parts, ensured close-to-peak perforance. The
transformation negligibly affected register locality, so speed ups up
to 1.5$\times$ were observed. For instance, on the Sandy Bridge, when
$p=4$ and a prismatic mesh is employed, the overall performance
improvement (i.e., the one over the original code) increases from
1.44$\times$ to 2.11$\times$. On the Xeon Phi, the impact of
\emph{split} is only marginal, since register spilling is limited by
the presence of 32 logical vector units.

On the Sandy Bridge, the performance of the Burgers kernel on a
prismatic mesh was 20.0 GFlop/s from $p=1$ to $p=3$, while it was 21.3
GFlop/s in the case of $p=4$. These values are notably close to the
peak performance of 30.4 GFlop/s. Disabling \emph{split} makes the
performance drop to 17.0 GFlop/s for $p=1, 2$, 18.2 GFlop/s for $p=3$,
and 14.3 GFlop/s for $p=4$. These values are in line with the speedups
shown in Figure~\ref{fig:split-burgers-speedup}.

The \emph{split} transformation was also tried in combination with
\emph{op-vect} (\emph{split-op-vect}), although the cost model
prevents its adoption on both platforms. Despite improvements up to
1.22$\times$, \emph{split-op-vect} never outperforms
\emph{split}. This is motivated by two factors: for small split
factors, such as 1 and 2, the data space to be tiled is still too big,
and register spilling affects run-time; for higher ones,
sub-expressions become so small that, as explained in
Section~\ref{sec:perf-eval-opvect}, extensive unrolling already allows
to achieve a certain degree of register re-use.

\begin{figure}
	\centering
	\subfloat[Sandy Bridge]{	\includegraphics[scale=0.48]{Pictures/burgers-normalized-split.pdf}}
	\subfloat[Xeon Phi]{	\includegraphics[scale=0.48]{Pictures/burgers-normalized-split-phi.pdf}}
	\caption{Performance improvement over \emph{licm-ap} obtained
          by \emph{split} in the Burgers kernel. Bars suffixed with
          ``CM'' indicate that the cost model was used to transform
          the kernel.}
	\label{fig:split-burgers-speedup}
\end{figure}

\subsubsection{Comparison with FEniCS Form Compiler's built-in Optimizations}
\label{sec:perf-results-fenics}
We have modified the FEniCS Form Compiler (FFC) to return an abstract
syntax tree representation of a local assembly kernel, rather than
plain C++ code, so as to enable code transformations in
COFFEE. Besides Firedrake, FFC is used in the FEniCS
project~\cite{Fenics}. In FEniCS, FFC can apply its own model-driven
optimizations to local assembly kernels~\cite{quadrature1}, which
mainly consist of loop-invariant code motion and elimination, at code
generation time, of floating point operations involving zero-valued
entries in basis function arrays.

The FEniCS Form Compiler's loop-invariant code motion is different
from COFFEE's. It is based on expansion of arithmetic operations, for
example applying distributivity and associativity to products and sums
at code generation time, to identify terms that are invariant of the
whole loop nest. Depending on the way expansion is performed,
operation count may not decrease significantly.

Elimination of zero-valued terms, which are the result of using
vector-valued quantities in the finite element problem, has the effect
of introducing indirection arrays in the generated code. This kind of
optimization is currently under development in COFFEE, although it
will differ from FEniCS' by avoiding non-contiguous memory accesses,
which would otherwise affect vectorization, at the price of removing
fewer zero-valued contributions.

% FIXME: fastest quadrature or does that include tensor representation?

Table~\ref{table:comparison-to-FFC-opt} summarizes the performance
achieved by COFFEE over the \emph{fastest} FEniCS (FFC) implementation
on the Sandy Bridge for the Burgers, Helmholtz and Diffusion
kernels. Burgers' slow downs occur in presence of a small iteration
space (triangular mesh, $p \in [1, 2]$; tetrahedral mesh, $p \in [1,
  2]$; prismatic mesh, $p = 1$). The result shown represents the worst
slow down, which was obtained with a triangular mesh and $p = 1$. This
is a result of removing zero-valued entries in FEniCS' basis function
arrays: some operations are avoided, but indirection arrays prevent
auto-vectorization, which significantly impacts performance as soon as
the element matrix becomes bigger than 12$\times$12. However, with the
forthcoming zero-removal optimization in COFFEE, we expect to
outperform FEniCS in all problems. In the cases of Helmholtz and
Diffusion, the minimum improvements are, respectively, 1.10$\times$
and 1.18$\times$ (2D mesh, $p=1$), which tend to increase with
polynomial order and element shape up to the values illustrated in the
table.

\begin{table}[h]
\tbl{Performance comparison between FEniCS and COFFEE on Sandy Bridge.}{
\scriptsize
\begin{tabulary}{1.0\columnwidth}{C|C|C}
\hline
Problem & Max slow down & Max speed up \\\hline
Helmholtz & - & 4.14$\times$ \\
Diffusion & - & 4.28$\times$ \\
Burgers & 2.24$\times$ & 1.61$\times$ \\\hline
\end{tabulary}
}
\label{table:comparison-to-FFC-opt}
\end{table}

\subsubsection{Comparison with hand-made BLAS-based implementations}
\label{sec:perf-eval-blas}
For the Helmholtz problem on a tetrahedral mesh, manual
implementations based on \texttt{Intel MKL BLAS} were tested on the
Sandy Bridge. This particular kernel can be easily reduced to a
sequence of four matrix-matrix multiplies that can be computed via
calls to BLAS \texttt{dgemm}. In the case of $p=4$, where the element
matrix is of size 35$\times$35, the computation was almost twice
slower than the case in which \emph{licm-ap} was used, with the slow
down being even worse for smaller problem sizes. These experiments
suggest that the question regarding to what extent linear algebra
libraries can improve performance cannot be trivially answered. This
is due to a combination of issues: the potential loss in data
locality, as exposed in Section~\ref{sec:expr-split}, the actual
effectiveness of these libraries when the arrays are relatively small,
and the problem inherent to assembly kernels concerning extraction of
matrix-matrix multiplies from static analysis of the kernel's code. A
comprehensive study of these aspects will be addressed in further
work.

\subsubsection{On the compilation time}
\label{sec:compile-time}
Firedrake and its various software components (see Figure~\ref{fig:coffee-pipeline}) are implemented in Python and Cython, whereas the generated code is pure C. COFFEE has been written in Python to be naturally integrated within Firedrake. Among the various Firedrake modules, COFFEE is where, in general, the least amount of time is spent. The process of transforming the abstract syntax tree usually takes order of milliseconds. For instance, for the Helmholtz equation with $p=4$ on a tiny tetrahedral mesh, it took 0.0009s to transform the abstract syntax tree, while the whole assembly process needed 0.384s. Being a domain-specific compiler, COFFEE expects C code with the structure outlined in Figure~\ref{code:general-structure}, so typical compilers functionalities such as dependency tracking are simplified with respect to a general-purpose compiler, which implies a clear advantage in compilation time and implementation complexity.

\subsection{Full application study}
\label{sec:full-app}
In this section, we investigate the performance gain for an entire finite element computation
developed in Firedrake, a static linear elasticity problem. The source code is available at~\cite{elasticity-code}. The equation is used to simulate deformation of an object caused by pre-established loading conditions.

The execution time of a steady finite element problem is dominated by two factors: assembly and solve. The evaluation of all local element matrices and vectors, and their insertion, respectively, in a ``global'' sparse matrix and a global vector, compose the ``assembly'' phase. The global matrix and vector form a linear system, usually solved by an iterative method; this is the ``solve'' phase. The percentage of time spent in assembly and solve varies from problem to problem. As reiterated in the available literature, for example in~\cite{quadrature1}, the computational cost of local assembly grows with the \textit{complexity} of the partial differential equation (e.g. because of larger loops and more articulated expressions), while the solve time increases with polynomial order and mesh discretization. The complexity of an equation depends on several factors, including the number of derivatives and coefficient functions (i.e. additional known functions that characterize the equation).

We study two versions of the static linear elasticity problem: 1) with one coefficient function, $f=1$, and 2) with two coefficient functions, $f=2$. More coefficient functions are also plausible; however, as we will see, the assembly process starts being particularly expensive already with $f=1$. For each of these two versions, we examine the cases of polynomial order $p=1$ and $p=2$. To run our experiments, we use a single core of the Sandy Bridge architecture described in Section~\ref{sec:perf-results}. The application runs on a tetrahedral mesh composed of 196608 elements.

\begin{figure}
	\centering
	\subfloat[]{\includegraphics[scale=0.48]{Pictures/elasticity-p1.pdf}}
	\subfloat[]{\includegraphics[scale=0.48]{Pictures/elasticity-p2.pdf}}
	\caption{Performance improvement over non-optimized code for the static linear elasticity equation on a single core of the Sandy Bridge architecture.}
	\label{fig:full-app-res}
\end{figure}

In Figure~\ref{fig:full-app-res}, we show the execution times for the four test cases, split over two figures (one for each polynomial order), without and with optimizations enabled. A stacked bar captures the time spent in the assembly phase (\texttt{assembly}), in solving the linear system (\texttt{solve}), and in the other various parts of the program (\texttt{other} - for example, setup of the problem and initialization of the coefficient functions). The \emph{non-opt} and \emph{opt} bars correspond, respectively, to the cases in which no optimizations and a combination of optimizations have been applied. The optimizations applied are generalized loop-invariant code motion, alignment and padding, and expression splitting. We recall that the cost of the insertion of the computed local element matrices (and vectors) in the global matrix (vector) is incorporated in \texttt{assembly}.

We first notice that, in the scenario ($f=1, p=1$), the assembly is
dominated by matrix insertion: despite the application of several
transformations, only a minimal performance gain is achieved. However if
either $p$ or $f$ is increased then the cost of \texttt{assembly} becomes
larger with respect to \texttt{solve}, and the matrix insertion cost becomes
negligible. In such cases, the transformations automatically applied by
COFFEE dramatically decrease the cost of \texttt{assembly}.  This results in
a significant overall speedup and a simulation which is now dominated by
\texttt{solve} time. It is interesting to note that
generalized loop-invariant code motion was particularly invasive in this
case, with 23 temporaries generated and several redundancies discovered (see
Section~\ref{sec:code-transf-licm}).

In these experiments, we observe a maximum performance improvement of
1.47$\times$ over the non-optimized local assembly code, obtained in the
case ($f=2, p=2$). However, we reiterate the fact that full application
speed ups increase proportionally with the amount of time spent in
assembly and, therefore, with the complexity of the equation. By
increasing polynomial order and number of coefficient functions, or by
simply studying a different, more complex equation, it is our experience
that performance gains become increasingly more relevant. The choice of
studying the static linear elasticity equation was to show that even
relatively simple problems can be characterized by a large proportion of
execution time spent in assembly.

\section{Generality of the Approach and Applicability to Other Domains}
\label{sec:generality}
We have demonstrated that our cross-loop optimizations for arithmetic intensity are effective in the context of automated code generation for finite element local assembly. In this section, we discuss their applicability in other computational domains and, in general, their integrability within a general-purpose compiler.

COFFEE was developed as a separate, self-contained software module, with
clear input/output interfaces, rather than incorporating it within
PyOP2. This choice was motivated by two critical aspects that
characterize the generality of our research:

\paragraph{Separation of concerns} We believe that in domain-specific
frameworks there must be a clear, logical separation of roles reflecting
the various levels of abstraction, where domain specialists are
completely separated from performance optimization. In Firedrake, for
instance, COFFEE decouples the mathematical specification of a finite
element method, captured by the Unified Form Language and the FEniCS
Form Compiler, from code optimization. This is of fundamental importance
to maximize productivity by allowing scientists to focus only on their
area of expertise. Practically speaking, from the perspective of the
domain-specific language and compiler designers, our optimization
strategy represents an incentive to produce extremely simple
representations of the code (e.g. fully-inlined mathematical expressions
in the form of an abstract syntax tree, in the case of Firedrake) so as
to make the architecture-aware code optimizer completely responsible for
choosing and applying the most suitable set of transformations.

\paragraph{Generalizability to other domains} There are neither
conceptual nor technical reasons which prevent our compiler from being
used in applications other than Firedrake. For example, integration with
the popular FEniCS framework, the pioneer of automated code generation
for finite element local assembly, would be relatively easy to achieve.
It is more challenging to assess the generality of the optimization
strategy: the extent to which COFFEE and its transformations are
transferable to other computational domains, perhaps other DSLs, and to
what extent this would be helpful for improving full application
performance. To answer these questions, we first need to go back to the
origins of our compiler. The starting point of our work was the
mathematical formulation of a local assembly operation, expressible as
follows

\begin{equation}
\label{eq:assembly-model}
\scriptsize
\forall_{i, j} ~~~ A_{ij}^K = \sum_{q=1}^{n_1} \sum_{k=1}^{n_2} \alpha_{k, q}(a', b', c', ...) \beta_{q, i, j}(a, b, c, d, ...) \gamma_{q}(w_K, z_K)
\end{equation}

The expression represents the numerical evaluation of an integral at $n_1$ points in the mesh element $K$ computing the local element matrix $A$. Functions $\alpha$, $\beta$ and $\gamma$ are problem-specific and can be intricately complex, involving for example the evaluation of derivatives. We can however abstract from the inherent structure of $\alpha$, $\beta$ and $\gamma$ to highlight a number of aspects

\begin{itemize}
\item \textbf{Optimizing mathematical expressions.} Expression manipulation
  (e.g. simplification, decomposition into sub-expressions) opens multiple
  semantically equivalent code generation opportunities, characterized by
  different trade-offs in parallelism, redundant computation, and data
  locality. The basic idea is to exploit properties of arithmetic operators,
  such as associativity and commutativity, to re-schedule the computation suitably for the underlying architecture. Loop-invariant code motion and expression splitting follow this principle, so they can be re-adapted or extended to any domains involving numerical evaluation of complex mathematical expressions (e.g. electronic structure calculations in physics and quantum chemistry relying on tensor contractions~\cite{TCE}). In this context, we highlight three notable points.
\begin{enumerate}
\item In Equation~\eqref{eq:assembly-model}, the summations correspond
  to reduction loops, whereas loops over indices $i$ and $j$ are fully
  parallel. Throughout the paper we assumed that a kernel will be executed
  by a single thread, which is likely to be the best strategy for standard
  multi-core CPUs. On the other hand, we note that for certain architectures
  (for example GPUs) this could be prohibitive due to memory requirements. Intra-kernel parallelization is one possible solution: a domain-specific compiler such as COFFEE could map mathematical quantifiers and operators to different parallelization schemes and generate distinct variants of multi-threaded kernel code. Based on our experience, we believe this is the right approach to achieve performance portability.
\item The various sub-expressions in $\beta$ only depend on (i.e. iterate along) a subset of the enclosing loops. In addition, some of these sub-expressions might reduce to the same values as iterating along certain iteration spaces. This code structure motivated the generalized loop-invariant code motion technique. The intuition is that whenever sub-expressions invariant with respect to different sets of affine loops can be identified, the question of whether, where and how to hoist them, while minimizing redundant computation, arises. Pre-computation of invariant terms also increases memory requirements due to the need for temporary arrays, so it is possible that for certain architectures the transformation could actually cause slowdowns (e.g. whenever the available per-core memory is small).
\item Associative arithmetic operators are the prerequisite for expression
  splitting. In essence, this transformation concerns resource-aware
  execution. In the context of COFFEE, expression splitting has been
  successfully applied to improve register pressure. However, the underlying
  idea of re-scheduling (re-associating) operations to optimize for some
  generic parameters is far more general. It could be used, for example, as
  a starting point to perform kernel fission; that is, splitting a kernel
  into multiple parts, each part characterized by less stringent memory
  requirements (a variant of this idea for non-affine loops in unstructured
  mesh applications has been adopted in~\citeN{carlolcpc}). In
  Equation~\eqref{eq:assembly-model}, for instance, not only can any of the
  functions $\alpha$, $\beta$ and $\gamma$ be split (assuming they include
  associative operators), but $\alpha$ could be completely extracted and
  evaluated in a separate kernel. This would reduce the working set size of
  each of the kernel functions, an option which is particularly attractive for many-core architectures in which the available per-core memory is much smaller than that in traditional CPUs.
\end{enumerate}
\item \textbf{Code generation and applicability of the transformations.} All
  array sizes and loop bounds, for example $n1$ and $n2$ in
  Equation~\ref{eq:assembly-model}, are known at code generation time. This
  means that ``good'' code can be generated. For example, loop bounds can be
  made explicit, arrays can be statically initialized, and pointer aliasing
  is easily avoidable. Further, all of these factors contribute to the
  applicability and the effectiveness of some of our code
  transformations. For instance, knowing loop bounds allows both generation of
  correct code when applying vector-register tiling (see
  Section~\ref{sec:op-vect}) and discovery of redundant computation
  opportunities (see Section~\ref{sec:code-transf-licm}). Padding and data
  alignment are special cases, since they could be performed at run-time
  if some values were not known at code generation time. Theoretically,
  they could also be automated by a general-purpose compiler through profile-guided optimization, provided that some sort of data-flow analysis is performed to ensure that the extra loop iterations over the padded region do not affect the numerical results.
\item \textbf{Multi-loop vectorization.} Compiler auto-vectorization has
  become increasingly effective in a variety of codes. However, to the best
  of our knowledge, multi-loop vectorization involving the loading and
  storing of data along a subset of the loops characterizing the iteration
  space (rather than just along the innermost loop), is not supported by
  available general-purpose and polyhedral compilers. The outer-product
  vectorization technique presented in this paper shows that two-loop
  vectorization can outperform standard auto-vectorization. In addition, we
  expect the performance gain to scale with the number of vectorized loops
  and the vector length (as demonstrated in  the Xeon Phi
  experiments). Although the automation of multi-loop vectorization in a
  general-purpose compiler is far from straightforward, especially if stencils are present, we believe that this could be more easily achieved in specific domains. The intuition is to map the memory access pattern onto vector registers, and then to exploit in-register shuffling to minimize the traffic between memory and processor. By demonstrating the effectiveness of multi-loop vectorization in a real scenario, our research represents an incentive for studying this technique in a broader and systematic way.
\end{itemize}
%We conclude noting that despite experiments were performed on Intel platforms only, performance gains are expected even on completely different CPU architectures, since the techniques proposed in this paper involves re-scheduling

%In this paper, we have shown how to optimize cross-loop arithmetic intensity for standard CPUs, although different optimization strategies are probably needed for other architectures (e.g. GPUs may benefit from distribution of fully-parallel loops among different threads to optimize shared memory usage).

%In general, there can be a number of loops, with

%TODO: from the expression tree to a decorated AST.
% Diderot, tensor contraction, image processing?


\section{Related Work}
\label{sec:related-work}
The finite element method is extensively used to approximate solutions
of PDEs. Well-known frameworks and applications include
nek5000~\cite{nek5000-web-page}, the FEniCS project~\cite{Fenics},
Fluidity~\cite{fluidity_manual_v4}, and of course Firedrake. Numerical
integration based on quadrature, as in Firedrake, is usually employed to implement the local assembly
phase. The recent introduction of domain specific languages (DSLs) to
decouple the finite element specification from its underlying
implementation facilitated, however, the development of novel
approaches. Methods based on tensor contraction~\cite{FFC-Compiler}
and symbolic manipulation~\cite{Francis} have been implemented. Nevertheless, it has been demonstrated that quadrature-based integration remains the most efficient choice for a wide class of problems~\cite{quadrature1}, which motivates our work in COFFEE.

Optimization of quadrature-based local assembly for CPU
architectures has been addressed in FEniCS~\cite{quadrature1}. The
comparison between COFFEE and this work has been presented in
Section~\ref{sec:perf-results-fenics}. In~\citeN{Markall20101815}, and
more recently in~\citeN{petsc-integration-gpu}, the same problem has been
studied for GPU architectures. In~\citeN{assembly-opencl}, variants of
the standard numerical integration algorithm have been specialized and
evaluated for the PowerXCell processor, but an exhaustive study from
the compiler viewpoint - like ours - is missing, and none of the
optimizations presented in Section~\ref{sec:code-transf} are
mentioned. Among these efforts, to the best of our knowledge, COFFEE
is the first work targeting low-level optimizations through a real
compiler approach.

The code transformations presented are inspired by standard compilers optimizations and exploit domain properties. Our loop-invariant code motion technique individuates invariant sub-expressions and redundant computation by analyzing all loops in an iteration space, which is a generalization of the algorithms often implemented by general-purpose compilers. Expression splitting is an abstract variant of loop fission based on properties of arithmetic operators. The outer-product vectorization is an implementation of tiling at the level of vector registers; tiling, or ``loop blocking'', is commonly used to improve data locality (especially for caches). Padding has been used to achieve data alignment and to improve the effectiveness of vectorization. A standard reference for the compilation techniques re-adapted in this work is~\citeN{dragonbook}.

Our compiler-based optimization approach is made possible by the
top-level DSL, which enables automated code generation. DSLs have been
proven successful in auto-generating optimized code for other domains:
Spiral~\cite{Pueschel:05} for digital signal processing numerical
algorithms, ~\citeN{Spampinato:14} for dense linear algebra, or
Pochoir~\cite{pochoir} and SDSL~\cite{stencil-compiler} for image
processing and finite difference stencils. Similarly, PyOP2 is used by
Firedrake to express iteration over unstructured meshes in scientific
codes. COFFEE improves automated code generation in Firedrake.

Many code generators, like those based on the Polyhedral
model~\cite{PLUTO} and those driven by
domain-knowledge~\cite{modeldriven}, make use of cost models. The
alternative of using auto-tuning to select the best implementation for
a given problem on a certain platform has been adopted by
nek5000~\cite{nek5000} for small matrix-matrix multiplies, the ATLAS
library~\cite{ATLAS}, and FFTW~\cite{FFTW} for fast fourier
transforms. In both cases, pruning the implementation space is
fundamental to mitigate complexity and overhead. Likewise, COFFEE uses
a cost model and heuristics (Section~\ref{sec:pyop2-compiler}) to
steer the optimization process.

%(i.e., given a mesh, those that lead to ``less accurate'' solutions than when using high-order polynomials, at a lower computational cost)

%Show other work of people working on optimizations for computational science kernels. Inter-kernel vectorisation paper from Istvan. Previous work in FFC. Spencer et-al + Shin et. al. attempt to optimize computations that could benefit from using BLAS, but in practice they don't, due to the very small dgemm operations employed. Saday's model-driven SIMD code vectorisation for the tensor contraction engine.


\section{Conclusions}
\label{sec:conclusions}

In this paper, we have presented the study and systematic performance
evaluation of a class of composable cross-loop optimizations for improving
arithmetic intensity in finite element local assembly kernels, and their
integration in a novel compiler, COFFEE. In the context of automated code
generation for finite element local assembly, COFFEE is the first compiler
capable of introducing low-level optimizations to maximize instruction-level
parallelism, register locality and SIMD vectorization. Assembly kernels have
particular characteristics. Their iteration space is usually very small, with
the size depending on aspects like the degree of accuracy one wants to
reach (polynomial order of the method) and the mesh discretization employed.
The data space, in terms of number of arrays and scalars required to evaluate
the element matrix, grows proportionally with the complexity of the
finite element problem. COFFEE has been developed taking into account
all of these degrees of freedom, based on the idea that reducing the
problem of local assembly optimization to a fixed sequence of
transformations is far too superficial if close-to-peak performance
needs to be reached. The various optimizations overcome limitations of
current vendor and research compilers. The exploitation of domain
knowledge allows some of them to be particularly effective, as
demonstrated by our experiments on two state-of-the-art Intel
platforms. Further work includes a comprehensive study about
feasibility and constraints on transforming kernels into a sequence of
calls to external linear algebra libraries. COFFEE supports all of the
problems expressible in Firedrake, and is already integrated with
this framework. The generality and the applicability of the proposed
code transformations to other domains has also been discussed.


% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{biblio}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012


\medskip

\end{document}
