CARLO

Hi,

First of all, thanks for sharing this - it is a highly practical yet well thought paper on an extremely critical subject.
In terms of contributions, it should be highlighted in the paper that the generality of the structure of the studied kernels enables indeed simple transformations, but these can be conversely applied to a large set of programs, apart from fenics/computational science - that is, simplicity is key to large applicability and, as you show, convincing performance improvements. I would really like to see a commercial compiler able to detect the cases you identify and apply your transformations, possibly not in a distant future.  On the other hand, very complex transformations for extremely specific cases may deliver incredibly good (read my lips) performance improvements, but no way commercial compilers will consider those corner cases. You study the tension between these two opposites, leaning towards the former, in a successful manner. Maybe I am biased.

Some comments that may be useful:

Introduction:

- To address this problem, domain-specific languages (DSLs) have been developed.

As you know, DSL are developed and used to both offer a high level model to computational scientists and give full opportunity to the software stacks that underly them to introduce optimizations.
Probably a rephrasing of this would be:  "DSL have been developed to offer a high level model to computation scientists, nearer to their ever-day abstractions, and to feed more information on the program to the underlying software stack, often with optimizations in mind." Or something like this, you write the actual text :-)


- with an implicit synchronization between the application of two consecutive kernels.

I am not sure this is important here - it may be more important to tell the reader that OP2 offers a parallel loop construct with access descriptors.


- it might need L2 cache when high-order methods are employed to improve the accuracy of the solution. However, we do not consider the latter case.

I am not sure about where the results point, but I thought that some transformations were more successful for high polynomial orders. This sentence seems to suggest that you are throwing out of the window the main optimization: I suggest rephrasing it to meet your needs.

- (the trip count rarely exceeds 30,

Is this related to a single loop or to the total trip count of the loop nest?

- With such small kernels, we focus on aspects like minimization of floating-point operations, register allocation and instruction- level parallelism, especially in the form of SIMD vectorization.

Does this complement data locality techniques e.g. targeting L2 cache? Is that trivial to do? What's the relationship?
This does not require any fixing in the paper, but it may be a question asked by a referee.

- can result in performance improvements up to 1.5x
over “softly-optimized” code (i.e. where only basic transfor- mations are performed, such as generalized loop-invariant code motion, padding, and data alignment), and up to 4.44x over original kernels.

Any idea of what a "optimal" improvement would be? This would put things into context. I am asking because, sometime, it is possible to see how good an optimization is related to hardware factors. For instance, in a 4 lane vector SIMD you'd ideally would like to see 4x improvement between a vectorized version and the corresponding un-vectorized one.

- Systematic evaluation using a suite of examples of 24
real-world importance, and evidence of significant performance improvements.

On what machines?

Background

- floatin point values

floatin*g*

-  (i.e. using static const in C language).

why? (I may know, but you should spell it out)

- 14 unique arrays

unique?

- called F in the code

A constant called F and a function called K?

- Despite assembly kernels being problem-dependent, meaning that the space of codes that Firedrake can generate is infinite, it is still possible to identify common domain-specific traits, which can be exploited for effective code transformations and SIMD vectorization.


Perhaps you may want to say that your compiler infrastructure targets a large and relevant subset of the kernels that can be generated by firedrake?
The paragraph that follows does not help in this direction: why are you not considering those other cases? You also mention that "This is not true in general, since bent elements might be used in the unstructured mesh, which would require the Jacobian be re-computed at every i iteration": what would happen to your transformation for these cases? How relevant are these cases?

Section 3:

- The code transformations presented in this section are applicable to all finite element problems that can be formulated in Firedrake.

But...you just said that you don't...I am confused.


Section 3D

This transformation looks like something that an off-the-shelf compiler may be able to do easily. However, the goal of this paper is not to show extremely complex (sequences of) transformations, but what relatively known ones can be composed successfully for the studied kernels and architectures. I think this comment should be highlighted at the beginning of section 3 (so that no one is going to complain about the relatively straightforwardness of the single transformations).

 After 3D

- we need a mechanism to prune such a large space of optimization.

This is the key I was trying to highlight above.

Section 4:

-  Autotuning might be used, although at the moment we avoid it to minimize the run- time overhead.

It is unclear why autotuning should impact run-time (I know the reason for this, but most compilers' people don't, so be super clear here).

- the transformed code seems to prevent the intel compiler from applying this optimization

What about gcc?

- The cost model is shown in Figure 8.

Figure 8 is Helmoltz assembly code. Figure 10 looks a better pick for this reference.

- Figure 10 label: The cost model is employed

Remove "The" and "is".

- Finally, the profitability of loop interchange is evaluated (line 12-16)

Missing full stop at the end of sentence.

Section 5

- You have to include phi results (!!): having two variations over the same (lousy) architecture will help delivering the message.

At this point it is just too late and I had too many beers - I will continue tomorrow.
In the meantime, as usual, I hope this helps.


Thanks


===================================================================================================

ANA

Hi Fabio,

A few issues in the abstract:

---

-- I think it is better to emphasize how large "huge" is, and to clearly
say that this is how often the kernel is invoked:

"Since the domain can be extremely large, the kernel will be executed up
to XXX times. Therefore, the efficiency of the kernel is fundamental
*for the overall performance of the FEM solver*."

-- You did not mention optimizations yet, so I think it's better to
first say there are opportunities for optimizations, but they are
difficult to automate; also - are you sure you want to mention
automation? I think here you want to state that there are many and it is
difficult to choose, like you say in the following paragraph. The
automation is a second concern, isn't it?
-- also, make sure that "small problem sizes" does not seem
contradictory to "huge domain" - it is unclear what the problem size is
here.
-- why do you want a "single/unique" sequence of transformations? I mean
- to me, it sounds like you want to point out that such a
"one-size-fits-all" solution doesn't exist, and "a specific sequence of
transformations must be determined for each case".

"Achieving high efficiency for a kernel is no longer a matter of
applying one single optimization, but requires specific sequences of
transformations. Even if an affine loop nest can be generally
distinguished, *particular problem sizes* and/or the complexity of the
mathematical expressions make it difficult to (automatically) determine
the sequence of transformations that leads to the highest performance."

-- having introduced the problem, "Therefore" might be more appropriate
here; the rest is very strong, I find.

*Therefore*, we present the design and systematic evaluation of COFFEE,
a domain-specific compiler for local assembly kernels.
COFFEE manipulates abstract syntax trees by introducing \emph{composable
optimizations} aimed at improving instruction-level parallelism, SIMD
vectorization, and register locality.
COFFEE generates optimized C code (often including vector intrinsics).

-- I usually like this in a different tone, first person and stronger.
Also, it is better to close the loop to the solver.

We have tested the performance of COFFEE with an extensive set of
experiments, based on XXX real-world examples (from xxx equations and 4
problem sizes) and 2 architectures (... and ...). Our results show that
an average kernel performance improvement of as much as ...%, which can
save XXX mins/hours for the entire FEM solver.

---

More details on the paper follow tomorrow. BTW, I am not at TUDelft,
officially, but "Informatics Institute, University of Amsterdam, The
Netherlands".

Regards,
Ana


===================================================================================================

FLORIAN

Since you put me on there as 2nd author I definitely own you some
contribution. Here's some thoughts for the abstract:

The finite element method is widely employed to approximate solutions of
partial differential equations. Its local assembly operation, in which a
problem-specific kernel is applied to all elements of the discretized
domain, makes it amenable to parallelization. Executing these kernels
efficiently is fundamental when solving very large problems. Automating
their optimization is, however, a challenging issue. Even though these
kernels generally contain affine loop nests, short loop lengths and the
complexity of mathematical expressions make it hard to determine a
single or unique sequence of successful [successive?] transformations.
In this context, we present the design and systematic evaluation of
COFFEE, a domain-specific compiler for local assembly kernels. COFFEE
manipulates abstract syntax trees by introducing composable
optimizations [what makes them composable?] aimed at improving
instruction-level parallelism, in particular SIMD vectorization, and
register locality. It then generates C code using vector intrinsics.
Systematic performance evaluation using a suite of real-world examples
[really?] demonstrates significant speed-ups [over?].

This is without having read the paper. More to follow at a later time.


